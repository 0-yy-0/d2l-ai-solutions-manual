{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 第5章 深度学习计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 层和块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.1.1 \n",
    "\n",
    "如果将MySequential中存储块的方式更改为Python列表，会出现什么样的问题？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果将MySequential中存储块的方式更改为Python列表，则会失去原有的顺序保持和快速查找的优势。同时，需要手动处理块的顺序和输入数据的传递，增加代码的复杂度和容易出错。因此，建议仍然使用OrderedDict来存储块。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.1.2\n",
    "\n",
    "实现一个块，它以两个块为参数，例如net1和net2，并返回前向传播中两个网络的串联输出。这也被称为平行块。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本书7.4节中GoogleLet模型中的Inception块使用了平行块技术.\n",
    "下面代码实现了一个并行网络，由两个子网络组成。输入数据先分别经过两个子网络的计算，分别得到两个部分的输出结果，然后在通道维度上合并结果得到最终输出。其中，net1和net2分别表示两个子网络，Sequential表示将多个层组成一个序列的容器，Linear表示一个线性层，ReLU表示一个激活函数，cat表示在指定维度上拼接张量。最后，输出结果的大小是(2, 36)，表示有2个样本，每个样本的特征维度是36。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 36])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class Parallel(nn.Module):\n",
    "    def __init__(self, net1, net2):\n",
    "        super().__init__()\n",
    "        self.net1=net1 # 第一个子网络\n",
    "        self.net2=net2 # 第二个子网络\n",
    "        \n",
    "    def forward(self,X):\n",
    "        x1= self.net1(X) # 第一个子网络的输出\n",
    "        x2= self.net2(X) # 第二个子网络的输出\n",
    "        return torch.cat((x1,x2),dim=1) # 在通道维度上合并输出结果\n",
    "      \n",
    "X = torch.rand(2,10) # 输入数据\n",
    "net = Parallel(nn.Sequential(nn.Linear(10,12),nn.ReLU()), nn.Sequential(nn.Linear(10,24),nn.ReLU())) # 实例化并行网络\n",
    "output = net(X) # 前向传播\n",
    "print(output.size()) # 输出结果的大小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.1.3\n",
    "\n",
    "假设我们想要连接同一网络的多个实例。实现一个函数，该函数生成同一个块的多个实例，并在此基础上构建更大的网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=5, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=5, out_features=10, bias=True)\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=5, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=5, out_features=10, bias=True)\n",
       "  )\n",
       "  (2): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=5, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=5, out_features=10, bias=True)\n",
       "  )\n",
       "  (output): Linear(in_features=10, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_network(num_instances, input_size, hidden_size, output_size):\n",
    "    # 创建一个线性层\n",
    "    linear_layer = nn.Sequential(\n",
    "        nn.Linear(input_size, hidden_size), nn.ReLU(),\n",
    "        nn.Linear(hidden_size, input_size)\n",
    "    )\n",
    "    \n",
    "    # 创建多个实例并连接\n",
    "    instances = [linear_layer for _ in range(num_instances)]\n",
    "    network = nn.Sequential(*instances)\n",
    "    \n",
    "    # 添加输出层\n",
    "    output_layer = nn.Linear(input_size, output_size)\n",
    "    network.add_module(\"output\", output_layer)\n",
    "    \n",
    "    return network\n",
    "# 示例用法\n",
    "net = create_network(num_instances=3, input_size=10, hidden_size=5, output_size=2)\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 参数管理 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.2.1\n",
    "\n",
    "使用`FancyMLP`模型，访问各个层的参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "引用上5.1节中的FancyMLP模型,并用state_dict()方法访问模型的全部参数。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 不计算梯度的随机权重参数。因此其在训练期间保持不变\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        # 使用创建的常量参数以及relu和mm函数\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n",
    "        # 复用全连接层。这相当于两个全连接层共享参数\n",
    "        X = self.linear(X)\n",
    "        # 控制流\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "net = FixedHiddenMLP()\n",
    "net(X)\n",
    "net.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.2.2\n",
    "\n",
    "查看初始化模块文档以了解不同的初始化方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.nn.init.uniform_(tensor, a=0.0, b=1.0)`：使输入的张量服从（a,b）的均匀分布并返回。\n",
    "\n",
    "- `torch.nn.init.normal_(tensor, mean=0.0, std=1.0)`：从给定的均值和标准差的正态分布N(mean,std2)中生成值，初始化张量。\n",
    "\n",
    "- `torch.nn.init.constant_(tensor, val)`：以一确定数值初始化张量。\n",
    "\n",
    "- `torch.nn.init.xavier_uniform_(tensor, gain=1.0)`：从均匀分布U(−a, a)中采样，初始化输入张量，其中a的值由如下公式确定\n",
    "\n",
    "  $a= gain * \\sqrt{\\frac{6}{fan_{in}+fan_{out}}}$\n",
    "\n",
    "  其中的`gain`为可缩放因子，可以用`torch.nn.init.calculate_gain(nonlinearity, param=None)`方法得到，此方法其实就是一查表，背后对应的表格如下\n",
    "\n",
    "  ![初始化方法-gain因子.png](../.././images/初始化方法-gain因子.png)\n",
    "\n",
    "- `torch.nn.init.xavier_normal_(tensor, gain=1.0)`:从正态分布N(0,std2)中采样，初始化输入张量，其中std值由下式确定：\n",
    "\n",
    "  $a= gain * \\sqrt{\\frac{2}{fan_{in}+fan_{out}}}$\n",
    "\n",
    "- `torch.nn.init.kaiming_uniform_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')`:服从均匀分布U(−bound, bound)，其中bound值由下式确定\n",
    "\n",
    "  $bound= gain * \\sqrt{\\frac{3}{fan_{mode}}}$\n",
    "\n",
    "- `torch.nn.init.kaiming_normal_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')`:服从从正态分布N(0,std2)中采样，其中std值由下式确定\n",
    "\n",
    "  $std= \\frac{gain}{\\sqrt{fan_{mode}}}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.2.3\n",
    "\n",
    "构建包含共享参数层的多层感知机并对其进行训练。在训练过程中，观察模型各层的参数和梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练过程中，我们每个epoch都打印了每层的参数和梯度。可以看到shared_fc层的参数和梯度都是相同的，因为它们共享同一个参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight tensor([[-0.1180,  0.1194],\n",
      "        [ 0.5747, -0.4074],\n",
      "        [ 0.2131,  0.1560],\n",
      "        [ 0.5971, -0.5614]]) tensor([[-0.0000, -0.0000],\n",
      "        [-0.1484, -0.1379],\n",
      "        [-0.0000, -0.0000],\n",
      "        [-0.0861, -0.0800]])\n",
      "0.bias tensor([-0.2144,  0.2941,  0.0637,  0.3801]) tensor([0.0000, 0.2614, 0.0000, 0.1518])\n",
      "2.weight tensor([[ 0.3504,  0.1069,  0.0545,  0.0266],\n",
      "        [ 0.0044,  0.4416,  0.4497,  0.4842],\n",
      "        [-0.3265,  0.2675, -0.2384, -0.3932],\n",
      "        [-0.2787,  0.2753,  0.1946,  0.2054]]) tensor([[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.1166,  0.0831,  0.3548],\n",
      "        [ 0.0000,  0.0602,  0.0529,  0.2004],\n",
      "        [ 0.0000,  0.0106, -0.0276, -0.0283]])\n",
      "2.bias tensor([-0.4919, -0.2057,  0.1845,  0.1653]) tensor([ 0.0000,  1.1674,  0.6683, -0.1243])\n",
      "6.weight tensor([[-0.2431,  0.4902,  0.3570, -0.3329],\n",
      "        [-0.4766, -0.4757, -0.2619,  0.0098]]) tensor([[ 0.0000,  0.0135,  0.0514,  0.1942],\n",
      "        [-0.0000, -0.0151, -0.0573, -0.2167]])\n",
      "6.bias tensor([ 0.0873, -0.0446]) tensor([ 0.7510, -0.8380])\n",
      "Epoch: 0, Loss: 0.6331449747085571\n",
      "0.weight tensor([[-0.1180,  0.1194],\n",
      "        [ 0.5742, -0.4078],\n",
      "        [ 0.2131,  0.1560],\n",
      "        [ 0.5973, -0.5612]]) tensor([[ 0.0000,  0.0000],\n",
      "        [ 0.0475,  0.0441],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [-0.0243, -0.0226]])\n",
      "0.bias tensor([-0.2144,  0.2949,  0.0637,  0.3797]) tensor([ 0.0000, -0.0836,  0.0000,  0.0429])\n",
      "2.weight tensor([[ 0.3504,  0.1069,  0.0545,  0.0266],\n",
      "        [ 0.0044,  0.4415,  0.4497,  0.4840],\n",
      "        [-0.3265,  0.2676, -0.2389, -0.3940],\n",
      "        [-0.2787,  0.2758,  0.1949,  0.2069]]) tensor([[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0106,  0.0000,  0.0195],\n",
      "        [ 0.0000, -0.0115,  0.0479,  0.0807],\n",
      "        [ 0.0000, -0.0533, -0.0254, -0.1523]])\n",
      "2.bias tensor([-0.4919, -0.2063,  0.1814,  0.1702]) tensor([ 0.0000,  0.0579,  0.3133, -0.4906])\n",
      "6.weight tensor([[-0.2431,  0.4902,  0.3566, -0.3348],\n",
      "        [-0.4766, -0.4757, -0.2615,  0.0119]]) tensor([[ 0.0000,  0.0000,  0.0430,  0.1859],\n",
      "        [ 0.0000,  0.0000, -0.0480, -0.2077]])\n",
      "6.bias tensor([ 0.0799, -0.0364]) tensor([ 0.7322, -0.8180])\n",
      "Epoch: 1, Loss: 0.6026402711868286\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 模型参数\n",
    "input_size = 2\n",
    "hidden_size = 4\n",
    "output_size = 2\n",
    "lr = 0.01\n",
    "epochs = 2\n",
    "\n",
    "# 构建带有共享参数层的多层感知机\n",
    "shared_fc = nn.Linear(hidden_size, hidden_size)\n",
    "MLP = nn.Sequential(nn.Linear(input_size, hidden_size), nn.ReLU(),\n",
    "                    shared_fc, nn.ReLU(),\n",
    "                    shared_fc, nn.ReLU(),\n",
    "                    nn.Linear(hidden_size, output_size)\n",
    ")\n",
    "\n",
    "# 训练数据\n",
    "X = torch.randn(1, input_size)\n",
    "Y = torch.randn(1, output_size)\n",
    "# 创建模型和优化器\n",
    "# MLP(input_size, hidden_size, output_size)\n",
    "optimizer = optim.SGD(MLP.parameters(), lr=lr)\n",
    "# 训练模型\n",
    "for epoch in range(epochs):\n",
    "    # 前向传播和计算损失\n",
    "    Y_pred = MLP(X)\n",
    "    loss = nn.functional.mse_loss(Y_pred, Y)\n",
    "    # 反向传播和更新梯度\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # 打印每层的参数和梯度\n",
    "    for name, param in MLP.named_parameters():\n",
    "        print(name, param.data, param.grad)\n",
    "    print('Epoch: {}, Loss: {}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.2.4\n",
    "\n",
    "为什么共享参数是个好主意？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 节约内存：共享参数可以减少模型中需要存储的参数数量，从而减少内存占用。\n",
    "\n",
    "2. 加速收敛：共享参数可以让模型更加稳定，加速收敛。\n",
    "\n",
    "3. 提高泛化能力：共享参数可以帮助模型更好地捕捉数据中的共性，提高模型的泛化能力。\n",
    "\n",
    "4. 加强模型的可解释性：共享参数可以让模型更加简洁明了，加强模型的可解释性。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 延后初始化 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.3.1 \n",
    "\n",
    "如果指定了第一层的输入尺寸，但没有指定后续层的尺寸，会发生什么？是否立即进行初始化？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以正常运行。第一层会立即初始化,但其他层是直到数据第一次通过模型传递才会初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): LazyLinear(in_features=0, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): LazyLinear(in_features=0, out_features=10, bias=True)\n",
      ")\n",
      "Sequential(\n",
      "  (0): Linear(in_features=20, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\anaconda_install\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\"\"\"延后初始化\"\"\"\n",
    "net = nn.Sequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\n",
    "# print(net[0].weight)  # 尚未初始化\n",
    "print(net)\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "net(X)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.3.2\n",
    "\n",
    "如果指定了不匹配的维度会发生什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "会由于矩阵乘法的维度不匹配而报错。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.3.3 \n",
    "\n",
    "如果输入具有不同的维度，需要做什么？提示：查看参数绑定的相关内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果输入维度比指定维度小，可以考虑使用padding填充；如果输入维度比指定维度大，可以考虑用pca等降维方法，将维度降至指定维度。再或者我们可以改变模型的指定维度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 自定义层 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.4.1 \n",
    "\n",
    "设计一个接受输入并计算张量降维的层，它返回$y_k = \\sum_{i, j} W_{ijk} x_i x_j$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TensorLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(TensorLayer, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.view(batch_size, -1)\n",
    "        y = torch.einsum('bij,bik->bjk', [x.unsqueeze(1), x.unsqueeze(2)])\n",
    "        y = torch.einsum('bij,jik->bjk', [y.view(batch_size, -1), self.weight])\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.4.2 \n",
    "\n",
    "设计一个返回输入数据的傅立叶系数前半部分的层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.fft as fft\n",
    "\n",
    "class FourierLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FourierLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = fft.fftn(x)\n",
    "        x = x[:, :, :x.shape[2] // 2]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 读写文件 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  练习 5.5.1\n",
    "\n",
    "即使不需要将经过训练的模型部署到不同的设备上，存储模型参数还有什么实际的好处？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 加速/避免重复训练：可用于在训练模型过程中生成checkout文件（存储模型参数），必要时可以go back，方便后续梯度下降时找到局部最优点。\n",
    "\n",
    "2. 便于共享和复制：可以将参数保存到文件中，并将文件发送给其他人，而无需共享整个模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.5.2\n",
    "\n",
    "假设我们只想复用网络的一部分，以将其合并到不同的网络架构中。比如想在一个新的网络中使用之前网络的前两层，该怎么做？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用保存模型某层参数的办法，保存网络的前两层，然后再加载到新的网络中使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "torch.save(net.hidden.state_dict(), 'mlp.hidden.params')\n",
    "clone = MLP()\n",
    "clone.hidden.load_state_dict(torch.load('mlp.hidden.params'))\n",
    "print(clone.hidden.weight == net.hidden.weight)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.5.3\n",
    "\n",
    "如何同时保存网络架构和参数？需要对架构加上什么限制？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch本身不提供同时保存网络架构和参数的方法，这点和TensorFlow不同。需要确保在保存模型之前，已经定义了网络对象，因为模型参数只能加载到与其形状相同的网络中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "\n",
    "# 存储模型\n",
    "torch.save(net.state_dict(), 'model.pt')\n",
    "\n",
    "# 导入模型\n",
    "model = Net()\n",
    "model.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 GPU "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.6.1 \n",
    "\n",
    "尝试一个计算量更大的任务，比如大矩阵的乘法，看看CPU和GPU之间的速度差异。再试一个计算量很小的任务呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算量很大的任务：使用GPU速度明显更快\n",
    "\n",
    "计算量很小的任务：CPU速度可能更快，因为数据传输到GPU需要时间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.6.2\n",
    "\n",
    "我们应该如何在GPU上读写模型参数？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.6.3 \n",
    "\n",
    "测量计算1000个$100 \\times 100$矩阵的矩阵乘法所需的时间，并记录输出矩阵的Frobenius范数，一次记录一个结果，而不是在GPU上保存日志并仅传输最终结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 生成随机矩阵\n",
    "matrices = [torch.randn(100, 100).to(device) for i in range(1000)]\n",
    "\n",
    "# 计算时间\n",
    "start_time = time.time()\n",
    "for i in range(1000):\n",
    "    result = torch.mm(matrices[i], matrices[i].t())\n",
    "    frobenius_norm = torch.norm(result)\n",
    "#     print(frobenius_norm)\n",
    "end_time = time.time()\n",
    "\n",
    "# print(\"Time taken:\", end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.6.4\n",
    "\n",
    "测量同时在两个GPU上执行两个矩阵乘法与在一个GPU上按顺序执行两个矩阵乘法所需的时间。提示：应该看到近乎线性的缩放。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "# 创建两个随机矩阵\n",
    "a = torch.randn(10000, 10000).cuda()\n",
    "b = torch.randn(10000, 10000).cuda()\n",
    "\n",
    "# 顺序计算\n",
    "start_time = time.time()\n",
    "c1 = torch.matmul(a, b)\n",
    "c2 = torch.matmul(a, b)\n",
    "end_time = time.time()\n",
    "sequential_time = end_time - start_time\n",
    "\n",
    "print(f\"Sequential time: {sequential_time:.4f} seconds\")\n",
    "\n",
    "# 检查GPU数量\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "# 分配GPU\n",
    "device1 = torch.device(\"cuda:0\")\n",
    "device2 = torch.device(\"cuda:1\")\n",
    "\n",
    "# 存到不同GPU\n",
    "a = torch.randn(10000, 10000).to(device1)\n",
    "b = torch.randn(10000, 10000).to(device2)\n",
    "\n",
    "# 并行计算\n",
    "start_time = time.time()\n",
    "a1, a2 = torch.chunk(a, 2, dim=1)\n",
    "b1, b2 = torch.chunk(b, 2, dim=0)\n",
    "c1 = torch.matmul(a1, b1) + torch.matmul(a2, b2)\n",
    "c2 = torch.matmul(a1, b1) + torch.matmul(a2, b2)\n",
    "end_time = time.time()\n",
    "parallel_time = end_time - start_time\n",
    "\n",
    "print(f\"Parallel time: {parallel_time:.4f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
