{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# 第5章 深度学习计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 层和块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.1.1 \n",
    "\n",
    "如果将MySequential中存储块的方式更改为Python列表，会出现什么样的问题？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如果将MySequential中存储块的方式从OrderedDict更改为Python列表，代码可以正常计算，但并没有注册给Module。无法像`_modules`一样使用很多内置方完成已经实现的功能。如无法通过`net.state_dict()`访问模型的网络结构和参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class MySequential(nn.Module):\n",
    "    # 使用OrderedDict存储块的类\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员\n",
    "            # 变量_modules中。_module的类型是OrderedDict\n",
    "            self._modules[str(idx)] = module\n",
    "\n",
    "    def forward(self, X):\n",
    "        # OrderedDict保证了按照成员添加的顺序遍历它们\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X\n",
    "\n",
    "class MySequential_list(nn.Module):\n",
    "    # 使用list存储块的类\n",
    "    def __init__(self, *args):\n",
    "        super(MySequential_list, self).__init__()\n",
    "        self.sequential = []\n",
    "        for module in args:\n",
    "            self.sequential.append(module)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for module in self.sequential:\n",
    "            X = module(X)\n",
    "        return X\n",
    "\n",
    "\n",
    "X = torch.rand(1,10)\n",
    "# 初始化两个block，确保传给MySequential和MySequential_list的是一样的参数。消除nn.Linear初始化时生成随机参数对结果的影响。\n",
    "block1 = nn.Linear(10, 20)\n",
    "block2 = nn.Linear(20, 10)\n",
    "net = MySequential(block1, nn.ReLU(), block2)\n",
    "net_list = MySequential_list(block1, nn.ReLU(), block2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;对比两种方式的结果，可以发现输出完全一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "采用python OrderedDict的方式存储 :\n",
      "tensor([[0.0582, 0.1493, 0.0690, 0.4035, 0.1546, 0.1691, 0.0714, 0.2457, 0.0915,\n",
      "         0.2701]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "采用python列表的方式存储 :\n",
      "tensor([[0.0582, 0.1493, 0.0690, 0.4035, 0.1546, 0.1691, 0.0714, 0.2457, 0.0915,\n",
      "         0.2701]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "两种方式的计算结果是否一致： True\n"
     ]
    }
   ],
   "source": [
    "net_y = net(X)\n",
    "net_list_y = net_list(X)\n",
    "\n",
    "print(\"采用python OrderedDict的方式存储 :\", net_y, sep='\\n')\n",
    "print(\"\\n\")\n",
    "print(\"采用python列表的方式存储 :\", net_list_y, sep='\\n')\n",
    "print(\"\\n\")\n",
    "print(\"两种方式的计算结果是否一致：\", net_y.equal(net_list_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;但是在查看模型结构和参数时存在差异。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "采用python OrderedDict的方式存储 :\n",
      "\n",
      "网络结构 : \n",
      " MySequential(\n",
      "  (0): Linear(in_features=10, out_features=20, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=20, out_features=10, bias=True)\n",
      ")\n",
      "参数 ：\n",
      "0.weight tensor([[-0.1362, -0.1046,  0.2931,  0.1037, -0.0262,  0.0397,  0.3039,  0.2889,\n",
      "         -0.2657, -0.2991],\n",
      "        [-0.1937,  0.1747,  0.1011, -0.2762, -0.2860,  0.1979, -0.2156, -0.2557,\n",
      "          0.2605, -0.0732]])\n",
      "0.bias tensor([-0.2270, -0.0875])\n",
      "2.weight tensor([[ 0.0352,  0.1039, -0.2078, -0.0484, -0.1247,  0.0517,  0.1218, -0.0847,\n",
      "         -0.0480,  0.0699,  0.1584,  0.0302,  0.1800,  0.0026, -0.2152, -0.2162,\n",
      "          0.1819,  0.0288, -0.0321, -0.1135],\n",
      "        [ 0.0210, -0.1428, -0.0452,  0.0554,  0.1378,  0.0806,  0.0480,  0.1631,\n",
      "          0.0599, -0.1287,  0.0318, -0.2155,  0.1711,  0.2007,  0.1199, -0.0329,\n",
      "         -0.1352, -0.0747,  0.1737,  0.0999]])\n",
      "2.bias tensor([ 0.1084, -0.0108])\n",
      "\n",
      "\n",
      "采用python列表的方式存储 :\n",
      "\n",
      "网络结构 : \n",
      " MySequential(\n",
      "  (0): Linear(in_features=10, out_features=20, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=20, out_features=10, bias=True)\n",
      ")\n",
      "参数： OrderedDict()\n"
     ]
    }
   ],
   "source": [
    "# 使用_modules方便打印net的网络结构和参数，而list则无法做到\n",
    "print(\"采用python OrderedDict的方式存储 :\\n\")\n",
    "print(\"网络结构 : \\n\", net)\n",
    "# 为了可视化效果简洁，这里只展示了每个参数的部分值\n",
    "print(\"参数 ：\", \"\\n\".join([str(k) + str(\" \") + str(v[:2]) for k,v in net.state_dict().items()]), sep='\\n')\n",
    "print(\"\\n\")\n",
    "print(\"采用python列表的方式存储 :\\n\")\n",
    "print(\"网络结构 : \\n\", net) \n",
    "print(\"参数：\", net_list.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.1.2\n",
    "\n",
    "实现一个块，它以两个块为参数，例如net1和net2，并返回前向传播中两个网络的并联输出。这也被称为并行块。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在本书7.4节中`GoogleLet`模型中的`Inception`块使用了并行块技术。\n",
    "下面代码实现了一个并行网络，由两个子网络组成。输入数据先分别经过两个子网络的计算，分别得到两个部分的输出结果，然后在通道维度上合并结果得到最终输出。\n",
    "\n",
    "&emsp;&emsp;其中，`net1`和`net2`分别表示两个子网络，`torch.cat`在指定维度上拼接张量。输出结果的大小为`torch.Size([2, 36])`，其中第一个维度表示batch_size为2，第二个维度表示输出特征图的通道数为36，正好为两个模型的输出特征图通道数之和（12+24=36）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型结构 :\n",
      "Parallel(\n",
      "  (net1): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=12, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (net2): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=24, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      ")\n",
      "模型输出的形状 : torch.Size([2, 36])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class Parallel(nn.Module):\n",
    "    # 自定义并行块\n",
    "    def __init__(self, net1, net2):\n",
    "        super().__init__()\n",
    "        self.net1=net1 # 第一个子网络\n",
    "        self.net2=net2 # 第二个子网络\n",
    "        \n",
    "    def forward(self,X):\n",
    "        x1= self.net1(X) # 第一个子网络的输出\n",
    "        x2= self.net2(X) # 第二个子网络的输出\n",
    "        return torch.cat((x1,x2),dim=1) # 在通道维度上合并输出结果\n",
    "      \n",
    "X = torch.rand(2,10) # 输入数据\n",
    "net = Parallel(nn.Sequential(nn.Linear(10,12),nn.ReLU()), nn.Sequential(nn.Linear(10,24),nn.ReLU())) # 实例化并行网络\n",
    "output = net(X)\n",
    "print(\"模型结构 :\", net, sep=\"\\n\") # 查看模型结构\n",
    "print(\"模型输出的形状 :\", output.size()) # 输出结果的大小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.1.3\n",
    "\n",
    "假设我们想要连接同一网络的多个实例。实现一个函数，该函数生成同一个块的多个实例，并在此基础上构建更大的网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;下面代码定义了一个函数`create_network`，该函数接受四个参数：`num_instances`、`input_size`、`hidden_size` 和 `output_size`，并返回一个Sequential模型。\n",
    "\n",
    "&emsp;&emsp;其中，该网络模型首先包含 `num_instances` 个相同的线性层，每个线性层有两个子层：一个输入维度为 `input_size`，输出维度为 `hidden_size` 的全连接层，和一个 ReLU 非线性激活层。然后，这 `hidden_size` 个线性层连接在一起作为整个网络的前馈部分。最后，额外添加一个输出层，其输入维度为 `input_size`，输出维度为 `output_size`。\n",
    "\n",
    "&emsp;&emsp;因此，最终的网络结构是由 `output_size` 个相同的线性层组成的前馈神经网络，每个线性层内部包含一个全连接层和一个ReLU激活层，以及一个独立的输出层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=5, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=5, out_features=10, bias=True)\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=5, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=5, out_features=10, bias=True)\n",
      "  )\n",
      "  (2): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=5, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=5, out_features=10, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=10, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "def create_network(num_instances, input_size, hidden_size, output_size):\n",
    "    # 创建一个线性层\n",
    "    linear_layer = nn.Sequential(\n",
    "        nn.Linear(input_size, hidden_size), nn.ReLU(),\n",
    "        nn.Linear(hidden_size, input_size)\n",
    "    )\n",
    "    \n",
    "    # 创建多个相同结构的实例并连接\n",
    "    instances = [linear_layer for _ in range(num_instances)]\n",
    "    network = nn.Sequential(*instances)\n",
    "    \n",
    "    # 添加输出层\n",
    "    output_layer = nn.Linear(input_size, output_size)\n",
    "    network.add_module(\"output\", output_layer)\n",
    "    \n",
    "    return network\n",
    "\n",
    "# 模型参数\n",
    "num_instances=3\n",
    "input_size=10\n",
    "hidden_size=5\n",
    "output_size=2\n",
    "\n",
    "# 示例用法\n",
    "net = create_network(num_instances=num_instances, input_size=input_size, \n",
    "                     hidden_size=hidden_size, output_size=output_size) # 重复3次相同的线形层\n",
    "\n",
    "# 查看模型结构\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 参数管理 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.2.1\n",
    "\n",
    "使用`NestMLP`模型，访问各个层的参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;引用上5.1节中的`NestMLP`模型，可以使用以下代码访问该模型各个层的参数，输出结果将显示每个层对应的参数名称、形状和具体参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "访问net层的参数\n",
      "参数名称: 0.weight, 形状:torch.Size([64, 20])\n",
      "参数名称: 0.bias, 形状:torch.Size([64])\n",
      "参数名称: 2.weight, 形状:torch.Size([32, 64])\n",
      "参数名称: 2.bias, 形状:torch.Size([32])\n",
      "\n",
      "访问linear层的参数\n",
      "参数名称: weight, 形状:torch.Size([16, 32])\n",
      "参数名称: bias, 形状:torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class NestMLP(nn.Module):\n",
    "    # 5.1节中的NestMLP模型\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n",
    "                                 nn.Linear(64, 32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "model = NestMLP()\n",
    "\n",
    "# 访问net层的参数\n",
    "print('访问net层的参数')\n",
    "for name, param in model.net.named_parameters():\n",
    "    print(f\"参数名称: {name}, 形状:{param.shape}\") # 打印具体参数值所占页面空间较大，读者可按照下面注释的代码自行打印查看。\n",
    "    # print(f\"参数名称: {name}, 形状:{param.shape}, 具体参数值: {param}\")\n",
    "\n",
    "# 访问linear层的参数\n",
    "print('\\n访问linear层的参数')\n",
    "for name, param in model.linear.named_parameters():\n",
    "    print(f\"参数名称: {name}, 形状:{param.shape}\") # 打印具体参数值所占页面空间较大，读者可按照下面注释的代码自行打印查看。\n",
    "    # print(f\"参数名称: {name}, 形状:{param.shape}, 具体参数值: {param}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.2.2\n",
    "\n",
    "查看初始化模块文档以了解不同的初始化方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;通过查看深度学习框架文档，有以下初始化方法 （参考链接：https://pytorch.org/docs/stable/nn.init.html ）\n",
    "- `torch.nn.init.uniform_(tensor, a=0.0, b=1.0)`：以均匀分布$U(a,b)$初始化输入张量。\n",
    "\n",
    "- `torch.nn.init.normal_(tensor, mean=0.0, std=1.0)`：以正态分布$N(mean, std^2)$初始化输入张量。\n",
    "\n",
    "- `torch.nn.init.constant_(tensor, val)`：以一确定数值初始化输入张量。\n",
    "\n",
    "- `torch.nn.init.ones_(tensor)`：用标量值 1 初始化输入张量。\n",
    "\n",
    "- `torch.nn.init.zeros_(tensor)`：用标量值 0 初始化输入张量。\n",
    "\n",
    "- `torch.nn.init.eye_(tensor)`：用单位矩阵初始化二维输入张量。\n",
    "\n",
    "- `torch.nn.init.xavier_uniform_(tensor, gain=1.0)`：从均匀分布$U(−a, a)$中采样，初始化输入张量，其中$a$的值由如下公式确定\n",
    "\n",
    "  $$a= gain * \\sqrt{\\frac{6}{fan_{in}+fan_{out}}}$$\n",
    "  \n",
    "  其中$gain$的取值如下表所示\n",
    "<style> table { margin: auto;} </style>\n",
    "非线性函数 | gain值\n",
    ":----:|:----:\n",
    "Linear/Identity | 1\n",
    "Conv1D | 1\n",
    "Conv2D | 1\n",
    "Conv3D | 1\n",
    "Sigmoid | 1\n",
    "Tanh | $\\displaystyle\\frac{5}{3}$\n",
    "ReLU | $\\sqrt{2}$\n",
    "Leaky ReLU | $$\\sqrt{\\frac{2}{1+negative\\_slope^2}}$$\n",
    "SELU | 1 (adaptive)\n",
    "\n",
    "- `torch.nn.init.xavier_normal_(tensor, gain=1.0)`:从正态分布$N(0,std^2)$中采样，初始化输入张量，其中$std$值由下式确定：\n",
    "\n",
    "  $$std= gain * \\sqrt{\\frac{2}{fan_{in}+fan_{out}}}$$\n",
    "\n",
    "- `torch.nn.init.kaiming_uniform_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')`:服从均匀分布$U(−bound, bound)$，其中$bound$值由下式确定\n",
    "\n",
    "  $$bound= gain * \\sqrt{\\frac{3}{fan_{mode}}}$$\n",
    "\n",
    "- `torch.nn.init.kaiming_normal_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')`:服从从正态分布$N(0,std^2)$中采样，其中$std$值由下式确定\n",
    "\n",
    "  $$std= \\frac{gain}{\\sqrt{fan_{mode}}}$$\n",
    "  \n",
    "- `torch.nn.init.trunc_normal_(tensor, mean=0.0, std=1.0, a=- 2.0, b=2.0)`:用从截断的正态分布中提取的值初始化输入张量。这些值实际上是从正态分布 $N(mean, std^2)$中提取的。\n",
    "\n",
    "- `torch.nn.init.sparse_(tensor, sparsity, std=0.01)`：将 2D 输入张量初始化为稀疏矩阵，其中非零元素服从正态分布$N(0,0.01)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.2.3\n",
    "\n",
    "构建包含共享参数层的多层感知机并对其进行训练。在训练过程中，观察模型各层的参数和梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在训练过程中，我们每个epoch都打印了每层的参数和梯度。可以看到`shared_fc`层的参数和梯度都是相同的，因为它们共享同一个参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "参数名称: 0.weight, 具体参数值: tensor([[ 0.5397,  0.3243, -0.1528],\n",
      "        [ 0.5680,  0.0952,  0.1176]]), 参数梯度: tensor([[-0., 0., -0.],\n",
      "        [-0., 0., -0.]])\n",
      "参数名称: 0.bias, 具体参数值: tensor([-0.4685,  0.5419]), 参数梯度: tensor([0., 0.])\n",
      "参数名称: 2.weight, 具体参数值: tensor([[ 0.3473, -0.1984,  0.2330, -0.0156],\n",
      "        [ 0.1252,  0.3434,  0.1973, -0.1091]]), 参数梯度: tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0301, 0.1367, 0.1547]])\n",
      "参数名称: 2.bias, 具体参数值: tensor([-0.1597,  0.0911]), 参数梯度: tensor([0.0000, 0.3813])\n",
      "参数名称: 6.weight, 具体参数值: tensor([[-0.4973, -0.1707, -0.2476,  0.2728],\n",
      "        [ 0.4359, -0.2659,  0.2447,  0.4783]]), 参数梯度: tensor([[-0.0000, -0.1695, -0.4040, -0.2494],\n",
      "        [ 0.0000,  0.0453,  0.1081,  0.0667]])\n",
      "参数名称: 6.bias, 具体参数值: tensor([ 0.4901, -0.4670]), 参数梯度: tensor([-1.0651,  0.2849])\n",
      "Epoch: 0, Loss: 1.0691800117492676\n",
      "\n",
      "Epoch: 1\n",
      "参数名称: 0.weight, 具体参数值: tensor([[ 0.5397,  0.3243, -0.1528],\n",
      "        [ 0.5680,  0.0952,  0.1176]]), 参数梯度: tensor([[-0., 0., -0.],\n",
      "        [-0., 0., -0.]])\n",
      "参数名称: 0.bias, 具体参数值: tensor([-0.4685,  0.5419]), 参数梯度: tensor([0., 0.])\n",
      "参数名称: 2.weight, 具体参数值: tensor([[ 0.3473, -0.1984,  0.2330, -0.0156],\n",
      "        [ 0.1252,  0.3431,  0.1960, -0.1106]]), 参数梯度: tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0281, 0.1328, 0.1505]])\n",
      "参数名称: 2.bias, 具体参数值: tensor([-0.1597,  0.0873]), 参数梯度: tensor([0.0000, 0.3740])\n",
      "参数名称: 6.weight, 具体参数值: tensor([[-0.4973, -0.1691, -0.2437,  0.2753],\n",
      "        [ 0.4359, -0.2664,  0.2437,  0.4776]]), 参数梯度: tensor([[-0.0000, -0.1606, -0.3941, -0.2489],\n",
      "        [ 0.0000,  0.0432,  0.1060,  0.0669]])\n",
      "参数名称: 6.bias, 具体参数值: tensor([ 0.5006, -0.4699]), 参数梯度: tensor([-1.0544,  0.2835])\n",
      "Epoch: 1, Loss: 1.0465747117996216\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 模型参数\n",
    "input_size = 3\n",
    "hidden_size = 4\n",
    "output_size = 3\n",
    "lr = 0.01\n",
    "epochs = 2\n",
    "\n",
    "# 构建带有共享参数层的多层感知机\n",
    "shared_fc = nn.Linear(hidden_size, hidden_size)\n",
    "MLP = nn.Sequential(nn.Linear(input_size, hidden_size), nn.ReLU(),\n",
    "                    shared_fc, nn.ReLU(),\n",
    "                    shared_fc, nn.ReLU(),\n",
    "                    nn.Linear(hidden_size, output_size)\n",
    ")\n",
    "\n",
    "# 训练数据\n",
    "X = torch.randn(1, input_size)\n",
    "Y = torch.randn(1, output_size)\n",
    "# 优化器\n",
    "optimizer = optim.SGD(MLP.parameters(), lr=lr)\n",
    "# 训练模型\n",
    "for epoch in range(epochs):\n",
    "    # 前向传播和计算损失\n",
    "    Y_pred = MLP(X)\n",
    "    loss = nn.functional.mse_loss(Y_pred, Y)\n",
    "    # 反向传播和更新梯度\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # 打印每层的参数和梯度\n",
    "    print(f'Epoch: {epoch}')\n",
    "    for name, param in MLP.named_parameters():\n",
    "        print(f\"参数名称: {name}, 具体参数值: {param.data[:2]}, 参数梯度: {param.grad[:2]}\") # 为了节省页面空间，这里只打印了每个参数和梯度的前两维，读者可以自行调整查看维度\n",
    "    print(f'Epoch: {epoch}, Loss: {loss.item()}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出这个过程只有两个共享层的参数和梯度数值是一样的，我们直接对这两层结果进行进一步的确认。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "参数是否相同: tensor([[True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True]])\n",
      "参数是否同时变化: tensor([True, True, True, True])\n",
      "是否时同一个对象: True\n",
      "梯度是否相同: tensor([[True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True]])\n",
      "梯度是否同时变化: tensor([True, True, True, True])\n",
      "是否时同一个对象: True\n"
     ]
    }
   ],
   "source": [
    "# 检查参数是否相同\n",
    "print(f\"参数是否相同: {MLP[2].weight.data == MLP[4].weight.data}\") \n",
    "# 确保它们实际上是同一个对象，而不只是有相同的值\n",
    "MLP[2].weight.data[0, 0] = 100 \n",
    "print(f\"参数是否同时变化: {MLP[2].weight.data[0] == MLP[4].weight.data[0]}\") \n",
    "print(f\"是否时同一个对象: {MLP[2].weight.data.equal(MLP[4].weight.data)}\")\n",
    "\n",
    "\n",
    "# 检查参数是否相同\n",
    "print(f\"梯度是否相同: {MLP[2].weight.grad == MLP[4].weight.grad}\") \n",
    "# 确保它们实际上是同一个对象，而不只是有相同的值\n",
    "MLP[2].weight.grad[0, 0] = 100 \n",
    "print(f\"梯度是否同时变化: {MLP[2].weight.grad[0] == MLP[4].weight.grad[0]}\") \n",
    "print(f\"是否时同一个对象: {MLP[2].weight.grad.equal(MLP[4].weight.grad)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.2.4\n",
    "\n",
    "为什么共享参数是个好主意？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;1. 节约内存：共享参数可以减少模型中需要存储的参数数量，从而减少内存占用。\n",
    "\n",
    "&emsp;&emsp;2. 加速收敛：共享参数可以让模型更加稳定，加速收敛。\n",
    "\n",
    "&emsp;&emsp;3. 提高泛化能力：共享参数可以帮助模型更好地捕捉数据中的共性，提高模型的泛化能力。\n",
    "\n",
    "&emsp;&emsp;4. 加强模型的可解释性：共享参数可以让模型更加简洁明了，加强模型的可解释性。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 延后初始化 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.3.1 \n",
    "\n",
    "如果指定了第一层的输入尺寸，但没有指定后续层的尺寸，会发生什么？是否立即进行初始化？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如果指定了第一层的输入尺寸，但没有指定后续层的尺寸，可以正常运行。第一层会立即初始化，但其他层是直到数据第一次通过模型传递才会初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 数据未通过模型 ---\n",
      "第一层权重： Parameter containing:\n",
      "tensor([[-0.3413, -0.5182, -0.3627],\n",
      "        [ 0.2411, -0.4914,  0.0811],\n",
      "        [ 0.0203,  0.4713,  0.1287],\n",
      "        [-0.1224,  0.2020,  0.0286]], requires_grad=True)\n",
      "其它层的权重： <UninitializedParameter>\n",
      "--- 数据第一次通过模型 ---\n",
      "第一层权重： Parameter containing:\n",
      "tensor([[-0.3413, -0.5182, -0.3627],\n",
      "        [ 0.2411, -0.4914,  0.0811],\n",
      "        [ 0.0203,  0.4713,  0.1287],\n",
      "        [-0.1224,  0.2020,  0.0286]], requires_grad=True)\n",
      "其它层的权重： Parameter containing:\n",
      "tensor([[ 0.0142,  0.1109, -0.2118,  0.2039],\n",
      "        [-0.3138, -0.1928,  0.2217, -0.0317],\n",
      "        [ 0.3973,  0.2929,  0.0405,  0.3447],\n",
      "        [-0.0141,  0.0498, -0.4321, -0.2023],\n",
      "        [ 0.2127,  0.0106, -0.3949, -0.2392],\n",
      "        [-0.3214,  0.4515, -0.0378,  0.2473],\n",
      "        [-0.2836,  0.3446,  0.0734, -0.4093],\n",
      "        [ 0.4049,  0.1875,  0.0278,  0.4894]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\"\"\"延后初始化\"\"\"\n",
    "# 定义模型\n",
    "net = nn.Sequential(nn.Linear(3, 4), nn.ReLU(), nn.LazyLinear(8))\n",
    "# 尚未初始化\n",
    "\n",
    "print(\"--- 数据未通过模型 ---\")\n",
    "print(\"第一层权重：\", net[0].weight)\n",
    "print(\"其它层的权重：\", net[2].weight)\n",
    "\n",
    "X = torch.rand(2, 3)\n",
    "net(X)\n",
    "print(\"--- 数据第一次通过模型 ---\")\n",
    "print(\"第一层权重：\", net[0].weight)\n",
    "print(\"其它层的权重：\", net[2].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.3.2\n",
    "\n",
    "如果指定了不匹配的维度会发生什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如果指定了不匹配的维度，会由于矩阵乘法时维度不匹配而报错。在下面的代码中便指定了不匹配的维度。\n",
    "\n",
    "&emsp;&emsp;由于第一层 nn.Linear(20, 256) 的输入维度为 20，所以输入数据 X 的最后一维必须为 20 才能与该层的权重矩阵相乘。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mat1 and mat2 shapes cannot be multiplied (2x10 and 20x256)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 定义模型\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(20, 256), nn.ReLU(),\n",
    "    nn.LazyLinear(128), nn.ReLU(),\n",
    "    nn.LazyLinear(10))\n",
    "\n",
    "X = torch.rand(2, 10)\n",
    "try:\n",
    "    net(X)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.3.3 \n",
    "\n",
    "如果输入具有不同的维度，需要做什么？提示：查看参数绑定的相关内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如果输入维度比指定维度小，可以考虑使用 padding 填充；如果输入维度比指定维度大，可以考虑用 PCA 等降维方法，将维度降至指定维度。对于不同的维度，还可以添加一个额外的线性层，并将第一个线性层的权重与该层的权重绑定在一起。这样就可以解决维度不匹配的问题，并且保持模型的权重不变。注意，在下面的代码中，我们假设第一个线性层的偏置项为零，因此不需要对其进行参数绑定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mat1 and mat2 shapes cannot be multiplied (2x10 and 20x256)\n",
      "第一个线性层的维度：torch.Size([256, 20])\n",
      "额外的线性层的维度：torch.Size([256, 10])\n",
      "第一个线性层的新维度：torch.Size([256, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0278,  0.0480,  0.0217, -0.0622, -0.0822,  0.0875,  0.0086, -0.0151,\n",
       "          0.0935,  0.0059],\n",
       "        [-0.0331,  0.0946, -0.0093, -0.0583, -0.0625,  0.0261,  0.0248, -0.0805,\n",
       "          0.1055,  0.0188]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义模型\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(20, 256), nn.ReLU(),\n",
    "    nn.Linear(256, 128), nn.ReLU(),\n",
    "    nn.Linear(128, 10))\n",
    "\n",
    "X = torch.rand(2, 10)\n",
    "\n",
    "try:\n",
    "    net(X)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# 添加额外的线性层\n",
    "extra_layer = nn.Linear(10, 256)\n",
    "print(f\"第一个线性层的维度：{net[0].weight.shape}\")\n",
    "print(f\"额外的线性层的维度：{extra_layer.weight.shape}\")\n",
    "\n",
    "# 将第一个线性层与额外的线性层的权重进行绑定\n",
    "net[0].weight = extra_layer.weight\n",
    "print(f\"第一个线性层的新维度：{net[0].weight.shape}\")\n",
    "\n",
    "# 使用新的输入（维度为20）调用模型\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 自定义层 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.4.1 \n",
    "\n",
    "设计一个接受输入并计算张量降维的层，它返回$y_k = \\sum_{i, j} W_{ijk} x_i x_j$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这个公式表示一个线性变换，将输入张量$x$中所有可能的二元组$(x_i,x_j)$进行组合，并对它们进行加权求和。其中，$W_{ijk}$表示权重张量中第$i,j,k$个元素的值。具体而言，该公式计算了输入张量$x$中所有二元组$(x_i, x_j)$对应的特征向量$u_{ij}$：\n",
    "\n",
    "\n",
    "$$\n",
    "u_{ij} = x_i \\cdot x_j\n",
    "$$\n",
    "\n",
    "\n",
    "&emsp;&emsp;然后，根据权重张量$W$中的权重$W_{ijk}$，对所有特征向量$u_{ij}$进行线性组合，得到输出向量$y_k$为：\n",
    "\n",
    "\n",
    "$$\n",
    "y_k = \\sum_{i,j} W_{ijk} u_{ij} = \\sum_{i,j} W_{ijk} x_i x_j\n",
    "$$\n",
    "\n",
    "\n",
    "&emsp;&emsp;该操作可以被视为一种降维操作，将高维输入$x$映射到低维输出空间$y$中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TensorReduction(nn.Module):\n",
    "    def __init__(self, dim1, dim2):\n",
    "        super(TensorReduction, self).__init__()\n",
    "        # 定义一个可训练的权重参数，维度为(dim2, dim1, dim1)\n",
    "        self.weight = nn.Parameter(torch.rand(dim2, dim1, dim1))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # 初始化一个全零张量，大小为(X.shape[0], self.weight.shape[0])\n",
    "        Y = torch.zeros(X.shape[0], self.weight.shape[0])\n",
    "        for k in range(self.weight.shape[0]):\n",
    "            # 计算temp = X @ weight[k] @ X^T\n",
    "            temp = torch.matmul(X, self.weight[k]) @ X.T\n",
    "            # 取temp的对角线元素，存入Y[:, k]\n",
    "            Y[:, k] = temp.diagonal()\n",
    "        return Y\n",
    "\n",
    "# 创建一个TensorReduction层，dim1=10, dim2=5\n",
    "layer = TensorReduction(10, 5)\n",
    "# 创建一个大小为(2, 10)的张量X\n",
    "X = torch.rand(2, 10)\n",
    "# 对layer(X)进行前向传播，返回一个大小为(2, 5)的张量\n",
    "layer(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.4.2 \n",
    "\n",
    "设计一个返回输入数据的傅立叶系数前半部分的层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;根据[维基百科](https://en.wikipedia.org/wiki/Fourier_series)中:\n",
    "> &emsp;&emsp;傅里叶级数是将任意周期函数表示为一组正弦和余弦函数的无限级数的方法。假设$f(x)$是在区间$[-L,L]$中定义的一个函数，其周期为$2L$，则其傅里叶级数可表示为：\n",
    "> \n",
    "> $$f(x) = \\frac{a_0}{2} + \\sum_{n=1}^{\\infty} [a_n \\cos(\\frac{n\\pi x}{L}) + b_n \\sin(\\frac{n\\pi x}{L})]$$\n",
    "> \n",
    "> &emsp;&emsp;其中，系数$a_0, a_n$和$b_n$可以通过以下公式计算得出：\n",
    "> \n",
    "> $$a_0 = \\frac{1}{2L} \\int_{-L}^{L} f(x)dx$$\n",
    "> \n",
    "> $$a_n = \\frac{1}{L} \\int_{-L}^{L} f(x) \\cos(\\frac{n\\pi x}{L}) dx, n>0$$\n",
    "> \n",
    "> $$b_n = \\frac{1}{L} \\int_{-L}^{L} f(x) \\sin(\\frac{n\\pi x}{L}) dx, n>0$$\n",
    "> \n",
    "> &emsp;&emsp;系数$a_n$和$b_n$实际上是$f(x)$与$\\cos(\\frac{n\\pi x}{L})$和$\\sin(\\frac{n\\pi x}{L})$的内积，而$a_0$是$f(x)$平均值的一半。\n",
    "\n",
    "&emsp;&emsp;在`torch`中有相应的函数可以轻松的实现傅里叶级数，如下代码所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.6075+0.0000j, -0.6063+1.0520j],\n",
       "         [ 0.5813+0.0000j,  0.4290-0.5440j]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.fft as fft\n",
    "\n",
    "class FourierLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FourierLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 对输入的张量 x 进行快速傅里叶变换\n",
    "        x = fft.fftn(x)\n",
    "        # 取出第三个维度的前半部分，即去掉直流分量和镜像分量\n",
    "        x = x[:, :, :x.shape[2] // 2]\n",
    "        # 返回处理后的张量\n",
    "        return x\n",
    "\n",
    "# 创建一个随机数值为 [0, 1) 的形状为 (1, 2, 5) 的张量 X\n",
    "X = torch.rand(1, 2, 5)\n",
    "# 实例化一个 FourierLayer 的网络对象 net\n",
    "net = FourierLayer()\n",
    "# 将 X 输入到网络 net 中进行前向计算，并输出结果\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 读写文件 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  练习 5.5.1\n",
    "\n",
    "即使不需要将经过训练的模型部署到不同的设备上，存储模型参数还有什么实际的好处？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;1. 加速模型训练：存储模型参数可以避免每次重新训练模型时需要重复计算之前已经计算过的权重和偏置。\n",
    "\n",
    "&emsp;&emsp;2. 节省内存空间：保存模型参数比保存完整的模型文件更加节省内存空间，这在处理大型模型或使用内存受限设备时尤为重要。\n",
    "\n",
    "&emsp;&emsp;3. 便于共享和复现：存储模型参数可以方便地共享和复现已经训练好的模型，其他人可以直接加载这些参数并使用它们进行预测或微调。\n",
    "\n",
    "&emsp;&emsp;4. 便于调试和分析：通过检查模型参数，可以更容易地诊断模型中存在的问题，并对其进行调整和优化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.5.2\n",
    "\n",
    "假设我们只想复用网络的一部分，以将其合并到不同的网络架构中。比如想在一个新的网络中使用之前网络的前两层，该怎么做？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;使用保存模型某层参数的办法，保存网络的前两层，然后再加载到新的网络中使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        ...,\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class MLP(nn.Module):             # 定义 MLP 类\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)   # 定义隐藏层层，输入尺寸为 20，输出尺寸为 256\n",
    "        self.output = nn.Linear(256, 10)   # 定义输出层，输入尺寸为 256，输出尺寸为 10\n",
    "\n",
    "    def forward(self, x):          # 定义前向传播函数\n",
    "        return self.output(F.relu(self.hidden(x)))  # 使用 ReLU 激活函数，计算隐藏层和输出层的输出\n",
    "\n",
    "class MLP_new(nn.Module):             # 定义 MLP 类\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)   # 定义隐藏层层，输入尺寸为 20，输出尺寸为 256\n",
    "        self.output = nn.Linear(256, 10)   # 定义输出层，输入尺寸为 256，输出尺寸为 10\n",
    "\n",
    "    def forward(self, x):          # 定义前向传播函数\n",
    "        return self.output(F.relu(self.hidden(x)))  # 使用 ReLU 激活函数，计算隐藏层和输出层的输出\n",
    "\n",
    "net = MLP()                       # 创建 MLP 的实例\n",
    "torch.save(net.hidden.state_dict(), 'mlp.hidden.params')  # 将隐藏层的参数保存到文件中\n",
    "clone = MLP_new()                     # 创建另一个 MLP 的实例\n",
    "clone.hidden.load_state_dict(torch.load('mlp.hidden.params'))  # 加载已保存的参数到克隆实例的隐藏层中\n",
    "print(clone.hidden.weight == net.hidden.weight)  # 比较两个 MLP 实例的隐藏层权重是否相等，并输出结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.5.3\n",
    "\n",
    "如何同时保存网络架构和参数？需要对架构加上什么限制？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在PyTorch中，可以使用`torch.save()`函数同时保存网络架构和参数。为了保存网络架构，需要将模型的结构定义在一个Python类中，并将该类实例化为模型对象。此外，必须确保该类的构造函数不包含任何随机性质的操作，例如`dropout`层的随机丢弃率应该是固定的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('hidden.weight',\n",
       "              tensor([[ 0.2060,  0.0169,  0.0780,  ...,  0.0201,  0.1381,  0.0925],\n",
       "                      [-0.1499, -0.2105,  0.0278,  ..., -0.0910, -0.0794, -0.0869],\n",
       "                      [ 0.0719, -0.1977, -0.2139,  ...,  0.2095, -0.0749,  0.1598],\n",
       "                      ...,\n",
       "                      [-0.1494, -0.0726,  0.1377,  ...,  0.2142,  0.1904,  0.1849],\n",
       "                      [-0.1231,  0.0370,  0.1983,  ..., -0.0256,  0.1969,  0.0533],\n",
       "                      [-0.1931, -0.1350, -0.0889,  ...,  0.0144,  0.0857,  0.0599]])),\n",
       "             ('hidden.bias',\n",
       "              tensor([ 0.1722,  0.2041, -0.1814,  0.0245, -0.1959, -0.1975, -0.1380, -0.0278,\n",
       "                       0.1295,  0.2230,  0.0138, -0.2027,  0.1607,  0.1151,  0.0499,  0.1869,\n",
       "                      -0.0012, -0.0401, -0.0526,  0.1953, -0.1076,  0.1268, -0.1275,  0.0566,\n",
       "                       0.1308, -0.1600,  0.0856,  0.1625, -0.0309, -0.0318,  0.0066,  0.1644,\n",
       "                       0.2228,  0.0217,  0.0786, -0.1837, -0.2075,  0.0751,  0.1296, -0.1117,\n",
       "                      -0.0271,  0.1353,  0.1735,  0.0563, -0.1702, -0.0309, -0.0729,  0.0210,\n",
       "                       0.0727, -0.0246, -0.2048,  0.0315,  0.0699, -0.1366, -0.1123, -0.0640,\n",
       "                       0.1667,  0.1426, -0.0723,  0.1246, -0.1875,  0.0593, -0.1419, -0.1433,\n",
       "                       0.0497, -0.0637,  0.1687, -0.0956,  0.0208, -0.1503,  0.0558, -0.1447,\n",
       "                      -0.1510, -0.1422,  0.1493,  0.0179, -0.0435,  0.1349, -0.1164, -0.0182,\n",
       "                      -0.1454,  0.0274,  0.1106,  0.0478, -0.0079,  0.0961, -0.0842, -0.2174,\n",
       "                      -0.1850,  0.1433, -0.1157,  0.1617,  0.0899,  0.1835,  0.1561, -0.1290,\n",
       "                       0.1306, -0.0515, -0.1401, -0.1542, -0.2223, -0.0782,  0.1827,  0.1523,\n",
       "                      -0.2125, -0.1440,  0.0162, -0.1768, -0.0189,  0.0053, -0.0280, -0.0210,\n",
       "                       0.2056, -0.0512,  0.0223, -0.0313,  0.1369,  0.0532,  0.1762, -0.0098,\n",
       "                      -0.1562, -0.1783, -0.0817, -0.0486,  0.0468, -0.1811,  0.1288,  0.0998,\n",
       "                       0.1569, -0.0643,  0.0402,  0.1598, -0.0921,  0.2195, -0.0329,  0.0618,\n",
       "                      -0.1934,  0.0618,  0.0867,  0.0273, -0.0878,  0.1578,  0.2067,  0.1666,\n",
       "                       0.1527, -0.0194,  0.1139, -0.0886, -0.2008,  0.1218,  0.0546,  0.0508,\n",
       "                      -0.0559,  0.1081,  0.1056, -0.0307,  0.0143, -0.2040,  0.0937,  0.0126,\n",
       "                      -0.1892, -0.0327, -0.0895, -0.1676,  0.0840, -0.2233,  0.0836,  0.0584,\n",
       "                       0.0477,  0.0140,  0.1190,  0.1716,  0.1288,  0.0832, -0.0140,  0.0495,\n",
       "                       0.0067,  0.1947,  0.2208,  0.1525, -0.0590,  0.2210, -0.1276,  0.2032,\n",
       "                       0.0533, -0.1583, -0.0676,  0.1895,  0.1252,  0.2080, -0.1390,  0.1188,\n",
       "                       0.1401, -0.2153, -0.1352, -0.0812, -0.1216,  0.1678,  0.1835,  0.1223,\n",
       "                       0.0734, -0.1577, -0.1111,  0.0928, -0.1451, -0.0313,  0.1135, -0.1309,\n",
       "                       0.1672, -0.1148,  0.2163,  0.2177,  0.0580, -0.0733, -0.1464,  0.2130,\n",
       "                       0.1123,  0.1957, -0.0623,  0.0541, -0.1842,  0.0136, -0.1442, -0.1137,\n",
       "                       0.1694, -0.0503, -0.2213,  0.0203,  0.2172,  0.2138,  0.0742,  0.0061,\n",
       "                       0.0402, -0.1414,  0.1631,  0.1699,  0.0945, -0.2167,  0.0705, -0.0046,\n",
       "                      -0.1239,  0.1637,  0.0474, -0.0347,  0.1746, -0.0585,  0.0537,  0.1811,\n",
       "                      -0.1139,  0.1605,  0.1827,  0.1609,  0.1603,  0.0499,  0.1106,  0.0616])),\n",
       "             ('output.weight',\n",
       "              tensor([[ 0.0371,  0.0239,  0.0280,  ...,  0.0341,  0.0575,  0.0118],\n",
       "                      [-0.0555,  0.0512, -0.0499,  ...,  0.0210,  0.0453, -0.0538],\n",
       "                      [ 0.0343,  0.0172, -0.0488,  ...,  0.0422, -0.0361,  0.0009],\n",
       "                      ...,\n",
       "                      [ 0.0152,  0.0496,  0.0168,  ..., -0.0239,  0.0582, -0.0501],\n",
       "                      [ 0.0115,  0.0052, -0.0013,  ...,  0.0335,  0.0360, -0.0456],\n",
       "                      [-0.0374, -0.0159, -0.0364,  ..., -0.0470, -0.0278,  0.0152]])),\n",
       "             ('output.bias',\n",
       "              tensor([ 0.0273,  0.0064,  0.0048, -0.0142, -0.0079,  0.0010, -0.0141,  0.0285,\n",
       "                       0.0147,  0.0526]))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class MLP(nn.Module):             # 定义 MLP 类\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)   # 定义隐藏层层，输入尺寸为 20，输出尺寸为 256\n",
    "        self.output = nn.Linear(256, 10)   # 定义输出层，输入尺寸为 256，输出尺寸为 10\n",
    "\n",
    "    def forward(self, x):          # 定义前向传播函数\n",
    "        return self.output(F.relu(self.hidden(x)))  # 使用 ReLU 激活函数，计算隐藏层和输出层的输出\n",
    "\n",
    "net = MLP()\n",
    "\n",
    "# 存储模型\n",
    "torch.save(net.state_dict(), 'model.pt')\n",
    "\n",
    "# 导入模型\n",
    "model = torch.load('model.pt')\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 GPU "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.6.1 \n",
    "\n",
    "尝试一个计算量更大的任务，比如大矩阵的乘法，看看CPU和GPU之间的速度差异。再试一个计算量很小的任务呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;计算量很大的任务：使用GPU速度明显更快。\n",
    "\n",
    "&emsp;&emsp;计算量很小的任务：CPU速度可能更快，因为数据传输到GPU需要时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu time cost: 2131.88ms\n",
      "gpu time cost: 37.21ms\n",
      "cpu time cost: 0ms\n",
      "gpu time cost: 0ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "# 计算量较大的任务\n",
    "X = torch.rand((10000, 10000))\n",
    "Y = X.cuda(0)\n",
    "time_start = time.time()\n",
    "Z = torch.mm(X, X)\n",
    "time_end = time.time()\n",
    "print(f'cpu time cost: {round((time_end - time_start) * 1000, 2)}ms')\n",
    "time_start = time.time()\n",
    "Z = torch.mm(Y, Y)\n",
    "time_end = time.time()\n",
    "print(f'gpu time cost: {round((time_end - time_start) * 1000, 2)}ms')\n",
    "\n",
    "# 计算量很小的任务\n",
    "X = torch.rand((100, 100))\n",
    "Y = X.cuda(0)\n",
    "time_start = time.time()\n",
    "Z = torch.mm(X, X)\n",
    "time_end = time.time()\n",
    "print(f'cpu time cost: {round((time_end - time_start) * 1000)}ms')\n",
    "time_start = time.time()\n",
    "Z = torch.mm(Y, Y)\n",
    "time_end = time.time()\n",
    "print(f'gpu time cost: {round((time_end - time_start) * 1000)}ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.6.2\n",
    "\n",
    "我们应该如何在GPU上读写模型参数？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;使用`net.to(device)`将模型迁移到GPU上，然后再按照之前的方法读写参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('hidden.weight',\n",
       "              tensor([[-0.0496,  0.1606,  0.0861,  ..., -0.1966, -0.1275, -0.1557],\n",
       "                      [ 0.0648,  0.1355,  0.1571,  ...,  0.0767,  0.0710, -0.1160],\n",
       "                      [-0.1992,  0.1369,  0.1053,  ...,  0.1187,  0.0040, -0.0608],\n",
       "                      ...,\n",
       "                      [-0.0032, -0.1003,  0.1094,  ...,  0.2188, -0.1616, -0.1339],\n",
       "                      [-0.0660, -0.0040,  0.0260,  ..., -0.0386,  0.1124, -0.1249],\n",
       "                      [-0.1442,  0.0856,  0.0353,  ...,  0.0307, -0.0522, -0.0307]],\n",
       "                     device='cuda:0')),\n",
       "             ('hidden.bias',\n",
       "              tensor([ 0.0323,  0.1698, -0.0760, -0.0415,  0.0097, -0.1557, -0.1857,  0.1272,\n",
       "                      -0.0262,  0.1571,  0.1617, -0.1123, -0.0720, -0.0987,  0.1544, -0.0892,\n",
       "                      -0.0234,  0.0718, -0.1665, -0.1429, -0.0378, -0.1961,  0.1036, -0.1972,\n",
       "                       0.0961,  0.0816, -0.1794, -0.0917, -0.2164,  0.1339,  0.0810,  0.0642,\n",
       "                      -0.1230,  0.1881, -0.0346,  0.1958, -0.0152,  0.1887,  0.1905,  0.1154,\n",
       "                      -0.0053, -0.1754,  0.2168, -0.1020,  0.1161, -0.0368, -0.0822,  0.0270,\n",
       "                       0.1661, -0.0590, -0.1149,  0.0487, -0.1592, -0.0495, -0.1059, -0.1404,\n",
       "                      -0.0086,  0.0545, -0.0401, -0.0636, -0.0316,  0.2224, -0.0911,  0.0101,\n",
       "                      -0.2084,  0.1496, -0.0492, -0.0329,  0.1506,  0.0272,  0.0734,  0.0872,\n",
       "                      -0.1252, -0.1886, -0.1663, -0.1509, -0.0790, -0.0619,  0.2152,  0.0293,\n",
       "                       0.0296,  0.1225, -0.2069, -0.2106,  0.2031, -0.2105,  0.0124,  0.0181,\n",
       "                      -0.0746,  0.0093, -0.1495, -0.0036, -0.0870, -0.1478, -0.1869,  0.0366,\n",
       "                      -0.0345,  0.0802, -0.0703,  0.0356, -0.0354,  0.0823,  0.2207, -0.1026,\n",
       "                       0.1786, -0.0630,  0.0896, -0.0671,  0.0333,  0.1873, -0.1729,  0.1177,\n",
       "                      -0.0685, -0.0448,  0.2070, -0.0474, -0.0343,  0.0947,  0.1001,  0.0653,\n",
       "                       0.0947, -0.0065, -0.1825,  0.2085, -0.1518, -0.1215,  0.1688, -0.0772,\n",
       "                       0.0257,  0.0376, -0.0080, -0.1920, -0.0579, -0.0692,  0.0619, -0.1578,\n",
       "                       0.1606,  0.0242, -0.1404, -0.1846, -0.0341,  0.0213, -0.0282, -0.1251,\n",
       "                      -0.1161, -0.0629,  0.0235,  0.0533, -0.0188,  0.0602, -0.1736,  0.0156,\n",
       "                      -0.0930, -0.1305,  0.1849,  0.0859, -0.0808,  0.1100,  0.0949, -0.1499,\n",
       "                       0.0252, -0.1040, -0.2043, -0.0705,  0.0488,  0.1087,  0.0507,  0.0984,\n",
       "                       0.0364, -0.0791, -0.1775,  0.0255,  0.2127, -0.0093, -0.0017,  0.2152,\n",
       "                       0.2138,  0.1405,  0.1015,  0.1655,  0.0864, -0.1920, -0.0168, -0.2198,\n",
       "                      -0.0594, -0.1798,  0.0520, -0.0830, -0.1924, -0.0648, -0.2174,  0.0514,\n",
       "                      -0.0220,  0.2049, -0.0120, -0.0786,  0.1952, -0.0887, -0.1613,  0.1394,\n",
       "                      -0.1639,  0.0319, -0.1120, -0.1164, -0.1697,  0.1665,  0.0358,  0.2181,\n",
       "                       0.0647, -0.1278,  0.2098, -0.0392, -0.0891, -0.0444,  0.1376,  0.0267,\n",
       "                      -0.0362, -0.1295, -0.0505, -0.2216,  0.2223,  0.0353, -0.1562, -0.0613,\n",
       "                      -0.1836,  0.1902, -0.0750, -0.0367, -0.0439,  0.1303, -0.0208, -0.1511,\n",
       "                       0.1436,  0.1433, -0.0654,  0.0350,  0.1071, -0.1644,  0.0881,  0.0351,\n",
       "                       0.0052, -0.2115, -0.1002, -0.1153, -0.0167, -0.1891,  0.0454,  0.1599,\n",
       "                       0.1127, -0.0027, -0.1774,  0.0662, -0.1886, -0.0646,  0.0421, -0.1051],\n",
       "                     device='cuda:0')),\n",
       "             ('output.weight',\n",
       "              tensor([[-0.0094, -0.0008, -0.0012,  ...,  0.0492, -0.0602,  0.0564],\n",
       "                      [-0.0622, -0.0028,  0.0406,  ..., -0.0479,  0.0416, -0.0018],\n",
       "                      [ 0.0235, -0.0539, -0.0405,  ..., -0.0079, -0.0073,  0.0544],\n",
       "                      ...,\n",
       "                      [ 0.0445, -0.0036,  0.0250,  ...,  0.0541,  0.0049, -0.0177],\n",
       "                      [ 0.0333,  0.0404,  0.0544,  ...,  0.0022,  0.0534, -0.0161],\n",
       "                      [-0.0198, -0.0559, -0.0306,  ...,  0.0173,  0.0218,  0.0523]],\n",
       "                     device='cuda:0')),\n",
       "             ('output.bias',\n",
       "              tensor([ 0.0528,  0.0130, -0.0609, -0.0217,  0.0383, -0.0024, -0.0217, -0.0505,\n",
       "                      -0.0371,  0.0048], device='cuda:0'))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class MLP(nn.Module):             # 定义 MLP 类\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)   # 定义隐藏层层，输入尺寸为 20，输出尺寸为 256\n",
    "        self.output = nn.Linear(256, 10)   # 定义输出层，输入尺寸为 256，输出尺寸为 10\n",
    "\n",
    "    def forward(self, x):          # 定义前向传播函数\n",
    "        return self.output(F.relu(self.hidden(x)))  # 使用 ReLU 激活函数，计算隐藏层和输出层的输出\n",
    "\n",
    "# 选择GPU，没有GPU就选CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 创建模型实例对象\n",
    "net = MLP()\n",
    "# 将模型参数传输到GPU上\n",
    "net.to(device)\n",
    "# 访问模型参数\n",
    "net.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.6.3 \n",
    "\n",
    "测量计算1000个$100 \\times 100$矩阵的矩阵乘法所需的时间，并记录输出矩阵的Frobenius范数，一次记录一个结果，而不是在GPU上保存日志并仅传输最终结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;中文版翻译有点问题，英文原版这句话是：\n",
    "\n",
    ">Measure the time it takes to compute 1000 matrix-matrix multiplications of $100×100$ matrices and log the Frobenius norm of the output matrix one result at a time vs. keeping a log on the GPU and transferring only the final result.\n",
    "\n",
    "&emsp;&emsp;所以这道题的本质还是希望我们做个比较。\n",
    "\n",
    "&emsp;&emsp;实验一：仅记录1000次$100×100$矩阵相乘所用的时间，不需要打印Frobenius范数。\n",
    "\n",
    "&emsp;&emsp;实验二：记录1000次$100×100$矩阵相乘所用的时间，并打印Frobenius范数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.1120142936706543\n",
      "Time taken: 0.09748530387878418\n",
      "实验一消耗时间：0.1120142936706543，实验二消耗时间：0.09748530387878418\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 生成随机矩阵\n",
    "matrices = [torch.randn(100, 100).to(device) for i in range(1000)]\n",
    "\n",
    "# 实验一：计算时间\n",
    "start_time_1 = time.time()\n",
    "for i in range(1000):\n",
    "    result = torch.mm(matrices[i], matrices[i].t())\n",
    "    frobenius_norm = torch.norm(result)\n",
    "#     print(frobenius_norm)\n",
    "end_time_1 = time.time()\n",
    "print(\"Time taken:\", end_time_1 - start_time_1)\n",
    "\n",
    "# 实验二：计算时间\n",
    "start_time_2 = time.time()\n",
    "for i in range(1000):\n",
    "    result = torch.mm(matrices[i], matrices[i].t())\n",
    "    frobenius_norm = torch.norm(result)\n",
    "    # print(frobenius_norm)\n",
    "end_time_2 = time.time()\n",
    "print(\"Time taken:\", end_time_2 - start_time_2)\n",
    "\n",
    "print(f'实验一消耗时间：{end_time_1 - start_time_1}，实验二消耗时间：{end_time_2 - start_time_2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.6.4\n",
    "\n",
    "测量同时在两个GPU上执行两个矩阵乘法与在一个GPU上按顺序执行两个矩阵乘法所需的时间。提示：应该看到近乎线性的缩放。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;执行两个矩阵乘法并行在两个GPU上所需的时间通常会比在单个GPU上按顺序执行这两个操作要快得多。但实际的时间取决于矩阵的大小、硬件配置和算法实现。\n",
    "\n",
    "&emsp;&emsp;但由于笔者只有一张卡，所以只做了在单个GPU上顺序执行两个矩阵乘法的实验。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential time: 0.00903034 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "# 创建两个随机矩阵\n",
    "a = torch.randn(10000, 10000).cuda()\n",
    "b = torch.randn(10000, 10000).cuda()\n",
    "\n",
    "# 顺序计算\n",
    "start_time = time.time()\n",
    "c1 = torch.matmul(a, b)\n",
    "c2 = torch.matmul(a, b)\n",
    "end_time = time.time()\n",
    "sequential_time = end_time - start_time\n",
    "\n",
    "print(f\"Sequential time: {sequential_time:.8f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
