{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 第5章 深度学习计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 层和块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.1.1 \n",
    "\n",
    "如果将MySequential中存储块的方式更改为Python列表，会出现什么样的问题？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如果将MySequential中存储块的方式从OrderedDict更改为Python列表，代码可以正常计算，但并没有注册给Module。无法像`_modules`一样使用很多内置方完成已经实现的功能。如无法通过`net.state_dict()`访问模型的网络结构和参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class MySequential(nn.Module):\n",
    "    # 使用OrderedDict存储块的类\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员\n",
    "            # 变量_modules中。_module的类型是OrderedDict\n",
    "            self._modules[str(idx)] = module\n",
    "\n",
    "    def forward(self, X):\n",
    "        # OrderedDict保证了按照成员添加的顺序遍历它们\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X\n",
    "\n",
    "class MySequential_list(nn.Module):\n",
    "    # 使用list存储块的类\n",
    "    def __init__(self, *args):\n",
    "        super(MySequential_list, self).__init__()\n",
    "        self.sequential = []\n",
    "        for module in args:\n",
    "            self.sequential.append(module)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for module in self.sequential:\n",
    "            X = module(X)\n",
    "        return X\n",
    "\n",
    "\n",
    "X = torch.rand(1,10)\n",
    "# 初始化两个block，确保传给MySequential和MySequential_list的是一样的参数。消除nn.Linear初始化时生成随机参数对结果的影响。\n",
    "block1 = nn.Linear(10, 20)\n",
    "block2 = nn.Linear(20, 10)\n",
    "net = MySequential(block1, nn.ReLU(), block2)\n",
    "net_list = MySequential_list(block1, nn.ReLU(), block2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;对比两种方式的结果，可以发现输出完全一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "采用python OrderedDict的方式存储 :\n",
      "tensor([[ 0.0546,  0.3916,  0.2864,  0.2916,  0.1270, -0.0430,  0.4110, -0.4329,\n",
      "          0.1405,  0.1050]], grad_fn=<AddmmBackward>)\n",
      "\n",
      "\n",
      "采用python列表的方式存储 :\n",
      "tensor([[ 0.0546,  0.3916,  0.2864,  0.2916,  0.1270, -0.0430,  0.4110, -0.4329,\n",
      "          0.1405,  0.1050]], grad_fn=<AddmmBackward>)\n",
      "\n",
      "\n",
      "两种方式的计算结果是否一致： True\n"
     ]
    }
   ],
   "source": [
    "net_y = net(X)\n",
    "net_list_y = net_list(X)\n",
    "\n",
    "print(\"采用python OrderedDict的方式存储 :\", net_y, sep='\\n')\n",
    "print(\"\\n\")\n",
    "print(\"采用python列表的方式存储 :\", net_list_y, sep='\\n')\n",
    "print(\"\\n\")\n",
    "print(\"两种方式的计算结果是否一致：\", net_y.equal(net_list_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;但是在查看模型结构和参数时存在差异。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "采用python OrderedDict的方式存储 :\n",
      "\n",
      "网络结构 : \n",
      " MySequential(\n",
      "  (0): Linear(in_features=10, out_features=20, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=20, out_features=10, bias=True)\n",
      ")\n",
      "参数 ：\n",
      "0.weight tensor([[-0.1253, -0.1309,  0.1625, -0.3000, -0.1993, -0.0035,  0.2483,  0.2356,\n",
      "          0.0647, -0.2197],\n",
      "        [-0.2451, -0.2869, -0.1658, -0.2204, -0.2124,  0.2028,  0.2128,  0.2117,\n",
      "         -0.0885, -0.0396]])\n",
      "0.bias tensor([0.2100, 0.2286])\n",
      "2.weight tensor([[-0.2138, -0.1808,  0.0393, -0.1733,  0.0933, -0.0221, -0.1919,  0.2134,\n",
      "         -0.1666,  0.0861,  0.0637,  0.0338, -0.1072, -0.0741,  0.1303, -0.0655,\n",
      "         -0.0905, -0.0866, -0.0166,  0.1838],\n",
      "        [ 0.1529, -0.0978,  0.1443, -0.0516, -0.0208, -0.0474, -0.0085,  0.1433,\n",
      "          0.1790,  0.0188, -0.0433,  0.1901,  0.0994, -0.1200, -0.0869, -0.1423,\n",
      "         -0.0362,  0.2152,  0.1582,  0.2118]])\n",
      "2.bias tensor([0.0355, 0.2129])\n",
      "\n",
      "\n",
      "采用python列表的方式存储 :\n",
      "\n",
      "网络结构 : \n",
      " MySequential(\n",
      "  (0): Linear(in_features=10, out_features=20, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=20, out_features=10, bias=True)\n",
      ")\n",
      "参数： OrderedDict()\n"
     ]
    }
   ],
   "source": [
    "# 使用_modules方便打印net的网络结构和参数，而list则无法做到\n",
    "print(\"采用python OrderedDict的方式存储 :\\n\")\n",
    "print(\"网络结构 : \\n\", net)\n",
    "# 为了可视化效果简洁，这里只展示了每个参数的部分值\n",
    "print(\"参数 ：\", \"\\n\".join([str(k) + str(\" \") + str(v[:2]) for k,v in net.state_dict().items()]), sep='\\n')\n",
    "print(\"\\n\")\n",
    "print(\"采用python列表的方式存储 :\\n\")\n",
    "print(\"网络结构 : \\n\", net) \n",
    "print(\"参数：\", net_list.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.1.2\n",
    "\n",
    "实现一个块，它以两个块为参数，例如net1和net2，并返回前向传播中两个网络的并联输出。这也被称为并行块。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在本书7.4节中`GoogleLet`模型中的`Inception`块使用了并行块技术。\n",
    "下面代码实现了一个并行网络，由两个子网络组成。输入数据先分别经过两个子网络的计算，分别得到两个部分的输出结果，然后在通道维度上合并结果得到最终输出。\n",
    "\n",
    "&emsp;&emsp;其中，`net1`和`net2`分别表示两个子网络，`torch.cat`在指定维度上拼接张量。输出结果的大小为`torch.Size([2, 36])`，其中第一个维度表示batch_size为2，第二个维度表示输出特征图的通道数为36，正好为两个模型的输出特征图通道数之和（12+24=36）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型结构 :\n",
      "Parallel(\n",
      "  (net1): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=12, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (net2): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=24, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      ")\n",
      "模型输出的形状 : torch.Size([2, 36])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class Parallel(nn.Module):\n",
    "    # 自定义并行块\n",
    "    def __init__(self, net1, net2):\n",
    "        super().__init__()\n",
    "        self.net1=net1 # 第一个子网络\n",
    "        self.net2=net2 # 第二个子网络\n",
    "        \n",
    "    def forward(self,X):\n",
    "        x1= self.net1(X) # 第一个子网络的输出\n",
    "        x2= self.net2(X) # 第二个子网络的输出\n",
    "        return torch.cat((x1,x2),dim=1) # 在通道维度上合并输出结果\n",
    "      \n",
    "X = torch.rand(2,10) # 输入数据\n",
    "net = Parallel(nn.Sequential(nn.Linear(10,12),nn.ReLU()), nn.Sequential(nn.Linear(10,24),nn.ReLU())) # 实例化并行网络\n",
    "output = net(X)\n",
    "print(\"模型结构 :\", net, sep=\"\\n\") # 查看模型结构\n",
    "print(\"模型输出的形状 :\", output.size()) # 输出结果的大小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.1.3\n",
    "\n",
    "假设我们想要连接同一网络的多个实例。实现一个函数，该函数生成同一个块的多个实例，并在此基础上构建更大的网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;下面代码定义了一个函数`create_network`，该函数接受四个参数：`num_instances`、`input_size`、`hidden_size` 和 `output_size`，并返回一个Sequential模型。\n",
    "\n",
    "&emsp;&emsp;其中，该网络模型首先包含 `num_instances` 个相同的线性层，每个线性层有两个子层：一个输入维度为 `input_size`，输出维度为 `hidden_size` 的全连接层，和一个 ReLU 非线性激活层。然后，这 `hidden_size` 个线性层连接在一起作为整个网络的前馈部分。最后，额外添加一个输出层，其输入维度为 `input_size`，输出维度为 `output_size`。\n",
    "\n",
    "&emsp;&emsp;因此，最终的网络结构是由 `output_size` 个相同的线性层组成的前馈神经网络，每个线性层内部包含一个全连接层和一个ReLU激活层，以及一个独立的输出层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=5, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=5, out_features=10, bias=True)\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=5, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=5, out_features=10, bias=True)\n",
      "  )\n",
      "  (2): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=5, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=5, out_features=10, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=10, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "def create_network(num_instances, input_size, hidden_size, output_size):\n",
    "    # 创建一个线性层\n",
    "    linear_layer = nn.Sequential(\n",
    "        nn.Linear(input_size, hidden_size), nn.ReLU(),\n",
    "        nn.Linear(hidden_size, input_size)\n",
    "    )\n",
    "    \n",
    "    # 创建多个相同结构的实例并连接\n",
    "    instances = [linear_layer for _ in range(num_instances)]\n",
    "    network = nn.Sequential(*instances)\n",
    "    \n",
    "    # 添加输出层\n",
    "    output_layer = nn.Linear(input_size, output_size)\n",
    "    network.add_module(\"output\", output_layer)\n",
    "    \n",
    "    return network\n",
    "\n",
    "# 模型参数\n",
    "num_instances=3\n",
    "input_size=10\n",
    "hidden_size=5\n",
    "output_size=2\n",
    "\n",
    "# 示例用法\n",
    "net = create_network(num_instances=num_instances, input_size=input_size, \n",
    "                     hidden_size=hidden_size, output_size=output_size) # 重复3次相同的线形层\n",
    "\n",
    "# 查看模型结构\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 参数管理 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.2.1\n",
    "\n",
    "使用`NestMLP`模型，访问各个层的参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;引用上5.1节中的`NestMLP`模型，可以使用以下代码访问该模型各个层的参数，输出结果将显示每个层对应的参数名称、形状和具体参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "访问net层的参数\n",
      "参数名称: 0.weight, 形状:torch.Size([64, 20])\n",
      "参数名称: 0.bias, 形状:torch.Size([64])\n",
      "参数名称: 2.weight, 形状:torch.Size([32, 64])\n",
      "参数名称: 2.bias, 形状:torch.Size([32])\n",
      "\n",
      "访问linear层的参数\n",
      "参数名称: weight, 形状:torch.Size([16, 32])\n",
      "参数名称: bias, 形状:torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class NestMLP(nn.Module):\n",
    "    # 5.1节中的NestMLP模型\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n",
    "                                 nn.Linear(64, 32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "model = NestMLP()\n",
    "\n",
    "# 访问net层的参数\n",
    "print('访问net层的参数')\n",
    "for name, param in model.net.named_parameters():\n",
    "    print(f\"参数名称: {name}, 形状:{param.shape}\") # 打印具体参数值所占页面空间较大，读者可按照下面注释的代码自行打印查看。\n",
    "    # print(f\"参数名称: {name}, 形状:{param.shape}, 具体参数值: {param}\")\n",
    "\n",
    "# 访问linear层的参数\n",
    "print('\\n访问linear层的参数')\n",
    "for name, param in model.linear.named_parameters():\n",
    "    print(f\"参数名称: {name}, 形状:{param.shape}\") # 打印具体参数值所占页面空间较大，读者可按照下面注释的代码自行打印查看。\n",
    "    # print(f\"参数名称: {name}, 形状:{param.shape}, 具体参数值: {param}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.2.2\n",
    "\n",
    "查看初始化模块文档以了解不同的初始化方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;通过查看深度学习框架文档，有以下初始化方法 （参考链接：https://pytorch.org/docs/stable/nn.init.html ）\n",
    "- `torch.nn.init.uniform_(tensor, a=0.0, b=1.0)`：以均匀分布$U(a,b)$初始化输入张量。\n",
    "\n",
    "- `torch.nn.init.normal_(tensor, mean=0.0, std=1.0)`：以正态分布$N(mean, std^2)$初始化输入张量。\n",
    "\n",
    "- `torch.nn.init.constant_(tensor, val)`：以一确定数值初始化输入张量。\n",
    "\n",
    "- `torch.nn.init.ones_(tensor)`：用标量值 1 初始化输入张量。\n",
    "\n",
    "- `torch.nn.init.zeros_(tensor)`：用标量值 0 初始化输入张量。\n",
    "\n",
    "- `torch.nn.init.eye_(tensor)`：用单位矩阵初始化二维输入张量。\n",
    "\n",
    "- `torch.nn.init.xavier_uniform_(tensor, gain=1.0)`：从均匀分布$U(−a, a)$中采样，初始化输入张量，其中$a$的值由如下公式确定\n",
    "\n",
    "  $$a= gain * \\sqrt{\\frac{6}{fan_{in}+fan_{out}}}$$\n",
    "  \n",
    "  其中$gain$的取值如下表所示\n",
    "<style> table { margin: auto;} </style>\n",
    "非线性函数 | gain值\n",
    ":----:|:----:\n",
    "Linear/Identity | 1\n",
    "Conv1D | 1\n",
    "Conv2D | 1\n",
    "Conv3D | 1\n",
    "Sigmoid | 1\n",
    "Tanh | $\\displaystyle\\frac{5}{3}$\n",
    "ReLU | $\\sqrt{2}$\n",
    "Leaky ReLU | $$\\sqrt{\\frac{2}{1+negative\\_slope^2}}$$\n",
    "SELU | 1 (adaptive)\n",
    "\n",
    "- `torch.nn.init.xavier_normal_(tensor, gain=1.0)`:从正态分布$N(0,std^2)$中采样，初始化输入张量，其中$std$值由下式确定：\n",
    "\n",
    "  $$std= gain * \\sqrt{\\frac{2}{fan_{in}+fan_{out}}}$$\n",
    "\n",
    "- `torch.nn.init.kaiming_uniform_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')`:服从均匀分布$U(−bound, bound)$，其中$bound$值由下式确定\n",
    "\n",
    "  $$bound= gain * \\sqrt{\\frac{3}{fan_{mode}}}$$\n",
    "\n",
    "- `torch.nn.init.kaiming_normal_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')`:服从从正态分布$N(0,std^2)$中采样，其中$std$值由下式确定\n",
    "\n",
    "  $$std= \\frac{gain}{\\sqrt{fan_{mode}}}$$\n",
    "  \n",
    "- `torch.nn.init.trunc_normal_(tensor, mean=0.0, std=1.0, a=- 2.0, b=2.0)`:用从截断的正态分布中提取的值初始化输入张量。这些值实际上是从正态分布 $N(mean, std^2)$中提取的。\n",
    "\n",
    "- `torch.nn.init.sparse_(tensor, sparsity, std=0.01)`：将 2D 输入张量初始化为稀疏矩阵，其中非零元素服从正态分布$N(0,0.01)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.2.3\n",
    "\n",
    "构建包含共享参数层的多层感知机并对其进行训练。在训练过程中，观察模型各层的参数和梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在训练过程中，我们每个epoch都打印了每层的参数和梯度。可以看到`shared_fc`层的参数和梯度都是相同的，因为它们共享同一个参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "参数名称: 0.weight, 具体参数值: tensor([[-0.3922,  0.4064, -0.0246],\n",
      "        [-0.2695,  0.4820, -0.1961]]), 参数梯度: tensor([[0., -0., 0.],\n",
      "        [0., -0., 0.]])\n",
      "参数名称: 0.bias, 具体参数值: tensor([-0.3853, -0.1208]), 参数梯度: tensor([0., 0.])\n",
      "参数名称: 2.weight, 具体参数值: tensor([[ 0.2374, -0.0735, -0.0410, -0.4920],\n",
      "        [ 0.4885,  0.0409,  0.3564, -0.1308]]), 参数梯度: tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0349, 0.0244, 0.0721]])\n",
      "参数名称: 2.bias, 具体参数值: tensor([-0.1918,  0.3889]), 参数梯度: tensor([0.0000, 0.1918])\n",
      "参数名称: 6.weight, 具体参数值: tensor([[ 0.1133,  0.3721, -0.1401,  0.1219],\n",
      "        [-0.3355, -0.3514,  0.0451,  0.2295]]), 参数梯度: tensor([[-0.0000, -0.0018, -0.0005, -0.0013],\n",
      "        [ 0.0000,  0.3521,  0.0893,  0.2498]])\n",
      "参数名称: 6.bias, 具体参数值: tensor([ 0.1476, -0.3919]), 参数梯度: tensor([-0.0040,  0.7845])\n",
      "Epoch: 0, Loss: 0.9601868987083435\n",
      "\n",
      "Epoch: 1\n",
      "参数名称: 0.weight, 具体参数值: tensor([[-0.3922,  0.4064, -0.0246],\n",
      "        [-0.2695,  0.4820, -0.1961]]), 参数梯度: tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "参数名称: 0.bias, 具体参数值: tensor([-0.3853, -0.1208]), 参数梯度: tensor([0., 0.])\n",
      "参数名称: 2.weight, 具体参数值: tensor([[ 0.2374, -0.0735, -0.0410, -0.4920],\n",
      "        [ 0.4885,  0.0406,  0.3561, -0.1315]]), 参数梯度: tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0321, 0.0226, 0.0702]])\n",
      "参数名称: 2.bias, 具体参数值: tensor([-0.1918,  0.3871]), 参数梯度: tensor([0.0000, 0.1841])\n",
      "参数名称: 6.weight, 具体参数值: tensor([[ 0.1133,  0.3721, -0.1401,  0.1219],\n",
      "        [-0.3355, -0.3549,  0.0443,  0.2270]]), 参数梯度: tensor([[ 0.0000, -0.0019, -0.0005, -0.0014],\n",
      "        [ 0.0000,  0.3476,  0.0878,  0.2500]])\n",
      "参数名称: 6.bias, 具体参数值: tensor([ 0.1476, -0.3997]), 参数梯度: tensor([-0.0043,  0.7785])\n",
      "Epoch: 1, Loss: 0.9425908923149109\n",
      "\n",
      "检查参数是否相同: tensor([True, True, True, True])\n",
      "检查参数是否同时变化: tensor([True, True, True, True])\n",
      "检查是否时同一个对象: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 模型参数\n",
    "input_size = 3\n",
    "hidden_size = 4\n",
    "output_size = 3\n",
    "lr = 0.01\n",
    "epochs = 2\n",
    "\n",
    "# 构建带有共享参数层的多层感知机\n",
    "shared_fc = nn.Linear(hidden_size, hidden_size)\n",
    "MLP = nn.Sequential(nn.Linear(input_size, hidden_size), nn.ReLU(),\n",
    "                    shared_fc, nn.ReLU(),\n",
    "                    shared_fc, nn.ReLU(),\n",
    "                    nn.Linear(hidden_size, output_size)\n",
    ")\n",
    "\n",
    "# 训练数据\n",
    "X = torch.randn(1, input_size)\n",
    "Y = torch.randn(1, output_size)\n",
    "# 优化器\n",
    "optimizer = optim.SGD(MLP.parameters(), lr=lr)\n",
    "# 训练模型\n",
    "for epoch in range(epochs):\n",
    "    # 前向传播和计算损失\n",
    "    Y_pred = MLP(X)\n",
    "    loss = nn.functional.mse_loss(Y_pred, Y)\n",
    "    # 反向传播和更新梯度\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # 打印每层的参数和梯度\n",
    "    print(f'Epoch: {epoch}')\n",
    "    for name, param in MLP.named_parameters():\n",
    "        print(f\"参数名称: {name}, 具体参数值: {param.data[:2]}, 参数梯度: {param.grad[:2]}\") # 为了节省页面空间，这里只打印了每个参数和梯度的前两维，读者可以自行调整查看维度\n",
    "    print(f'Epoch: {epoch}, Loss: {loss.item()}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出这个过程只有两个共享层的参数和梯度数值是一样的，我们直接对这两层结果进行进一步的确认。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "参数是否相同: tensor([[True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True]])\n",
      "参数是否同时变化: tensor([True, True, True, True])\n",
      "是否时同一个对象: True\n",
      "梯度是否相同: tensor([[True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True]])\n",
      "梯度是否同时变化: tensor([True, True, True, True])\n",
      "是否时同一个对象: True\n"
     ]
    }
   ],
   "source": [
    "# 检查参数是否相同\n",
    "print(f\"参数是否相同: {MLP[2].weight.data == MLP[4].weight.data}\") \n",
    "# 确保它们实际上是同一个对象，而不只是有相同的值\n",
    "MLP[2].weight.data[0, 0] = 100 \n",
    "print(f\"参数是否同时变化: {MLP[2].weight.data[0] == MLP[4].weight.data[0]}\") \n",
    "print(f\"是否时同一个对象: {MLP[2].weight.data.equal(MLP[4].weight.data)}\")\n",
    "\n",
    "\n",
    "# 检查参数是否相同\n",
    "print(f\"梯度是否相同: {MLP[2].weight.grad == MLP[4].weight.grad}\") \n",
    "# 确保它们实际上是同一个对象，而不只是有相同的值\n",
    "MLP[2].weight.grad[0, 0] = 100 \n",
    "print(f\"梯度是否同时变化: {MLP[2].weight.grad[0] == MLP[4].weight.grad[0]}\") \n",
    "print(f\"是否时同一个对象: {MLP[2].weight.grad.equal(MLP[4].weight.grad)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.2.4\n",
    "\n",
    "为什么共享参数是个好主意？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;1. 节约内存：共享参数可以减少模型中需要存储的参数数量，从而减少内存占用。\n",
    "\n",
    "&emsp;&emsp;2. 加速收敛：共享参数可以让模型更加稳定，加速收敛。\n",
    "\n",
    "&emsp;&emsp;3. 提高泛化能力：共享参数可以帮助模型更好地捕捉数据中的共性，提高模型的泛化能力。\n",
    "\n",
    "&emsp;&emsp;4. 加强模型的可解释性：共享参数可以让模型更加简洁明了，加强模型的可解释性。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 延后初始化 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.3.1 \n",
    "\n",
    "如果指定了第一层的输入尺寸，但没有指定后续层的尺寸，会发生什么？是否立即进行初始化？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;可以正常运行。第一层会立即初始化，但其他层是直到数据第一次通过模型传递才会初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): LazyLinear(in_features=0, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): LazyLinear(in_features=0, out_features=10, bias=True)\n",
      ")\n",
      "Sequential(\n",
      "  (0): Linear(in_features=20, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\"\"\"延后初始化\"\"\"\n",
    "net = nn.Sequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\n",
    "# 尚未初始化\n",
    "print(net)\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "net(X)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.3.2\n",
    "\n",
    "如果指定了不匹配的维度会发生什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;会由于矩阵乘法的维度不匹配而报错。在下面的代码中便指定了不匹配的维度。\n",
    "\n",
    "&emsp;&emsp;由于第一层 nn.Linear(20, 256) 的输入维度为 20，所以输入数据 X 的最后一维必须为 20 才能与该层的权重矩阵相乘。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mat1 and mat2 shapes cannot be multiplied (2x10 and 20x256)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(20, 256), nn.ReLU(),\n",
    "    nn.LazyLinear(128), nn.ReLU(),\n",
    "    nn.LazyLinear(10))\n",
    "\n",
    "X = torch.rand(2, 10)\n",
    "try:\n",
    "    net(X)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.3.3 \n",
    "\n",
    "如果输入具有不同的维度，需要做什么？提示：查看参数绑定的相关内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;添加一个额外的线性层，并将第一个线性层的权重与该层的权重绑定在一起。这样就可以解决维度不匹配的问题，并且保持模型的权重不变。注意，在上面的代码中，我们假设第一个线性层的偏置项为零，因此不需要对其进行参数绑定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mat1 and mat2 shapes cannot be multiplied (2x10 and 20x256)\n",
      "第一个线性层的维度：torch.Size([256, 20])\n",
      "额外的线性层的维度：torch.Size([256, 10])\n",
      "第一个线性层的新维度：torch.Size([256, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0583,  0.0507,  0.0882,  0.0361,  0.0139, -0.0563, -0.1265, -0.0549,\n",
       "          0.0812, -0.0299],\n",
       "        [-0.0485,  0.0538,  0.0838,  0.0298, -0.0502, -0.0324, -0.1437, -0.0458,\n",
       "          0.1361, -0.0202]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义模型\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(20, 256), nn.ReLU(),\n",
    "    nn.Linear(256, 128), nn.ReLU(),\n",
    "    nn.Linear(128, 10))\n",
    "\n",
    "X = torch.rand(2, 10)\n",
    "\n",
    "try:\n",
    "    net(X)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# 添加额外的线性层\n",
    "extra_layer = nn.Linear(10, 256)\n",
    "print(f\"第一个线性层的维度：{net[0].weight.shape}\")\n",
    "print(f\"额外的线性层的维度：{extra_layer.weight.shape}\")\n",
    "\n",
    "# 将第一个线性层与额外的线性层的权重进行绑定\n",
    "net[0].weight = extra_layer.weight\n",
    "print(f\"第一个线性层的新维度：{net[0].weight.shape}\")\n",
    "\n",
    "# 使用新的输入（维度为20）调用模型\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 自定义层 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.4.1 \n",
    "\n",
    "设计一个接受输入并计算张量降维的层，它返回$y_k = \\sum_{i, j} W_{ijk} x_i x_j$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这个公式表示一个线性变换，将输入张量$x$中所有可能的二元组$(x_i,x_j)$进行组合，并对它们进行加权求和。其中，$W_{ijk}$表示权重张量中第$i,j,k$个元素的值。具体而言，该公式计算了输入张量$x$中所有二元组$(x_i, x_j)$对应的特征向量$u_{ij}$为：\n",
    "\n",
    "\n",
    "$$\n",
    "u_{ij} = x_i \\cdot x_j\n",
    "$$\n",
    "\n",
    "\n",
    "&emsp;&emsp;然后，根据权重张量$W$中的权重$W_{ijk}$，对所有特征向量$u_{ij}$进行线性组合，得到输出向量$y_k$为：\n",
    "\n",
    "\n",
    "$$\n",
    "y_k = \\sum_{i,j} W_{ijk} u_{ij} = \\sum_{i,j} W_{ijk} x_i x_j\n",
    "$$\n",
    "\n",
    "\n",
    "&emsp;&emsp;该操作可以被视为一种降维操作，将高维输入$x$映射到低维输出空间$y$中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TensorReduction(nn.Module):\n",
    "    def __init__(self, dim1, dim2):\n",
    "        super(TensorReduction, self).__init__()\n",
    "        # 定义一个可训练的权重参数，维度为(dim2, dim1, dim1)\n",
    "        self.weight = nn.Parameter(torch.rand(dim2, dim1, dim1))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # 初始化一个全零张量，大小为(X.shape[0], self.weight.shape[0])\n",
    "        Y = torch.zeros(X.shape[0], self.weight.shape[0])\n",
    "        for k in range(self.weight.shape[0]):\n",
    "            # 计算temp = X @ weight[k] @ X^T\n",
    "            temp = torch.matmul(X, self.weight[k]) @ X.T\n",
    "            # 取temp的对角线元素，存入Y[:, k]\n",
    "            Y[:, k] = temp.diagonal()\n",
    "        return Y\n",
    "\n",
    "# 创建一个TensorReduction层，dim1=10, dim2=5\n",
    "layer = TensorReduction(10, 5)\n",
    "# 创建一个大小为(2, 10)的张量X\n",
    "X = torch.rand(2, 10)\n",
    "# 对layer(X)进行前向传播，返回一个大小为(2, 5)的张量\n",
    "layer(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.4.2 \n",
    "\n",
    "设计一个返回输入数据的傅立叶系数前半部分的层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;根据[维基百科](https://en.wikipedia.org/wiki/Fourier_series)中:\n",
    "> &emsp;&emsp;傅里叶级数是将任意周期函数表示为一组正弦和余弦函数的无限级数的方法。假设$f(x)$是在区间$[-L,L]$中定义的一个函数，其周期为$2L$，则其傅里叶级数可表示为：\n",
    "> \n",
    "> $$f(x) = \\frac{a_0}{2} + \\sum_{n=1}^{\\infty} [a_n \\cos(\\frac{n\\pi x}{L}) + b_n \\sin(\\frac{n\\pi x}{L})]$$\n",
    "> \n",
    "> &emsp;&emsp;其中，系数$a_0, a_n$和$b_n$可以通过以下公式计算得出：\n",
    "> \n",
    "> $$a_0 = \\frac{1}{2L} \\int_{-L}^{L} f(x)dx$$\n",
    "> \n",
    "> $$a_n = \\frac{1}{L} \\int_{-L}^{L} f(x) \\cos(\\frac{n\\pi x}{L}) dx, n>0$$\n",
    "> \n",
    "> $$b_n = \\frac{1}{L} \\int_{-L}^{L} f(x) \\sin(\\frac{n\\pi x}{L}) dx, n>0$$\n",
    "> \n",
    "> &emsp;&emsp;系数$a_n$和$b_n$实际上是$f(x)$与$\\cos(\\frac{n\\pi x}{L})$和$\\sin(\\frac{n\\pi x}{L})$的内积，而$a_0$是$f(x)$平均值的一半。\n",
    "\n",
    "&emsp;&emsp;在torch中有相应的函数可以轻松的实现傅里叶级数，如下代码所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 5.7054+0.0000j,  0.0402+0.0496j],\n",
       "         [-0.2460+0.0000j,  1.0908-0.0832j]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.fft as fft\n",
    "\n",
    "class FourierLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FourierLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 对输入的张量 x 进行快速傅里叶变换\n",
    "        x = fft.fftn(x)\n",
    "        # 取出第三个维度的前半部分，即去掉直流分量和镜像分量\n",
    "        x = x[:, :, :x.shape[2] // 2]\n",
    "        # 返回处理后的张量\n",
    "        return x\n",
    "\n",
    "# 创建一个随机数值为 [0, 1) 的形状为 (1, 2, 5) 的张量 X\n",
    "X = torch.rand(1, 2, 5)\n",
    "# 实例化一个 FourierLayer 的网络对象 net\n",
    "net = FourierLayer()\n",
    "# 将 X 输入到网络 net 中进行前向计算，并输出结果\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 读写文件 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  练习 5.5.1\n",
    "\n",
    "即使不需要将经过训练的模型部署到不同的设备上，存储模型参数还有什么实际的好处？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;1. 加速模型训练：存储模型参数可以避免每次重新训练模型时需要重复计算之前已经计算过的权重和偏置。\n",
    "\n",
    "&emsp;&emsp;2. 节省内存空间：保存模型参数比保存完整的模型文件更加节省内存空间，这在处理大型模型或使用内存受限设备时尤为重要。\n",
    "\n",
    "&emsp;&emsp;3. 便于共享和复现：存储模型参数可以方便地共享和复现已经训练好的模型，其他人可以直接加载这些参数并使用它们进行预测或微调。\n",
    "\n",
    "&emsp;&emsp;4. 便于调试和分析：通过检查模型参数，可以更容易地诊断模型中存在的问题，并对其进行调整和优化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.5.2\n",
    "\n",
    "假设我们只想复用网络的一部分，以将其合并到不同的网络架构中。比如想在一个新的网络中使用之前网络的前两层，该怎么做？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;使用保存模型某层参数的办法，保存网络的前两层，然后再加载到新的网络中使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        ...,\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class MLP(nn.Module):             # 定义 MLP 类\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)   # 定义隐藏层层，输入尺寸为 20，输出尺寸为 256\n",
    "        self.output = nn.Linear(256, 10)   # 定义输出层，输入尺寸为 256，输出尺寸为 10\n",
    "\n",
    "    def forward(self, x):          # 定义前向传播函数\n",
    "        return self.output(F.relu(self.hidden(x)))  # 使用 ReLU 激活函数，计算隐藏层和输出层的输出\n",
    "\n",
    "class MLP_new(nn.Module):             # 定义 MLP 类\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)   # 定义隐藏层层，输入尺寸为 20，输出尺寸为 256\n",
    "        self.output = nn.Linear(256, 10)   # 定义输出层，输入尺寸为 256，输出尺寸为 10\n",
    "\n",
    "    def forward(self, x):          # 定义前向传播函数\n",
    "        return self.output(F.relu(self.hidden(x)))  # 使用 ReLU 激活函数，计算隐藏层和输出层的输出\n",
    "\n",
    "net = MLP()                       # 创建 MLP 的实例\n",
    "torch.save(net.hidden.state_dict(), 'mlp.hidden.params')  # 将隐藏层的参数保存到文件中\n",
    "clone = MLP_new()                     # 创建另一个 MLP 的实例\n",
    "clone.hidden.load_state_dict(torch.load('mlp.hidden.params'))  # 加载已保存的参数到克隆实例的隐藏层中\n",
    "print(clone.hidden.weight == net.hidden.weight)  # 比较两个 MLP 实例的隐藏层权重是否相等，并输出结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.5.3\n",
    "\n",
    "如何同时保存网络架构和参数？需要对架构加上什么限制？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在PyTorch中，可以使用`torch.save()`函数同时保存网络架构和参数。为了保存网络架构，需要将模型的结构定义在一个Python类中，并将该类实例化为模型对象。此外，必须确保该类的构造函数不包含任何随机性质的操作，例如dropout层的随机丢弃率应该是固定的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('hidden.weight',\n",
       "              tensor([[-0.1156,  0.1863, -0.0175,  ..., -0.1466, -0.2019,  0.0216],\n",
       "                      [ 0.0895, -0.1677, -0.0554,  ..., -0.1252, -0.2170,  0.0083],\n",
       "                      [ 0.0219,  0.1506, -0.0535,  ...,  0.1031, -0.1729,  0.1682],\n",
       "                      ...,\n",
       "                      [-0.1097,  0.0599, -0.1763,  ...,  0.1636,  0.1202, -0.0097],\n",
       "                      [-0.1457,  0.2051,  0.2128,  ...,  0.1272, -0.0476, -0.0765],\n",
       "                      [-0.1741, -0.1148,  0.1382,  ...,  0.0671,  0.1791, -0.1454]])),\n",
       "             ('hidden.bias',\n",
       "              tensor([ 0.1766, -0.0295, -0.1861, -0.0068,  0.2015, -0.0649, -0.0570, -0.2172,\n",
       "                       0.0714, -0.1939,  0.2156,  0.0916,  0.1790, -0.1583,  0.0692, -0.1513,\n",
       "                      -0.1918,  0.1775,  0.2189, -0.0194, -0.1804, -0.1995, -0.0825, -0.0024,\n",
       "                       0.0511,  0.1024,  0.0159,  0.1635,  0.1716,  0.2139,  0.1466, -0.1488,\n",
       "                       0.0122, -0.0310,  0.1765, -0.1931,  0.1383,  0.0725,  0.1630,  0.0622,\n",
       "                      -0.0836,  0.0341, -0.0112,  0.0356,  0.0274, -0.1440,  0.0020, -0.0793,\n",
       "                      -0.1728,  0.1986, -0.2185,  0.0159, -0.1773, -0.1332,  0.0944, -0.0076,\n",
       "                       0.1248,  0.0570, -0.1019, -0.1706, -0.1054,  0.1165,  0.1299, -0.0062,\n",
       "                      -0.1740,  0.1895, -0.1824, -0.1933,  0.1936, -0.1204,  0.2207,  0.1267,\n",
       "                       0.1236,  0.0671, -0.1981,  0.0835, -0.0292,  0.0989,  0.0729,  0.1339,\n",
       "                       0.0196, -0.0299, -0.1500,  0.0006,  0.1628, -0.0434,  0.0067, -0.1456,\n",
       "                       0.1304, -0.1666,  0.1803,  0.2052,  0.1699, -0.1716,  0.1595,  0.1105,\n",
       "                      -0.1859,  0.0438,  0.0998,  0.0628, -0.1161,  0.0794, -0.1899,  0.1666,\n",
       "                      -0.1218, -0.1331, -0.0222, -0.1483,  0.2037,  0.0646,  0.0780, -0.1663,\n",
       "                      -0.0258, -0.0954,  0.0507,  0.1409,  0.0035,  0.0075,  0.0272,  0.1016,\n",
       "                       0.2113, -0.0838,  0.2073, -0.0815,  0.1675,  0.0785, -0.0213,  0.0250,\n",
       "                       0.0124, -0.1796,  0.1304,  0.0415,  0.0501, -0.0830, -0.1177, -0.0184,\n",
       "                       0.1451,  0.2138, -0.0849,  0.1035, -0.1483,  0.1134,  0.0372,  0.0114,\n",
       "                       0.2180,  0.1472, -0.0085,  0.0218,  0.0432,  0.0168, -0.0995,  0.0535,\n",
       "                       0.2126,  0.0948,  0.0413, -0.1711, -0.0574,  0.1348, -0.1017,  0.2058,\n",
       "                       0.0341, -0.0094,  0.0182, -0.1423,  0.0180,  0.0717,  0.0249,  0.1916,\n",
       "                       0.0109, -0.0301, -0.0721,  0.0041, -0.1025, -0.1381, -0.1428, -0.2062,\n",
       "                       0.1903,  0.2106,  0.0542, -0.0143, -0.0606, -0.0076,  0.1080,  0.0385,\n",
       "                      -0.1319, -0.0086,  0.1028, -0.2122, -0.1795, -0.0077, -0.2020, -0.0379,\n",
       "                       0.1401,  0.0745, -0.2204, -0.2118, -0.2111, -0.1001, -0.1703,  0.0028,\n",
       "                      -0.1287, -0.1110, -0.1749,  0.0086,  0.1138, -0.0624,  0.0978,  0.1115,\n",
       "                      -0.0148,  0.0990,  0.1336,  0.0328,  0.0651,  0.1320,  0.1443,  0.0800,\n",
       "                       0.2155,  0.1570,  0.0192, -0.1268,  0.1085,  0.0400, -0.0108,  0.1133,\n",
       "                      -0.1684,  0.2001, -0.0513, -0.0052, -0.0617,  0.1917,  0.1170,  0.1218,\n",
       "                      -0.2125, -0.0716, -0.0619, -0.0967, -0.0929, -0.2126,  0.2150,  0.0424,\n",
       "                      -0.2172,  0.1137,  0.0601, -0.0266,  0.1998,  0.0429, -0.0619, -0.0983,\n",
       "                      -0.1875,  0.0654, -0.1324, -0.0647, -0.1327, -0.0441, -0.0721,  0.0988])),\n",
       "             ('output.weight',\n",
       "              tensor([[-0.0009,  0.0294, -0.0051,  ..., -0.0006,  0.0582,  0.0326],\n",
       "                      [ 0.0174, -0.0336, -0.0176,  ..., -0.0027,  0.0142, -0.0435],\n",
       "                      [-0.0136,  0.0579, -0.0130,  ..., -0.0463,  0.0178, -0.0148],\n",
       "                      ...,\n",
       "                      [ 0.0538,  0.0350, -0.0365,  ..., -0.0312,  0.0151, -0.0308],\n",
       "                      [ 0.0241, -0.0240, -0.0613,  ...,  0.0461, -0.0597,  0.0160],\n",
       "                      [ 0.0439,  0.0540,  0.0307,  ..., -0.0360, -0.0272, -0.0426]])),\n",
       "             ('output.bias',\n",
       "              tensor([-0.0548, -0.0609, -0.0241,  0.0394, -0.0228, -0.0331, -0.0540, -0.0416,\n",
       "                       0.0147,  0.0345]))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class MLP(nn.Module):             # 定义 MLP 类\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)   # 定义隐藏层层，输入尺寸为 20，输出尺寸为 256\n",
    "        self.output = nn.Linear(256, 10)   # 定义输出层，输入尺寸为 256，输出尺寸为 10\n",
    "\n",
    "    def forward(self, x):          # 定义前向传播函数\n",
    "        return self.output(F.relu(self.hidden(x)))  # 使用 ReLU 激活函数，计算隐藏层和输出层的输出\n",
    "\n",
    "net = MLP()\n",
    "\n",
    "# 存储模型\n",
    "torch.save(net.state_dict(), 'model.pt')\n",
    "\n",
    "# 导入模型\n",
    "model = torch.load('model.pt')\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 GPU "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.6.1 \n",
    "\n",
    "尝试一个计算量更大的任务，比如大矩阵的乘法，看看CPU和GPU之间的速度差异。再试一个计算量很小的任务呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;计算量很大的任务：使用GPU速度明显更快\n",
    "\n",
    "&emsp;&emsp;计算量很小的任务：CPU速度可能更快，因为数据传输到GPU需要时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu time cost: 10666.56ms\n",
      "gpu time cost: 1193.48ms\n",
      "cpu time cost: 0ms\n",
      "gpu time cost: 0ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "# 计算量较大的任务\n",
    "X = torch.rand((10000, 10000))\n",
    "Y = X.cuda(0)\n",
    "time_start = time.time()\n",
    "Z = torch.mm(X, X)\n",
    "time_end = time.time()\n",
    "print(f'cpu time cost: {round((time_end - time_start) * 1000, 2)}ms')\n",
    "time_start = time.time()\n",
    "Z = torch.mm(Y, Y)\n",
    "time_end = time.time()\n",
    "print(f'gpu time cost: {round((time_end - time_start) * 1000, 2)}ms')\n",
    "\n",
    "# 计算量很小的任务\n",
    "X = torch.rand((100, 100))\n",
    "Y = X.cuda(0)\n",
    "time_start = time.time()\n",
    "Z = torch.mm(X, X)\n",
    "time_end = time.time()\n",
    "print(f'cpu time cost: {round((time_end - time_start) * 1000)}ms')\n",
    "time_start = time.time()\n",
    "Z = torch.mm(Y, Y)\n",
    "time_end = time.time()\n",
    "print(f'gpu time cost: {round((time_end - time_start) * 1000)}ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.6.2\n",
    "\n",
    "我们应该如何在GPU上读写模型参数？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;使用`net.to(device)`将模型迁移到GPU上，然后再按照之前的方法读写参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('hidden.weight',\n",
       "              tensor([[-0.0432, -0.1930,  0.2123,  ...,  0.0261,  0.1068, -0.1317],\n",
       "                      [ 0.0600, -0.0118,  0.1694,  ...,  0.0250,  0.1431, -0.0099],\n",
       "                      [-0.0433,  0.0347,  0.0848,  ..., -0.1433, -0.1530,  0.0941],\n",
       "                      ...,\n",
       "                      [ 0.0463, -0.0126, -0.1281,  ...,  0.0086,  0.1511, -0.0612],\n",
       "                      [-0.0961,  0.0380,  0.1699,  ...,  0.0807, -0.1931, -0.1482],\n",
       "                      [ 0.0692,  0.1754, -0.1558,  ..., -0.0194, -0.0038, -0.1192]],\n",
       "                     device='cuda:0')),\n",
       "             ('hidden.bias',\n",
       "              tensor([-0.0917,  0.1111,  0.0606,  0.1126, -0.0134,  0.1238, -0.1166, -0.1253,\n",
       "                      -0.2017, -0.0720, -0.1909, -0.1292, -0.2150, -0.0914,  0.0252,  0.1119,\n",
       "                      -0.1487,  0.1433, -0.0134, -0.0190, -0.0965,  0.0042, -0.0422, -0.1297,\n",
       "                      -0.1726,  0.1291,  0.0240, -0.0975, -0.0563, -0.0059,  0.0663, -0.1109,\n",
       "                       0.1318, -0.0842,  0.1192, -0.0094, -0.0738,  0.0525, -0.0794,  0.0294,\n",
       "                      -0.0594, -0.0539,  0.0464, -0.1584, -0.1031,  0.0378, -0.0634, -0.0937,\n",
       "                       0.1925,  0.0734, -0.0786,  0.1569,  0.2214,  0.0881, -0.0914, -0.0423,\n",
       "                      -0.1731,  0.0549,  0.1029, -0.1754,  0.0755,  0.1749,  0.1429, -0.1584,\n",
       "                       0.0883, -0.0443,  0.2183,  0.0243, -0.1041, -0.1404, -0.2173,  0.0926,\n",
       "                      -0.0041, -0.1878,  0.0221,  0.0115,  0.0226, -0.2167,  0.2117, -0.0912,\n",
       "                       0.1352, -0.0309, -0.1846,  0.0896, -0.0330, -0.1251, -0.0828,  0.1357,\n",
       "                      -0.1486,  0.1482,  0.2110, -0.1634,  0.0424, -0.0310, -0.2083,  0.1316,\n",
       "                       0.1220,  0.1329, -0.0534,  0.1006, -0.0690, -0.2045,  0.1394, -0.0197,\n",
       "                      -0.1316,  0.0192, -0.1713, -0.0355,  0.0741, -0.1259, -0.0676,  0.1689,\n",
       "                       0.0559,  0.0930,  0.1198,  0.1338, -0.0996, -0.1344, -0.1331, -0.1800,\n",
       "                       0.1331,  0.1098,  0.0771, -0.1508, -0.0768, -0.1873,  0.0574,  0.1658,\n",
       "                       0.0138,  0.0859,  0.1685,  0.0780, -0.0465, -0.0818,  0.0875,  0.1189,\n",
       "                      -0.0610, -0.2144,  0.0712, -0.0919, -0.0027, -0.1690, -0.0875, -0.1343,\n",
       "                       0.1530, -0.0886, -0.0698,  0.1540,  0.1790, -0.0707,  0.1121,  0.1836,\n",
       "                       0.1868, -0.0792,  0.2077, -0.0793, -0.1420,  0.0465, -0.0088,  0.1172,\n",
       "                      -0.0367, -0.1313, -0.0480,  0.0676, -0.1355,  0.0982,  0.1367, -0.0494,\n",
       "                      -0.0798,  0.0952,  0.1556,  0.0431,  0.1364,  0.0783,  0.1828,  0.0923,\n",
       "                      -0.1137,  0.1838,  0.1636,  0.1233,  0.1776, -0.1573,  0.0854,  0.0007,\n",
       "                       0.1211,  0.0110, -0.2182, -0.0487, -0.0093,  0.2103, -0.2203, -0.1809,\n",
       "                       0.0250, -0.1747, -0.0134,  0.1462,  0.0606,  0.0120,  0.1267, -0.1504,\n",
       "                       0.0651,  0.2034, -0.1150, -0.1709, -0.1697,  0.0113, -0.0758,  0.1178,\n",
       "                       0.0164, -0.1215, -0.0651,  0.0680, -0.1631,  0.0755, -0.1190,  0.0384,\n",
       "                      -0.1965,  0.0182, -0.0724, -0.0316,  0.0176, -0.1655,  0.0507, -0.1219,\n",
       "                       0.1202,  0.0254,  0.1698, -0.0890,  0.1968, -0.1419, -0.1922,  0.0703,\n",
       "                       0.1278,  0.0852,  0.0899, -0.1138, -0.0535, -0.0802, -0.1164,  0.1638,\n",
       "                      -0.1939,  0.1950, -0.1853, -0.2065,  0.1341,  0.0865,  0.1317,  0.0514,\n",
       "                      -0.0098, -0.0319, -0.0282,  0.1304,  0.1279,  0.0616,  0.0902,  0.1834],\n",
       "                     device='cuda:0')),\n",
       "             ('output.weight',\n",
       "              tensor([[ 0.0378,  0.0368, -0.0586,  ...,  0.0592, -0.0195, -0.0052],\n",
       "                      [-0.0527, -0.0614, -0.0420,  ...,  0.0054, -0.0490,  0.0007],\n",
       "                      [ 0.0558, -0.0129,  0.0006,  ..., -0.0409,  0.0259,  0.0535],\n",
       "                      ...,\n",
       "                      [-0.0624, -0.0108,  0.0038,  ..., -0.0055,  0.0586,  0.0554],\n",
       "                      [ 0.0592,  0.0014, -0.0083,  ..., -0.0480, -0.0523, -0.0138],\n",
       "                      [ 0.0562,  0.0339,  0.0364,  ...,  0.0122,  0.0533, -0.0364]],\n",
       "                     device='cuda:0')),\n",
       "             ('output.bias',\n",
       "              tensor([ 0.0610, -0.0384, -0.0256, -0.0007,  0.0381, -0.0479, -0.0177, -0.0192,\n",
       "                       0.0078, -0.0304], device='cuda:0'))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class MLP(nn.Module):             # 定义 MLP 类\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)   # 定义隐藏层层，输入尺寸为 20，输出尺寸为 256\n",
    "        self.output = nn.Linear(256, 10)   # 定义输出层，输入尺寸为 256，输出尺寸为 10\n",
    "\n",
    "    def forward(self, x):          # 定义前向传播函数\n",
    "        return self.output(F.relu(self.hidden(x)))  # 使用 ReLU 激活函数，计算隐藏层和输出层的输出\n",
    "\n",
    "# 选择GPU，没有GPU就选CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 创建模型实例对象\n",
    "net = MLP()\n",
    "# 将模型参数传输到GPU上\n",
    "net.to(device)\n",
    "# 访问模型参数\n",
    "net.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.6.3 \n",
    "\n",
    "测量计算1000个$100 \\times 100$矩阵的矩阵乘法所需的时间，并记录输出矩阵的Frobenius范数，一次记录一个结果，而不是在GPU上保存日志并仅传输最终结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;中文版翻译有点问题，英文原版这句话是：\n",
    "\n",
    ">&emsp;&emsp;Measure the time it takes to compute 1000 matrix-matrix multiplications of $100×100$ matrices and log the Frobenius norm of the output matrix one result at a time vs. keeping a log on the GPU and transferring only the final result.\n",
    "\n",
    "&emsp;&emsp;所以这道题的本质还是希望我们做个比较。\n",
    "\n",
    "&emsp;&emsp;实验一：仅记录1000次$100×100$矩阵相乘所用的时间，不需要打印Frobenius范数。\n",
    "\n",
    "&emsp;&emsp;实验二：记录1000次$100×100$矩阵相乘所用的时间，并打印Frobenius范数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.083892822265625\n",
      "tensor(1414.3264, device='cuda:0')\n",
      "tensor(1420.3613, device='cuda:0')\n",
      "tensor(1426.7102, device='cuda:0')\n",
      "tensor(1404.4108, device='cuda:0')\n",
      "tensor(1399.9667, device='cuda:0')\n",
      "tensor(1423.5759, device='cuda:0')\n",
      "tensor(1404.5486, device='cuda:0')\n",
      "tensor(1386.4878, device='cuda:0')\n",
      "tensor(1453.0780, device='cuda:0')\n",
      "tensor(1399.2872, device='cuda:0')\n",
      "tensor(1401.3347, device='cuda:0')\n",
      "tensor(1442.3737, device='cuda:0')\n",
      "tensor(1409.5651, device='cuda:0')\n",
      "tensor(1412.1494, device='cuda:0')\n",
      "tensor(1399.0287, device='cuda:0')\n",
      "tensor(1377.4283, device='cuda:0')\n",
      "tensor(1409.3846, device='cuda:0')\n",
      "tensor(1440.5291, device='cuda:0')\n",
      "tensor(1383.9038, device='cuda:0')\n",
      "tensor(1401.8132, device='cuda:0')\n",
      "tensor(1444.8940, device='cuda:0')\n",
      "tensor(1407.5876, device='cuda:0')\n",
      "tensor(1392.9998, device='cuda:0')\n",
      "tensor(1447.4484, device='cuda:0')\n",
      "tensor(1441.7650, device='cuda:0')\n",
      "tensor(1424.2852, device='cuda:0')\n",
      "tensor(1412.0476, device='cuda:0')\n",
      "tensor(1446.4708, device='cuda:0')\n",
      "tensor(1439.6016, device='cuda:0')\n",
      "tensor(1400.5818, device='cuda:0')\n",
      "tensor(1442.8463, device='cuda:0')\n",
      "tensor(1401.3495, device='cuda:0')\n",
      "tensor(1416.1384, device='cuda:0')\n",
      "tensor(1407.5288, device='cuda:0')\n",
      "tensor(1400.6179, device='cuda:0')\n",
      "tensor(1439.5996, device='cuda:0')\n",
      "tensor(1390.1395, device='cuda:0')\n",
      "tensor(1424.9067, device='cuda:0')\n",
      "tensor(1419.1511, device='cuda:0')\n",
      "tensor(1397.7457, device='cuda:0')\n",
      "tensor(1424.5667, device='cuda:0')\n",
      "tensor(1431.6057, device='cuda:0')\n",
      "tensor(1414.7839, device='cuda:0')\n",
      "tensor(1404.5869, device='cuda:0')\n",
      "tensor(1410.4812, device='cuda:0')\n",
      "tensor(1426.4489, device='cuda:0')\n",
      "tensor(1395.1388, device='cuda:0')\n",
      "tensor(1432.4199, device='cuda:0')\n",
      "tensor(1388.4635, device='cuda:0')\n",
      "tensor(1412.4142, device='cuda:0')\n",
      "tensor(1409.0317, device='cuda:0')\n",
      "tensor(1444.3677, device='cuda:0')\n",
      "tensor(1425.9957, device='cuda:0')\n",
      "tensor(1399.0273, device='cuda:0')\n",
      "tensor(1391.0610, device='cuda:0')\n",
      "tensor(1422.5696, device='cuda:0')\n",
      "tensor(1385.3455, device='cuda:0')\n",
      "tensor(1405.2266, device='cuda:0')\n",
      "tensor(1414.0631, device='cuda:0')\n",
      "tensor(1441.1052, device='cuda:0')\n",
      "tensor(1404.9672, device='cuda:0')\n",
      "tensor(1434.1139, device='cuda:0')\n",
      "tensor(1382.1471, device='cuda:0')\n",
      "tensor(1431.5863, device='cuda:0')\n",
      "tensor(1393.2568, device='cuda:0')\n",
      "tensor(1398.7456, device='cuda:0')\n",
      "tensor(1452.7546, device='cuda:0')\n",
      "tensor(1422.0884, device='cuda:0')\n",
      "tensor(1426.9169, device='cuda:0')\n",
      "tensor(1406.6737, device='cuda:0')\n",
      "tensor(1415.6372, device='cuda:0')\n",
      "tensor(1425.7087, device='cuda:0')\n",
      "tensor(1436.4384, device='cuda:0')\n",
      "tensor(1408.1709, device='cuda:0')\n",
      "tensor(1397.7487, device='cuda:0')\n",
      "tensor(1432.1602, device='cuda:0')\n",
      "tensor(1401.4945, device='cuda:0')\n",
      "tensor(1364.3336, device='cuda:0')\n",
      "tensor(1432.4294, device='cuda:0')\n",
      "tensor(1420.6379, device='cuda:0')\n",
      "tensor(1402.3220, device='cuda:0')\n",
      "tensor(1415.9830, device='cuda:0')\n",
      "tensor(1427.9037, device='cuda:0')\n",
      "tensor(1426.5095, device='cuda:0')\n",
      "tensor(1404.6687, device='cuda:0')\n",
      "tensor(1412.2659, device='cuda:0')\n",
      "tensor(1425.6060, device='cuda:0')\n",
      "tensor(1455.0330, device='cuda:0')\n",
      "tensor(1425.2367, device='cuda:0')\n",
      "tensor(1412.7449, device='cuda:0')\n",
      "tensor(1383.8060, device='cuda:0')\n",
      "tensor(1462.7919, device='cuda:0')\n",
      "tensor(1441.7439, device='cuda:0')\n",
      "tensor(1428.8705, device='cuda:0')\n",
      "tensor(1388.8622, device='cuda:0')\n",
      "tensor(1409.4866, device='cuda:0')\n",
      "tensor(1408.3340, device='cuda:0')\n",
      "tensor(1409.9652, device='cuda:0')\n",
      "tensor(1428.4456, device='cuda:0')\n",
      "tensor(1458.7307, device='cuda:0')\n",
      "tensor(1412.8656, device='cuda:0')\n",
      "tensor(1398.3041, device='cuda:0')\n",
      "tensor(1454.4297, device='cuda:0')\n",
      "tensor(1420.5155, device='cuda:0')\n",
      "tensor(1409.4517, device='cuda:0')\n",
      "tensor(1437.3512, device='cuda:0')\n",
      "tensor(1439.3440, device='cuda:0')\n",
      "tensor(1420.8407, device='cuda:0')\n",
      "tensor(1385.2998, device='cuda:0')\n",
      "tensor(1403.2085, device='cuda:0')\n",
      "tensor(1443.1156, device='cuda:0')\n",
      "tensor(1436.5155, device='cuda:0')\n",
      "tensor(1412.8850, device='cuda:0')\n",
      "tensor(1430.8556, device='cuda:0')\n",
      "tensor(1426.4655, device='cuda:0')\n",
      "tensor(1437.0756, device='cuda:0')\n",
      "tensor(1448.4524, device='cuda:0')\n",
      "tensor(1406.4270, device='cuda:0')\n",
      "tensor(1416.3265, device='cuda:0')\n",
      "tensor(1385.5703, device='cuda:0')\n",
      "tensor(1409.6227, device='cuda:0')\n",
      "tensor(1387.6338, device='cuda:0')\n",
      "tensor(1387.0051, device='cuda:0')\n",
      "tensor(1401.6917, device='cuda:0')\n",
      "tensor(1459.0684, device='cuda:0')\n",
      "tensor(1415.2648, device='cuda:0')\n",
      "tensor(1400.3497, device='cuda:0')\n",
      "tensor(1427.4443, device='cuda:0')\n",
      "tensor(1427.0580, device='cuda:0')\n",
      "tensor(1434.4236, device='cuda:0')\n",
      "tensor(1428.7977, device='cuda:0')\n",
      "tensor(1415.7834, device='cuda:0')\n",
      "tensor(1419.2070, device='cuda:0')\n",
      "tensor(1450.7157, device='cuda:0')\n",
      "tensor(1426.0618, device='cuda:0')\n",
      "tensor(1411.4818, device='cuda:0')\n",
      "tensor(1393.3303, device='cuda:0')\n",
      "tensor(1429.1759, device='cuda:0')\n",
      "tensor(1438.4055, device='cuda:0')\n",
      "tensor(1382.0458, device='cuda:0')\n",
      "tensor(1414.6213, device='cuda:0')\n",
      "tensor(1416.8164, device='cuda:0')\n",
      "tensor(1402.7170, device='cuda:0')\n",
      "tensor(1424.0973, device='cuda:0')\n",
      "tensor(1396.5586, device='cuda:0')\n",
      "tensor(1397.8713, device='cuda:0')\n",
      "tensor(1413.1685, device='cuda:0')\n",
      "tensor(1420.2782, device='cuda:0')\n",
      "tensor(1451.3582, device='cuda:0')\n",
      "tensor(1389.3899, device='cuda:0')\n",
      "tensor(1417.5210, device='cuda:0')\n",
      "tensor(1405.4589, device='cuda:0')\n",
      "tensor(1418.8301, device='cuda:0')\n",
      "tensor(1441.7411, device='cuda:0')\n",
      "tensor(1393.5591, device='cuda:0')\n",
      "tensor(1459.0192, device='cuda:0')\n",
      "tensor(1384.6997, device='cuda:0')\n",
      "tensor(1367.1498, device='cuda:0')\n",
      "tensor(1411.3000, device='cuda:0')\n",
      "tensor(1449.4939, device='cuda:0')\n",
      "tensor(1432.4763, device='cuda:0')\n",
      "tensor(1407.5201, device='cuda:0')\n",
      "tensor(1423.5007, device='cuda:0')\n",
      "tensor(1421.6755, device='cuda:0')\n",
      "tensor(1407.4553, device='cuda:0')\n",
      "tensor(1396.2762, device='cuda:0')\n",
      "tensor(1438.9589, device='cuda:0')\n",
      "tensor(1426.8402, device='cuda:0')\n",
      "tensor(1426.7487, device='cuda:0')\n",
      "tensor(1408.7693, device='cuda:0')\n",
      "tensor(1412.2242, device='cuda:0')\n",
      "tensor(1390.1692, device='cuda:0')\n",
      "tensor(1397.5320, device='cuda:0')\n",
      "tensor(1418.8942, device='cuda:0')\n",
      "tensor(1409.5818, device='cuda:0')\n",
      "tensor(1450.8793, device='cuda:0')\n",
      "tensor(1445.1687, device='cuda:0')\n",
      "tensor(1436.6711, device='cuda:0')\n",
      "tensor(1380.7662, device='cuda:0')\n",
      "tensor(1420.0610, device='cuda:0')\n",
      "tensor(1449.6732, device='cuda:0')\n",
      "tensor(1410.6570, device='cuda:0')\n",
      "tensor(1399.0934, device='cuda:0')\n",
      "tensor(1402.6781, device='cuda:0')\n",
      "tensor(1419.4244, device='cuda:0')\n",
      "tensor(1431.7424, device='cuda:0')\n",
      "tensor(1425.0427, device='cuda:0')\n",
      "tensor(1422.3596, device='cuda:0')\n",
      "tensor(1446.0736, device='cuda:0')\n",
      "tensor(1393.0781, device='cuda:0')\n",
      "tensor(1403.0770, device='cuda:0')\n",
      "tensor(1420.0957, device='cuda:0')\n",
      "tensor(1443.6151, device='cuda:0')\n",
      "tensor(1412.4930, device='cuda:0')\n",
      "tensor(1459.8573, device='cuda:0')\n",
      "tensor(1405.6935, device='cuda:0')\n",
      "tensor(1372.5243, device='cuda:0')\n",
      "tensor(1411.5518, device='cuda:0')\n",
      "tensor(1391.6947, device='cuda:0')\n",
      "tensor(1415.2728, device='cuda:0')\n",
      "tensor(1444.5618, device='cuda:0')\n",
      "tensor(1408.9080, device='cuda:0')\n",
      "tensor(1392.5614, device='cuda:0')\n",
      "tensor(1427.8918, device='cuda:0')\n",
      "tensor(1419.5483, device='cuda:0')\n",
      "tensor(1415.6810, device='cuda:0')\n",
      "tensor(1397.2209, device='cuda:0')\n",
      "tensor(1396.9866, device='cuda:0')\n",
      "tensor(1426.4923, device='cuda:0')\n",
      "tensor(1456.2041, device='cuda:0')\n",
      "tensor(1369.0884, device='cuda:0')\n",
      "tensor(1405.4847, device='cuda:0')\n",
      "tensor(1434.1417, device='cuda:0')\n",
      "tensor(1429.5665, device='cuda:0')\n",
      "tensor(1471.4082, device='cuda:0')\n",
      "tensor(1424.3124, device='cuda:0')\n",
      "tensor(1403.1338, device='cuda:0')\n",
      "tensor(1405.5498, device='cuda:0')\n",
      "tensor(1450.0192, device='cuda:0')\n",
      "tensor(1415.6967, device='cuda:0')\n",
      "tensor(1396.4835, device='cuda:0')\n",
      "tensor(1420.1073, device='cuda:0')\n",
      "tensor(1467.6332, device='cuda:0')\n",
      "tensor(1442.1469, device='cuda:0')\n",
      "tensor(1435.9219, device='cuda:0')\n",
      "tensor(1429.7675, device='cuda:0')\n",
      "tensor(1382.4639, device='cuda:0')\n",
      "tensor(1421.8973, device='cuda:0')\n",
      "tensor(1385.1857, device='cuda:0')\n",
      "tensor(1421.3765, device='cuda:0')\n",
      "tensor(1399.2683, device='cuda:0')\n",
      "tensor(1415.0687, device='cuda:0')\n",
      "tensor(1429.7700, device='cuda:0')\n",
      "tensor(1387.0018, device='cuda:0')\n",
      "tensor(1431.9097, device='cuda:0')\n",
      "tensor(1417.7034, device='cuda:0')\n",
      "tensor(1443.3807, device='cuda:0')\n",
      "tensor(1383.3411, device='cuda:0')\n",
      "tensor(1453.2593, device='cuda:0')\n",
      "tensor(1443.6071, device='cuda:0')\n",
      "tensor(1407.7806, device='cuda:0')\n",
      "tensor(1460.8347, device='cuda:0')\n",
      "tensor(1385.2125, device='cuda:0')\n",
      "tensor(1442.6860, device='cuda:0')\n",
      "tensor(1435.8127, device='cuda:0')\n",
      "tensor(1385.6194, device='cuda:0')\n",
      "tensor(1423.0961, device='cuda:0')\n",
      "tensor(1397.1876, device='cuda:0')\n",
      "tensor(1429.1403, device='cuda:0')\n",
      "tensor(1383.8939, device='cuda:0')\n",
      "tensor(1420.3212, device='cuda:0')\n",
      "tensor(1428.1782, device='cuda:0')\n",
      "tensor(1405.0005, device='cuda:0')\n",
      "tensor(1403.5986, device='cuda:0')\n",
      "tensor(1366.1530, device='cuda:0')\n",
      "tensor(1409.2677, device='cuda:0')\n",
      "tensor(1393.9169, device='cuda:0')\n",
      "tensor(1411.2216, device='cuda:0')\n",
      "tensor(1401.5234, device='cuda:0')\n",
      "tensor(1419.8258, device='cuda:0')\n",
      "tensor(1402.1510, device='cuda:0')\n",
      "tensor(1382.2053, device='cuda:0')\n",
      "tensor(1428.4900, device='cuda:0')\n",
      "tensor(1432.6130, device='cuda:0')\n",
      "tensor(1432.9546, device='cuda:0')\n",
      "tensor(1402.5962, device='cuda:0')\n",
      "tensor(1414.0640, device='cuda:0')\n",
      "tensor(1390.7435, device='cuda:0')\n",
      "tensor(1421.4161, device='cuda:0')\n",
      "tensor(1396.6892, device='cuda:0')\n",
      "tensor(1438.0793, device='cuda:0')\n",
      "tensor(1434.9421, device='cuda:0')\n",
      "tensor(1422.1665, device='cuda:0')\n",
      "tensor(1381.1476, device='cuda:0')\n",
      "tensor(1420.3358, device='cuda:0')\n",
      "tensor(1404.7455, device='cuda:0')\n",
      "tensor(1395.9352, device='cuda:0')\n",
      "tensor(1432.2086, device='cuda:0')\n",
      "tensor(1462.7440, device='cuda:0')\n",
      "tensor(1455.9264, device='cuda:0')\n",
      "tensor(1434.2323, device='cuda:0')\n",
      "tensor(1412.7642, device='cuda:0')\n",
      "tensor(1416.1359, device='cuda:0')\n",
      "tensor(1436.3552, device='cuda:0')\n",
      "tensor(1424.6135, device='cuda:0')\n",
      "tensor(1405.4802, device='cuda:0')\n",
      "tensor(1382.1775, device='cuda:0')\n",
      "tensor(1444.7761, device='cuda:0')\n",
      "tensor(1391.2672, device='cuda:0')\n",
      "tensor(1407.9937, device='cuda:0')\n",
      "tensor(1444.0236, device='cuda:0')\n",
      "tensor(1425.7430, device='cuda:0')\n",
      "tensor(1434.1818, device='cuda:0')\n",
      "tensor(1388.1573, device='cuda:0')\n",
      "tensor(1378.3517, device='cuda:0')\n",
      "tensor(1433.2560, device='cuda:0')\n",
      "tensor(1413.1733, device='cuda:0')\n",
      "tensor(1425.3997, device='cuda:0')\n",
      "tensor(1415.9795, device='cuda:0')\n",
      "tensor(1426.1742, device='cuda:0')\n",
      "tensor(1401.4117, device='cuda:0')\n",
      "tensor(1400.5546, device='cuda:0')\n",
      "tensor(1409.5291, device='cuda:0')\n",
      "tensor(1424.6991, device='cuda:0')\n",
      "tensor(1404.9939, device='cuda:0')\n",
      "tensor(1416.3623, device='cuda:0')\n",
      "tensor(1413.5933, device='cuda:0')\n",
      "tensor(1414.0509, device='cuda:0')\n",
      "tensor(1419.7173, device='cuda:0')\n",
      "tensor(1429.9849, device='cuda:0')\n",
      "tensor(1408.4830, device='cuda:0')\n",
      "tensor(1403.2603, device='cuda:0')\n",
      "tensor(1394.5729, device='cuda:0')\n",
      "tensor(1414.0874, device='cuda:0')\n",
      "tensor(1465.8975, device='cuda:0')\n",
      "tensor(1451.1857, device='cuda:0')\n",
      "tensor(1470.9673, device='cuda:0')\n",
      "tensor(1380.7628, device='cuda:0')\n",
      "tensor(1412.7468, device='cuda:0')\n",
      "tensor(1426.3804, device='cuda:0')\n",
      "tensor(1405.4205, device='cuda:0')\n",
      "tensor(1428.5094, device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1412.5598, device='cuda:0')\n",
      "tensor(1417.3813, device='cuda:0')\n",
      "tensor(1426.7632, device='cuda:0')\n",
      "tensor(1396.8458, device='cuda:0')\n",
      "tensor(1390.5662, device='cuda:0')\n",
      "tensor(1465.5986, device='cuda:0')\n",
      "tensor(1368.3411, device='cuda:0')\n",
      "tensor(1451.1282, device='cuda:0')\n",
      "tensor(1451.7244, device='cuda:0')\n",
      "tensor(1389.8445, device='cuda:0')\n",
      "tensor(1444.2655, device='cuda:0')\n",
      "tensor(1416.1716, device='cuda:0')\n",
      "tensor(1429.8531, device='cuda:0')\n",
      "tensor(1467.9452, device='cuda:0')\n",
      "tensor(1447.1864, device='cuda:0')\n",
      "tensor(1409.8638, device='cuda:0')\n",
      "tensor(1420.3434, device='cuda:0')\n",
      "tensor(1423.9861, device='cuda:0')\n",
      "tensor(1429.1010, device='cuda:0')\n",
      "tensor(1439.2847, device='cuda:0')\n",
      "tensor(1466.9221, device='cuda:0')\n",
      "tensor(1438.8833, device='cuda:0')\n",
      "tensor(1433.8713, device='cuda:0')\n",
      "tensor(1416.6282, device='cuda:0')\n",
      "tensor(1400.9237, device='cuda:0')\n",
      "tensor(1418.0923, device='cuda:0')\n",
      "tensor(1433.3290, device='cuda:0')\n",
      "tensor(1429.4817, device='cuda:0')\n",
      "tensor(1418.6892, device='cuda:0')\n",
      "tensor(1412.2926, device='cuda:0')\n",
      "tensor(1415.1140, device='cuda:0')\n",
      "tensor(1427.6580, device='cuda:0')\n",
      "tensor(1470.7190, device='cuda:0')\n",
      "tensor(1428.6884, device='cuda:0')\n",
      "tensor(1449.6207, device='cuda:0')\n",
      "tensor(1404.9960, device='cuda:0')\n",
      "tensor(1410.9005, device='cuda:0')\n",
      "tensor(1405.3414, device='cuda:0')\n",
      "tensor(1374.9852, device='cuda:0')\n",
      "tensor(1398.2344, device='cuda:0')\n",
      "tensor(1365.1029, device='cuda:0')\n",
      "tensor(1384.9991, device='cuda:0')\n",
      "tensor(1418.4922, device='cuda:0')\n",
      "tensor(1397.8925, device='cuda:0')\n",
      "tensor(1421.8087, device='cuda:0')\n",
      "tensor(1409.1899, device='cuda:0')\n",
      "tensor(1409.6622, device='cuda:0')\n",
      "tensor(1437.6113, device='cuda:0')\n",
      "tensor(1435.6271, device='cuda:0')\n",
      "tensor(1424.1324, device='cuda:0')\n",
      "tensor(1407.0251, device='cuda:0')\n",
      "tensor(1398.9154, device='cuda:0')\n",
      "tensor(1402.5077, device='cuda:0')\n",
      "tensor(1401.2939, device='cuda:0')\n",
      "tensor(1360.6781, device='cuda:0')\n",
      "tensor(1454.7137, device='cuda:0')\n",
      "tensor(1454.7367, device='cuda:0')\n",
      "tensor(1388.7833, device='cuda:0')\n",
      "tensor(1408.0422, device='cuda:0')\n",
      "tensor(1432.1945, device='cuda:0')\n",
      "tensor(1404.3351, device='cuda:0')\n",
      "tensor(1418.7017, device='cuda:0')\n",
      "tensor(1432.2267, device='cuda:0')\n",
      "tensor(1421.2924, device='cuda:0')\n",
      "tensor(1384.1738, device='cuda:0')\n",
      "tensor(1437.2968, device='cuda:0')\n",
      "tensor(1423.3580, device='cuda:0')\n",
      "tensor(1404.5396, device='cuda:0')\n",
      "tensor(1444.3097, device='cuda:0')\n",
      "tensor(1401.6013, device='cuda:0')\n",
      "tensor(1412.9150, device='cuda:0')\n",
      "tensor(1422.6688, device='cuda:0')\n",
      "tensor(1422.4749, device='cuda:0')\n",
      "tensor(1428.0391, device='cuda:0')\n",
      "tensor(1402.7891, device='cuda:0')\n",
      "tensor(1403.7410, device='cuda:0')\n",
      "tensor(1403.7869, device='cuda:0')\n",
      "tensor(1420.3461, device='cuda:0')\n",
      "tensor(1423.8771, device='cuda:0')\n",
      "tensor(1430.0901, device='cuda:0')\n",
      "tensor(1432.7458, device='cuda:0')\n",
      "tensor(1375.4868, device='cuda:0')\n",
      "tensor(1413.0044, device='cuda:0')\n",
      "tensor(1418.3054, device='cuda:0')\n",
      "tensor(1426.6083, device='cuda:0')\n",
      "tensor(1380.1199, device='cuda:0')\n",
      "tensor(1392.9200, device='cuda:0')\n",
      "tensor(1439.8191, device='cuda:0')\n",
      "tensor(1391.5569, device='cuda:0')\n",
      "tensor(1415.3403, device='cuda:0')\n",
      "tensor(1394.4769, device='cuda:0')\n",
      "tensor(1411.8767, device='cuda:0')\n",
      "tensor(1387.7863, device='cuda:0')\n",
      "tensor(1417.2864, device='cuda:0')\n",
      "tensor(1408.4110, device='cuda:0')\n",
      "tensor(1413.7633, device='cuda:0')\n",
      "tensor(1418.6460, device='cuda:0')\n",
      "tensor(1426.2806, device='cuda:0')\n",
      "tensor(1411.6372, device='cuda:0')\n",
      "tensor(1402.4010, device='cuda:0')\n",
      "tensor(1397.1362, device='cuda:0')\n",
      "tensor(1439.5264, device='cuda:0')\n",
      "tensor(1437.9934, device='cuda:0')\n",
      "tensor(1414.0233, device='cuda:0')\n",
      "tensor(1407.7537, device='cuda:0')\n",
      "tensor(1402.5347, device='cuda:0')\n",
      "tensor(1408.2753, device='cuda:0')\n",
      "tensor(1438.6881, device='cuda:0')\n",
      "tensor(1408.5067, device='cuda:0')\n",
      "tensor(1439.3190, device='cuda:0')\n",
      "tensor(1435.7350, device='cuda:0')\n",
      "tensor(1407.3895, device='cuda:0')\n",
      "tensor(1399.8881, device='cuda:0')\n",
      "tensor(1422.3414, device='cuda:0')\n",
      "tensor(1408.3064, device='cuda:0')\n",
      "tensor(1365.7664, device='cuda:0')\n",
      "tensor(1433.2239, device='cuda:0')\n",
      "tensor(1424.7485, device='cuda:0')\n",
      "tensor(1420.8862, device='cuda:0')\n",
      "tensor(1426.5361, device='cuda:0')\n",
      "tensor(1386.8502, device='cuda:0')\n",
      "tensor(1430.3644, device='cuda:0')\n",
      "tensor(1409.7175, device='cuda:0')\n",
      "tensor(1455.3163, device='cuda:0')\n",
      "tensor(1439.4346, device='cuda:0')\n",
      "tensor(1400.4198, device='cuda:0')\n",
      "tensor(1374.9525, device='cuda:0')\n",
      "tensor(1437.6892, device='cuda:0')\n",
      "tensor(1425.2881, device='cuda:0')\n",
      "tensor(1395.4740, device='cuda:0')\n",
      "tensor(1436.3489, device='cuda:0')\n",
      "tensor(1402.9910, device='cuda:0')\n",
      "tensor(1407.4963, device='cuda:0')\n",
      "tensor(1405.6246, device='cuda:0')\n",
      "tensor(1435.1608, device='cuda:0')\n",
      "tensor(1390.6985, device='cuda:0')\n",
      "tensor(1442.0021, device='cuda:0')\n",
      "tensor(1441.5020, device='cuda:0')\n",
      "tensor(1426.5122, device='cuda:0')\n",
      "tensor(1473.3478, device='cuda:0')\n",
      "tensor(1416.2357, device='cuda:0')\n",
      "tensor(1421.5352, device='cuda:0')\n",
      "tensor(1431.3108, device='cuda:0')\n",
      "tensor(1415.8021, device='cuda:0')\n",
      "tensor(1395.7290, device='cuda:0')\n",
      "tensor(1422.5176, device='cuda:0')\n",
      "tensor(1430.6995, device='cuda:0')\n",
      "tensor(1417.5221, device='cuda:0')\n",
      "tensor(1413.3586, device='cuda:0')\n",
      "tensor(1408.5188, device='cuda:0')\n",
      "tensor(1414.3170, device='cuda:0')\n",
      "tensor(1447.1042, device='cuda:0')\n",
      "tensor(1385.1370, device='cuda:0')\n",
      "tensor(1418.7260, device='cuda:0')\n",
      "tensor(1380.2251, device='cuda:0')\n",
      "tensor(1404.6027, device='cuda:0')\n",
      "tensor(1417.0959, device='cuda:0')\n",
      "tensor(1439.8796, device='cuda:0')\n",
      "tensor(1417.5627, device='cuda:0')\n",
      "tensor(1433.6448, device='cuda:0')\n",
      "tensor(1413.6857, device='cuda:0')\n",
      "tensor(1436.9205, device='cuda:0')\n",
      "tensor(1425.8083, device='cuda:0')\n",
      "tensor(1433.5669, device='cuda:0')\n",
      "tensor(1395.6774, device='cuda:0')\n",
      "tensor(1384.6754, device='cuda:0')\n",
      "tensor(1405.2471, device='cuda:0')\n",
      "tensor(1401.4568, device='cuda:0')\n",
      "tensor(1426.3107, device='cuda:0')\n",
      "tensor(1439.5176, device='cuda:0')\n",
      "tensor(1419.9882, device='cuda:0')\n",
      "tensor(1380.9194, device='cuda:0')\n",
      "tensor(1413.2659, device='cuda:0')\n",
      "tensor(1411.4631, device='cuda:0')\n",
      "tensor(1439.6840, device='cuda:0')\n",
      "tensor(1385.5994, device='cuda:0')\n",
      "tensor(1429.2371, device='cuda:0')\n",
      "tensor(1394.6469, device='cuda:0')\n",
      "tensor(1439.3892, device='cuda:0')\n",
      "tensor(1415.7286, device='cuda:0')\n",
      "tensor(1391.0919, device='cuda:0')\n",
      "tensor(1382.4852, device='cuda:0')\n",
      "tensor(1386.4874, device='cuda:0')\n",
      "tensor(1422.5387, device='cuda:0')\n",
      "tensor(1447.5110, device='cuda:0')\n",
      "tensor(1416.0176, device='cuda:0')\n",
      "tensor(1400.7793, device='cuda:0')\n",
      "tensor(1398.1163, device='cuda:0')\n",
      "tensor(1427.2731, device='cuda:0')\n",
      "tensor(1425.4888, device='cuda:0')\n",
      "tensor(1439.1068, device='cuda:0')\n",
      "tensor(1433.0099, device='cuda:0')\n",
      "tensor(1408.9768, device='cuda:0')\n",
      "tensor(1389.0740, device='cuda:0')\n",
      "tensor(1405.4551, device='cuda:0')\n",
      "tensor(1417.8910, device='cuda:0')\n",
      "tensor(1392.1174, device='cuda:0')\n",
      "tensor(1416.3024, device='cuda:0')\n",
      "tensor(1441.5596, device='cuda:0')\n",
      "tensor(1418.6179, device='cuda:0')\n",
      "tensor(1421.4349, device='cuda:0')\n",
      "tensor(1468.2260, device='cuda:0')\n",
      "tensor(1448.2383, device='cuda:0')\n",
      "tensor(1412.8549, device='cuda:0')\n",
      "tensor(1443.3477, device='cuda:0')\n",
      "tensor(1416.3853, device='cuda:0')\n",
      "tensor(1406.0111, device='cuda:0')\n",
      "tensor(1443.9180, device='cuda:0')\n",
      "tensor(1425.6541, device='cuda:0')\n",
      "tensor(1413.3318, device='cuda:0')\n",
      "tensor(1421.2944, device='cuda:0')\n",
      "tensor(1419.3005, device='cuda:0')\n",
      "tensor(1430.3191, device='cuda:0')\n",
      "tensor(1412.7074, device='cuda:0')\n",
      "tensor(1421.5516, device='cuda:0')\n",
      "tensor(1413.7882, device='cuda:0')\n",
      "tensor(1436.7692, device='cuda:0')\n",
      "tensor(1425.3563, device='cuda:0')\n",
      "tensor(1417.0079, device='cuda:0')\n",
      "tensor(1403.8787, device='cuda:0')\n",
      "tensor(1441.4609, device='cuda:0')\n",
      "tensor(1414.7581, device='cuda:0')\n",
      "tensor(1414.9227, device='cuda:0')\n",
      "tensor(1417.8657, device='cuda:0')\n",
      "tensor(1446.0958, device='cuda:0')\n",
      "tensor(1402.4999, device='cuda:0')\n",
      "tensor(1414.6422, device='cuda:0')\n",
      "tensor(1410.7300, device='cuda:0')\n",
      "tensor(1400.5583, device='cuda:0')\n",
      "tensor(1426.6515, device='cuda:0')\n",
      "tensor(1443.7634, device='cuda:0')\n",
      "tensor(1426.3607, device='cuda:0')\n",
      "tensor(1402.7223, device='cuda:0')\n",
      "tensor(1415.4741, device='cuda:0')\n",
      "tensor(1404.5396, device='cuda:0')\n",
      "tensor(1414.0891, device='cuda:0')\n",
      "tensor(1420.7969, device='cuda:0')\n",
      "tensor(1396.9275, device='cuda:0')\n",
      "tensor(1458.3097, device='cuda:0')\n",
      "tensor(1434.4164, device='cuda:0')\n",
      "tensor(1393.2557, device='cuda:0')\n",
      "tensor(1441.0255, device='cuda:0')\n",
      "tensor(1380.6505, device='cuda:0')\n",
      "tensor(1409.9803, device='cuda:0')\n",
      "tensor(1368.8228, device='cuda:0')\n",
      "tensor(1400.6229, device='cuda:0')\n",
      "tensor(1415.9321, device='cuda:0')\n",
      "tensor(1424.6715, device='cuda:0')\n",
      "tensor(1385.3785, device='cuda:0')\n",
      "tensor(1417.8522, device='cuda:0')\n",
      "tensor(1421.3883, device='cuda:0')\n",
      "tensor(1449.0311, device='cuda:0')\n",
      "tensor(1426.6940, device='cuda:0')\n",
      "tensor(1438.8097, device='cuda:0')\n",
      "tensor(1407.8922, device='cuda:0')\n",
      "tensor(1435.3757, device='cuda:0')\n",
      "tensor(1455.0043, device='cuda:0')\n",
      "tensor(1391.4935, device='cuda:0')\n",
      "tensor(1417.1824, device='cuda:0')\n",
      "tensor(1428.1558, device='cuda:0')\n",
      "tensor(1431.2241, device='cuda:0')\n",
      "tensor(1420.5492, device='cuda:0')\n",
      "tensor(1425.2817, device='cuda:0')\n",
      "tensor(1389.0997, device='cuda:0')\n",
      "tensor(1409.1003, device='cuda:0')\n",
      "tensor(1409.8082, device='cuda:0')\n",
      "tensor(1379.0896, device='cuda:0')\n",
      "tensor(1408.6285, device='cuda:0')\n",
      "tensor(1383.6857, device='cuda:0')\n",
      "tensor(1410.5594, device='cuda:0')\n",
      "tensor(1400.9836, device='cuda:0')\n",
      "tensor(1405.1199, device='cuda:0')\n",
      "tensor(1449.0201, device='cuda:0')\n",
      "tensor(1381.7177, device='cuda:0')\n",
      "tensor(1435.9558, device='cuda:0')\n",
      "tensor(1395.7903, device='cuda:0')\n",
      "tensor(1414.4789, device='cuda:0')\n",
      "tensor(1418.1703, device='cuda:0')\n",
      "tensor(1439.0444, device='cuda:0')\n",
      "tensor(1433.3157, device='cuda:0')\n",
      "tensor(1450.6144, device='cuda:0')\n",
      "tensor(1382.4734, device='cuda:0')\n",
      "tensor(1421.0452, device='cuda:0')\n",
      "tensor(1432.8387, device='cuda:0')\n",
      "tensor(1416.5317, device='cuda:0')\n",
      "tensor(1449.9954, device='cuda:0')\n",
      "tensor(1382.6801, device='cuda:0')\n",
      "tensor(1448.8501, device='cuda:0')\n",
      "tensor(1417.5383, device='cuda:0')\n",
      "tensor(1430.1782, device='cuda:0')\n",
      "tensor(1435.8892, device='cuda:0')\n",
      "tensor(1367.6129, device='cuda:0')\n",
      "tensor(1384.8370, device='cuda:0')\n",
      "tensor(1377.2517, device='cuda:0')\n",
      "tensor(1392.1387, device='cuda:0')\n",
      "tensor(1412.3425, device='cuda:0')\n",
      "tensor(1441.9358, device='cuda:0')\n",
      "tensor(1447.9532, device='cuda:0')\n",
      "tensor(1404.0717, device='cuda:0')\n",
      "tensor(1424.9260, device='cuda:0')\n",
      "tensor(1428.9032, device='cuda:0')\n",
      "tensor(1444.6342, device='cuda:0')\n",
      "tensor(1446.0267, device='cuda:0')\n",
      "tensor(1434.1827, device='cuda:0')\n",
      "tensor(1392.0759, device='cuda:0')\n",
      "tensor(1375.9364, device='cuda:0')\n",
      "tensor(1442.5314, device='cuda:0')\n",
      "tensor(1428.5896, device='cuda:0')\n",
      "tensor(1378.6542, device='cuda:0')\n",
      "tensor(1398.1370, device='cuda:0')\n",
      "tensor(1400.0981, device='cuda:0')\n",
      "tensor(1431.5441, device='cuda:0')\n",
      "tensor(1403.0608, device='cuda:0')\n",
      "tensor(1440.6503, device='cuda:0')\n",
      "tensor(1429.8313, device='cuda:0')\n",
      "tensor(1441.6162, device='cuda:0')\n",
      "tensor(1391.8026, device='cuda:0')\n",
      "tensor(1425.2073, device='cuda:0')\n",
      "tensor(1423.4188, device='cuda:0')\n",
      "tensor(1389.9656, device='cuda:0')\n",
      "tensor(1411.0577, device='cuda:0')\n",
      "tensor(1423.1200, device='cuda:0')\n",
      "tensor(1401.7646, device='cuda:0')\n",
      "tensor(1408.5344, device='cuda:0')\n",
      "tensor(1412.2050, device='cuda:0')\n",
      "tensor(1436.6595, device='cuda:0')\n",
      "tensor(1407.4791, device='cuda:0')\n",
      "tensor(1394.5499, device='cuda:0')\n",
      "tensor(1413.3774, device='cuda:0')\n",
      "tensor(1415.2762, device='cuda:0')\n",
      "tensor(1453.4368, device='cuda:0')\n",
      "tensor(1434.2491, device='cuda:0')\n",
      "tensor(1388.5255, device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1383.2640, device='cuda:0')\n",
      "tensor(1424.7712, device='cuda:0')\n",
      "tensor(1397.3434, device='cuda:0')\n",
      "tensor(1418.6311, device='cuda:0')\n",
      "tensor(1402.5612, device='cuda:0')\n",
      "tensor(1440.5050, device='cuda:0')\n",
      "tensor(1402.3740, device='cuda:0')\n",
      "tensor(1408.8771, device='cuda:0')\n",
      "tensor(1420.6165, device='cuda:0')\n",
      "tensor(1427.5487, device='cuda:0')\n",
      "tensor(1410.8777, device='cuda:0')\n",
      "tensor(1383.2748, device='cuda:0')\n",
      "tensor(1395.0757, device='cuda:0')\n",
      "tensor(1393.4822, device='cuda:0')\n",
      "tensor(1393.1779, device='cuda:0')\n",
      "tensor(1441.6558, device='cuda:0')\n",
      "tensor(1444.5702, device='cuda:0')\n",
      "tensor(1421.3900, device='cuda:0')\n",
      "tensor(1431.2898, device='cuda:0')\n",
      "tensor(1430.2230, device='cuda:0')\n",
      "tensor(1386.1057, device='cuda:0')\n",
      "tensor(1405.4774, device='cuda:0')\n",
      "tensor(1450.7740, device='cuda:0')\n",
      "tensor(1401.5381, device='cuda:0')\n",
      "tensor(1438.6777, device='cuda:0')\n",
      "tensor(1416.8682, device='cuda:0')\n",
      "tensor(1404.4166, device='cuda:0')\n",
      "tensor(1412.6957, device='cuda:0')\n",
      "tensor(1429.1792, device='cuda:0')\n",
      "tensor(1447.5914, device='cuda:0')\n",
      "tensor(1384.9110, device='cuda:0')\n",
      "tensor(1375.4346, device='cuda:0')\n",
      "tensor(1417.9133, device='cuda:0')\n",
      "tensor(1425.8375, device='cuda:0')\n",
      "tensor(1422.2826, device='cuda:0')\n",
      "tensor(1416.1715, device='cuda:0')\n",
      "tensor(1448.8301, device='cuda:0')\n",
      "tensor(1426.6920, device='cuda:0')\n",
      "tensor(1428.4994, device='cuda:0')\n",
      "tensor(1429.9166, device='cuda:0')\n",
      "tensor(1403.6028, device='cuda:0')\n",
      "tensor(1366.8010, device='cuda:0')\n",
      "tensor(1396.8983, device='cuda:0')\n",
      "tensor(1446.0986, device='cuda:0')\n",
      "tensor(1402.0897, device='cuda:0')\n",
      "tensor(1417.4302, device='cuda:0')\n",
      "tensor(1395.6691, device='cuda:0')\n",
      "tensor(1426.8137, device='cuda:0')\n",
      "tensor(1409.6072, device='cuda:0')\n",
      "tensor(1425.8777, device='cuda:0')\n",
      "tensor(1395.2622, device='cuda:0')\n",
      "tensor(1450.3542, device='cuda:0')\n",
      "tensor(1416.1274, device='cuda:0')\n",
      "tensor(1428.4930, device='cuda:0')\n",
      "tensor(1414.3604, device='cuda:0')\n",
      "tensor(1418.1160, device='cuda:0')\n",
      "tensor(1410.1843, device='cuda:0')\n",
      "tensor(1406.0295, device='cuda:0')\n",
      "tensor(1390.4836, device='cuda:0')\n",
      "tensor(1416.7998, device='cuda:0')\n",
      "tensor(1420.3181, device='cuda:0')\n",
      "tensor(1414.9705, device='cuda:0')\n",
      "tensor(1441.2191, device='cuda:0')\n",
      "tensor(1432.8761, device='cuda:0')\n",
      "tensor(1409.1808, device='cuda:0')\n",
      "tensor(1423.1477, device='cuda:0')\n",
      "tensor(1381.2469, device='cuda:0')\n",
      "tensor(1382.9114, device='cuda:0')\n",
      "tensor(1367.7100, device='cuda:0')\n",
      "tensor(1408.9882, device='cuda:0')\n",
      "tensor(1422.5929, device='cuda:0')\n",
      "tensor(1479.0715, device='cuda:0')\n",
      "tensor(1444.9457, device='cuda:0')\n",
      "tensor(1404.3279, device='cuda:0')\n",
      "tensor(1406.7020, device='cuda:0')\n",
      "tensor(1401.9086, device='cuda:0')\n",
      "tensor(1411.5190, device='cuda:0')\n",
      "tensor(1412.3406, device='cuda:0')\n",
      "tensor(1407.0355, device='cuda:0')\n",
      "tensor(1425.3019, device='cuda:0')\n",
      "tensor(1471.7324, device='cuda:0')\n",
      "tensor(1458.9967, device='cuda:0')\n",
      "tensor(1458.5677, device='cuda:0')\n",
      "tensor(1421.5378, device='cuda:0')\n",
      "tensor(1424.3644, device='cuda:0')\n",
      "tensor(1399.3219, device='cuda:0')\n",
      "tensor(1426.1769, device='cuda:0')\n",
      "tensor(1428.2299, device='cuda:0')\n",
      "tensor(1394.2687, device='cuda:0')\n",
      "tensor(1450.3566, device='cuda:0')\n",
      "tensor(1429.1484, device='cuda:0')\n",
      "tensor(1409.4882, device='cuda:0')\n",
      "tensor(1401.9738, device='cuda:0')\n",
      "tensor(1408.9937, device='cuda:0')\n",
      "tensor(1431.1775, device='cuda:0')\n",
      "tensor(1446.2382, device='cuda:0')\n",
      "tensor(1389.8379, device='cuda:0')\n",
      "tensor(1380.1997, device='cuda:0')\n",
      "tensor(1434.7024, device='cuda:0')\n",
      "tensor(1413.0824, device='cuda:0')\n",
      "tensor(1375.3699, device='cuda:0')\n",
      "tensor(1456.1954, device='cuda:0')\n",
      "tensor(1448.0901, device='cuda:0')\n",
      "tensor(1411.6409, device='cuda:0')\n",
      "tensor(1381.5652, device='cuda:0')\n",
      "tensor(1434.2756, device='cuda:0')\n",
      "tensor(1439.8295, device='cuda:0')\n",
      "tensor(1403.3726, device='cuda:0')\n",
      "tensor(1388.3306, device='cuda:0')\n",
      "tensor(1420.6050, device='cuda:0')\n",
      "tensor(1438.3170, device='cuda:0')\n",
      "tensor(1422.2806, device='cuda:0')\n",
      "tensor(1403.1416, device='cuda:0')\n",
      "tensor(1455.1946, device='cuda:0')\n",
      "tensor(1393.4119, device='cuda:0')\n",
      "tensor(1429.4374, device='cuda:0')\n",
      "tensor(1388.5776, device='cuda:0')\n",
      "tensor(1402.1976, device='cuda:0')\n",
      "tensor(1419.8860, device='cuda:0')\n",
      "tensor(1418.9652, device='cuda:0')\n",
      "tensor(1420.8337, device='cuda:0')\n",
      "tensor(1414.4735, device='cuda:0')\n",
      "tensor(1408.0876, device='cuda:0')\n",
      "tensor(1456.0737, device='cuda:0')\n",
      "tensor(1406.4535, device='cuda:0')\n",
      "tensor(1384.9897, device='cuda:0')\n",
      "tensor(1414.2520, device='cuda:0')\n",
      "tensor(1413.4073, device='cuda:0')\n",
      "tensor(1400.8571, device='cuda:0')\n",
      "tensor(1380.0747, device='cuda:0')\n",
      "tensor(1377.0696, device='cuda:0')\n",
      "tensor(1379.1771, device='cuda:0')\n",
      "tensor(1399.6715, device='cuda:0')\n",
      "tensor(1441.7579, device='cuda:0')\n",
      "tensor(1399.0868, device='cuda:0')\n",
      "tensor(1456.7535, device='cuda:0')\n",
      "tensor(1401.8491, device='cuda:0')\n",
      "tensor(1413.5541, device='cuda:0')\n",
      "tensor(1429.7538, device='cuda:0')\n",
      "tensor(1406.7175, device='cuda:0')\n",
      "tensor(1453.6948, device='cuda:0')\n",
      "tensor(1402.9825, device='cuda:0')\n",
      "tensor(1411.7798, device='cuda:0')\n",
      "tensor(1441.6416, device='cuda:0')\n",
      "tensor(1404.1726, device='cuda:0')\n",
      "tensor(1432.4253, device='cuda:0')\n",
      "tensor(1413.1604, device='cuda:0')\n",
      "tensor(1401.4050, device='cuda:0')\n",
      "tensor(1425.7926, device='cuda:0')\n",
      "tensor(1413.5303, device='cuda:0')\n",
      "tensor(1405.1923, device='cuda:0')\n",
      "tensor(1450.2354, device='cuda:0')\n",
      "tensor(1407.7283, device='cuda:0')\n",
      "tensor(1436.4534, device='cuda:0')\n",
      "tensor(1384.6461, device='cuda:0')\n",
      "tensor(1398.5176, device='cuda:0')\n",
      "tensor(1438.2490, device='cuda:0')\n",
      "tensor(1457.0647, device='cuda:0')\n",
      "tensor(1426.2281, device='cuda:0')\n",
      "tensor(1419.3256, device='cuda:0')\n",
      "tensor(1399.1471, device='cuda:0')\n",
      "tensor(1423.1528, device='cuda:0')\n",
      "tensor(1397.5256, device='cuda:0')\n",
      "tensor(1415.1674, device='cuda:0')\n",
      "tensor(1407.4146, device='cuda:0')\n",
      "tensor(1452.3379, device='cuda:0')\n",
      "tensor(1446.0323, device='cuda:0')\n",
      "tensor(1441.5829, device='cuda:0')\n",
      "tensor(1407.5884, device='cuda:0')\n",
      "tensor(1420.2661, device='cuda:0')\n",
      "tensor(1431.3302, device='cuda:0')\n",
      "tensor(1410.5631, device='cuda:0')\n",
      "tensor(1417.2163, device='cuda:0')\n",
      "tensor(1437.8044, device='cuda:0')\n",
      "tensor(1412.6334, device='cuda:0')\n",
      "tensor(1436.4994, device='cuda:0')\n",
      "tensor(1418.1632, device='cuda:0')\n",
      "tensor(1420.6115, device='cuda:0')\n",
      "tensor(1439.8173, device='cuda:0')\n",
      "tensor(1414.1357, device='cuda:0')\n",
      "tensor(1394.1591, device='cuda:0')\n",
      "tensor(1401.5677, device='cuda:0')\n",
      "tensor(1402.8163, device='cuda:0')\n",
      "tensor(1418.9330, device='cuda:0')\n",
      "tensor(1441.8569, device='cuda:0')\n",
      "tensor(1423.7625, device='cuda:0')\n",
      "tensor(1399.2745, device='cuda:0')\n",
      "tensor(1393.4146, device='cuda:0')\n",
      "tensor(1401.2682, device='cuda:0')\n",
      "tensor(1424.5221, device='cuda:0')\n",
      "tensor(1394.0769, device='cuda:0')\n",
      "tensor(1366.8616, device='cuda:0')\n",
      "tensor(1410.2253, device='cuda:0')\n",
      "tensor(1399.7590, device='cuda:0')\n",
      "tensor(1407.2003, device='cuda:0')\n",
      "tensor(1418.5439, device='cuda:0')\n",
      "tensor(1460.2466, device='cuda:0')\n",
      "tensor(1429.7393, device='cuda:0')\n",
      "tensor(1385.3613, device='cuda:0')\n",
      "tensor(1438.1396, device='cuda:0')\n",
      "tensor(1390.1895, device='cuda:0')\n",
      "tensor(1463.1154, device='cuda:0')\n",
      "tensor(1409.0605, device='cuda:0')\n",
      "tensor(1412.3632, device='cuda:0')\n",
      "tensor(1437.3600, device='cuda:0')\n",
      "tensor(1395.8154, device='cuda:0')\n",
      "tensor(1365.6179, device='cuda:0')\n",
      "tensor(1441.3519, device='cuda:0')\n",
      "tensor(1386.6602, device='cuda:0')\n",
      "tensor(1423.1627, device='cuda:0')\n",
      "tensor(1410.9005, device='cuda:0')\n",
      "tensor(1390.1128, device='cuda:0')\n",
      "tensor(1435.8666, device='cuda:0')\n",
      "tensor(1444.9093, device='cuda:0')\n",
      "tensor(1387.8092, device='cuda:0')\n",
      "tensor(1444.0508, device='cuda:0')\n",
      "tensor(1418.8849, device='cuda:0')\n",
      "tensor(1390.7021, device='cuda:0')\n",
      "tensor(1424.4188, device='cuda:0')\n",
      "tensor(1444.6207, device='cuda:0')\n",
      "tensor(1423.9720, device='cuda:0')\n",
      "tensor(1408.1212, device='cuda:0')\n",
      "tensor(1435.5199, device='cuda:0')\n",
      "tensor(1439.0817, device='cuda:0')\n",
      "tensor(1428.3363, device='cuda:0')\n",
      "tensor(1420.5914, device='cuda:0')\n",
      "tensor(1456.4099, device='cuda:0')\n",
      "tensor(1443.0768, device='cuda:0')\n",
      "tensor(1404.6733, device='cuda:0')\n",
      "tensor(1383.9767, device='cuda:0')\n",
      "tensor(1411.3325, device='cuda:0')\n",
      "tensor(1444.2469, device='cuda:0')\n",
      "tensor(1422.6304, device='cuda:0')\n",
      "tensor(1406.2065, device='cuda:0')\n",
      "tensor(1426.7931, device='cuda:0')\n",
      "tensor(1442.3892, device='cuda:0')\n",
      "tensor(1475.6561, device='cuda:0')\n",
      "tensor(1426.5789, device='cuda:0')\n",
      "tensor(1421.5833, device='cuda:0')\n",
      "tensor(1430.1593, device='cuda:0')\n",
      "tensor(1438.6716, device='cuda:0')\n",
      "tensor(1432.8663, device='cuda:0')\n",
      "tensor(1440.8177, device='cuda:0')\n",
      "tensor(1453.5370, device='cuda:0')\n",
      "tensor(1414.8185, device='cuda:0')\n",
      "tensor(1409.4082, device='cuda:0')\n",
      "tensor(1443.3878, device='cuda:0')\n",
      "tensor(1410.9117, device='cuda:0')\n",
      "tensor(1450.1927, device='cuda:0')\n",
      "tensor(1439.9794, device='cuda:0')\n",
      "tensor(1455.7130, device='cuda:0')\n",
      "tensor(1408.6846, device='cuda:0')\n",
      "tensor(1370.5558, device='cuda:0')\n",
      "tensor(1411.1130, device='cuda:0')\n",
      "tensor(1425.2698, device='cuda:0')\n",
      "tensor(1415.9679, device='cuda:0')\n",
      "tensor(1426.5792, device='cuda:0')\n",
      "tensor(1430.0621, device='cuda:0')\n",
      "tensor(1429.0353, device='cuda:0')\n",
      "tensor(1436.3405, device='cuda:0')\n",
      "tensor(1415.4597, device='cuda:0')\n",
      "tensor(1398.7305, device='cuda:0')\n",
      "tensor(1412.2063, device='cuda:0')\n",
      "tensor(1396.1304, device='cuda:0')\n",
      "tensor(1392.3531, device='cuda:0')\n",
      "tensor(1447.8683, device='cuda:0')\n",
      "tensor(1408.5255, device='cuda:0')\n",
      "tensor(1443.4084, device='cuda:0')\n",
      "tensor(1392.5245, device='cuda:0')\n",
      "tensor(1424.6589, device='cuda:0')\n",
      "tensor(1386.9657, device='cuda:0')\n",
      "tensor(1404.4888, device='cuda:0')\n",
      "tensor(1411.2113, device='cuda:0')\n",
      "tensor(1418.5787, device='cuda:0')\n",
      "tensor(1425.8960, device='cuda:0')\n",
      "tensor(1429.8405, device='cuda:0')\n",
      "tensor(1439.3208, device='cuda:0')\n",
      "tensor(1435.7345, device='cuda:0')\n",
      "tensor(1378.0029, device='cuda:0')\n",
      "tensor(1435.4418, device='cuda:0')\n",
      "tensor(1424.1644, device='cuda:0')\n",
      "tensor(1433.0961, device='cuda:0')\n",
      "tensor(1409.1449, device='cuda:0')\n",
      "tensor(1435.2638, device='cuda:0')\n",
      "tensor(1427.7124, device='cuda:0')\n",
      "tensor(1399.4819, device='cuda:0')\n",
      "tensor(1431.6205, device='cuda:0')\n",
      "tensor(1450.2629, device='cuda:0')\n",
      "tensor(1408.6251, device='cuda:0')\n",
      "tensor(1441.3998, device='cuda:0')\n",
      "tensor(1424.9849, device='cuda:0')\n",
      "tensor(1433.3588, device='cuda:0')\n",
      "tensor(1412.4851, device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1411.2235, device='cuda:0')\n",
      "tensor(1418.6223, device='cuda:0')\n",
      "tensor(1390.9233, device='cuda:0')\n",
      "tensor(1391.8456, device='cuda:0')\n",
      "tensor(1389.3798, device='cuda:0')\n",
      "tensor(1441.3324, device='cuda:0')\n",
      "tensor(1425.0355, device='cuda:0')\n",
      "tensor(1428.2485, device='cuda:0')\n",
      "tensor(1414.7605, device='cuda:0')\n",
      "tensor(1451.6646, device='cuda:0')\n",
      "tensor(1421.8340, device='cuda:0')\n",
      "tensor(1419.5419, device='cuda:0')\n",
      "tensor(1429.1060, device='cuda:0')\n",
      "tensor(1424.3914, device='cuda:0')\n",
      "tensor(1463.5829, device='cuda:0')\n",
      "tensor(1442.8510, device='cuda:0')\n",
      "tensor(1418.9368, device='cuda:0')\n",
      "tensor(1417.0631, device='cuda:0')\n",
      "tensor(1423.2100, device='cuda:0')\n",
      "tensor(1418.9729, device='cuda:0')\n",
      "tensor(1384.8419, device='cuda:0')\n",
      "tensor(1415.7753, device='cuda:0')\n",
      "tensor(1446.4684, device='cuda:0')\n",
      "tensor(1367.5852, device='cuda:0')\n",
      "tensor(1400.4757, device='cuda:0')\n",
      "tensor(1425.3563, device='cuda:0')\n",
      "tensor(1433.1383, device='cuda:0')\n",
      "tensor(1422.0845, device='cuda:0')\n",
      "tensor(1415.9822, device='cuda:0')\n",
      "tensor(1424.4999, device='cuda:0')\n",
      "tensor(1373.3406, device='cuda:0')\n",
      "tensor(1412.9338, device='cuda:0')\n",
      "tensor(1413.8733, device='cuda:0')\n",
      "tensor(1384.5674, device='cuda:0')\n",
      "tensor(1439.4919, device='cuda:0')\n",
      "tensor(1423.0312, device='cuda:0')\n",
      "tensor(1413.1295, device='cuda:0')\n",
      "tensor(1444.2146, device='cuda:0')\n",
      "tensor(1433.5981, device='cuda:0')\n",
      "tensor(1446.2844, device='cuda:0')\n",
      "tensor(1436.2203, device='cuda:0')\n",
      "tensor(1388.2151, device='cuda:0')\n",
      "tensor(1410.3483, device='cuda:0')\n",
      "tensor(1428.2197, device='cuda:0')\n",
      "tensor(1467.3116, device='cuda:0')\n",
      "tensor(1387.5735, device='cuda:0')\n",
      "tensor(1418.7992, device='cuda:0')\n",
      "tensor(1389.2145, device='cuda:0')\n",
      "tensor(1441.4642, device='cuda:0')\n",
      "tensor(1430.0614, device='cuda:0')\n",
      "tensor(1425.9270, device='cuda:0')\n",
      "tensor(1379.5526, device='cuda:0')\n",
      "Time taken: 1.1767361164093018\n",
      "实验一消耗时间：0.083892822265625，实验二消耗时间：1.1767361164093018\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 生成随机矩阵\n",
    "matrices = [torch.randn(100, 100).to(device) for i in range(1000)]\n",
    "\n",
    "# 实验一：计算时间\n",
    "start_time_1 = time.time()\n",
    "for i in range(1000):\n",
    "    result = torch.mm(matrices[i], matrices[i].t())\n",
    "    frobenius_norm = torch.norm(result)\n",
    "#     print(frobenius_norm)\n",
    "end_time_1 = time.time()\n",
    "print(\"Time taken:\", end_time_1 - start_time_1)\n",
    "\n",
    "# 实验二：计算时间\n",
    "start_time_2 = time.time()\n",
    "for i in range(1000):\n",
    "    result = torch.mm(matrices[i], matrices[i].t())\n",
    "    frobenius_norm = torch.norm(result)\n",
    "    print(frobenius_norm)\n",
    "end_time_2 = time.time()\n",
    "print(\"Time taken:\", end_time_2 - start_time_2)\n",
    "\n",
    "print(f'实验一消耗时间：{end_time_1 - start_time_1}，实验二消耗时间：{end_time_2 - start_time_2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 5.6.4\n",
    "\n",
    "测量同时在两个GPU上执行两个矩阵乘法与在一个GPU上按顺序执行两个矩阵乘法所需的时间。提示：应该看到近乎线性的缩放。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;执行两个矩阵乘法并行在两个GPU上所需的时间通常会比在单个GPU上按顺序执行这两个操作要快得多。但实际的时间取决于矩阵的大小、硬件配置和算法实现。\n",
    "\n",
    "&emsp;&emsp;但由于笔者只有一张卡，所以只做了在单个GPU上顺序执行两个矩阵乘法的实验。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential time: 0.00299954 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "# 创建两个随机矩阵\n",
    "a = torch.randn(10000, 10000).cuda()\n",
    "b = torch.randn(10000, 10000).cuda()\n",
    "\n",
    "# 顺序计算\n",
    "start_time = time.time()\n",
    "c1 = torch.matmul(a, b)\n",
    "c2 = torch.matmul(a, b)\n",
    "end_time = time.time()\n",
    "sequential_time = end_time - start_time\n",
    "\n",
    "print(f\"Sequential time: {sequential_time:.8f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
