{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "from utlis import *\n",
    "import models\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "def train_models(net, \n",
    "                 train_loader, \n",
    "                 test_loader, \n",
    "                 epochs, \n",
    "                 lr, \n",
    "                 net_type : str =  None,\n",
    "                 use_wandb : bool = False,\n",
    "                 device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')):\n",
    "    # 设置优化器\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "    # 设置损失函数\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "    net.apply(init_weights)\n",
    "    net = net.to(device)\n",
    "    \n",
    "    epoch_list = []\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    \n",
    "    if use_wandb:\n",
    "        wandb.watch(net, log=\"all\")\n",
    "\n",
    "    print('training on', device)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "    # 训练\n",
    "        loss_sum = 0\n",
    "        acc = 0\n",
    "        net.train()\n",
    "        for idx, (x, y) in enumerate(train_loader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predict = net(x)\n",
    "            l = loss(predict, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_sum += l.item() * x.size(0)\n",
    "            acc += predict.max(dim=1)[1].eq(y).sum().item()\n",
    "            if (idx + 1) % (len(train_loader) // 5) == 0 or idx == len(train_loader) - 1:\n",
    "                if (idx + 1) != len(train_loader):\n",
    "                    loss_now = loss_sum / (idx + 1) / x.size(0)\n",
    "                    acc_now = acc / (idx + 1) / x.size(0)\n",
    "                else:\n",
    "                    loss_now = loss_sum / len(train_loader.dataset)\n",
    "                    acc_now = acc / len(train_loader.dataset)\n",
    "                \n",
    "                if use_wandb:\n",
    "                    wandb.log({\"train_loss\": loss_now, \"train_acc\": acc_now})\n",
    "                train_acc.append(acc_now)\n",
    "                train_loss.append(loss_now)\n",
    "                epoch_list.append(epoch + (idx + 1) / len(train_loader))         \n",
    "        # loss_sum /= len(train_loader.dataset)\n",
    "        # acc /= len(train_loader.dataset)\n",
    "        # train_loss.append(loss_sum)\n",
    "        # train_acc.append(acc)\n",
    "            \n",
    "        # 测试\n",
    "        loss_sum = 0\n",
    "        acc = 0\n",
    "        for idx, (x, y) in enumerate(test_loader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            predict = net(x)\n",
    "            l = loss(predict, y)\n",
    "            \n",
    "            acc += predict.max(dim=1)[1].eq(y).sum().item()\n",
    "            \n",
    "        acc /= len(test_loader.dataset)\n",
    "        test_acc.append(acc)\n",
    "        # print(epoch_list)\n",
    "        \n",
    "        # print('epoch: {}, train loss: {:.4f}, train acc: {:.4f}, test acc: {:.4f}'.format(epoch + 1, train_loss[-1], train_acc[-1], test_acc[-1]))\n",
    "    # print(len(train_loss), len(train_acc), len(test_acc))\n",
    "    \n",
    "    epoch_test = list(range(1, int(epoch_list[-1]) + 1, 1))\n",
    "    \n",
    "    if net_type is not None: \n",
    "    \n",
    "        write2csv('ch7_01.csv', epoch_list, net_type + '_epoch_train')\n",
    "        write2csv('ch7_01.csv', train_loss, net_type + '_train_loss')\n",
    "        write2csv('ch7_01.csv', train_acc, net_type + '_train_acc')\n",
    "        write2csv('ch7_01_eval.csv', epoch_test, net_type + '_epoch_test')\n",
    "        write2csv('ch7_01_eval.csv', test_acc, net_type + '_test_acc')\n",
    "        print('record the data')\n",
    "    else:\n",
    "        print('net_type is None, don\\'t record the data')\n",
    "    \n",
    "    # draw_loss_acc(train_loss, train_acc, test_acc, epoch_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 5)\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "AlexNet = models.AlexNet()\n",
    "LeNet = models.LeNet()\n",
    "train_loader, test_loader = load_data_fashion_mnist(batch_size=128)\n",
    "# print(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, train loss: 2.3141, train acc: 0.1026, test acc: 0.1000\n",
      "epoch: 2, train loss: 1.4155, train acc: 0.4433, test acc: 0.6476\n",
      "epoch: 3, train loss: 0.7276, train acc: 0.7164, test acc: 0.7335\n",
      "epoch: 4, train loss: 0.6053, train acc: 0.7628, test acc: 0.7469\n",
      "epoch: 5, train loss: 0.5366, train acc: 0.7934, test acc: 0.7969\n",
      "epoch: 6, train loss: 0.4911, train acc: 0.8137, test acc: 0.8145\n",
      "epoch: 7, train loss: 0.4517, train acc: 0.8304, test acc: 0.8220\n",
      "epoch: 8, train loss: 0.4288, train acc: 0.8398, test acc: 0.8190\n",
      "epoch: 9, train loss: 0.4099, train acc: 0.8470, test acc: 0.8364\n",
      "epoch: 10, train loss: 0.3936, train acc: 0.8521, test acc: 0.8209\n",
      "epoch: 11, train loss: 0.3803, train acc: 0.8571, test acc: 0.8474\n",
      "epoch: 12, train loss: 0.3692, train acc: 0.8619, test acc: 0.8516\n",
      "epoch: 13, train loss: 0.3581, train acc: 0.8656, test acc: 0.8534\n",
      "epoch: 14, train loss: 0.3486, train acc: 0.8683, test acc: 0.8515\n",
      "epoch: 15, train loss: 0.3393, train acc: 0.8727, test acc: 0.8554\n",
      "epoch: 16, train loss: 0.3333, train acc: 0.8751, test acc: 0.8639\n",
      "epoch: 17, train loss: 0.3252, train acc: 0.8788, test acc: 0.8510\n",
      "epoch: 18, train loss: 0.3185, train acc: 0.8804, test acc: 0.8700\n",
      "epoch: 19, train loss: 0.3116, train acc: 0.8836, test acc: 0.8603\n",
      "epoch: 20, train loss: 0.3082, train acc: 0.8843, test acc: 0.8637\n",
      "epoch: 21, train loss: 0.3037, train acc: 0.8866, test acc: 0.8372\n",
      "epoch: 22, train loss: 0.2985, train acc: 0.8863, test acc: 0.8794\n",
      "epoch: 23, train loss: 0.2913, train acc: 0.8911, test acc: 0.8770\n",
      "epoch: 24, train loss: 0.2889, train acc: 0.8911, test acc: 0.8764\n",
      "epoch: 25, train loss: 0.2850, train acc: 0.8924, test acc: 0.8776\n",
      "epoch: 26, train loss: 0.2793, train acc: 0.8951, test acc: 0.8700\n",
      "epoch: 27, train loss: 0.2759, train acc: 0.8971, test acc: 0.8833\n",
      "epoch: 28, train loss: 0.2723, train acc: 0.8972, test acc: 0.8801\n",
      "epoch: 29, train loss: 0.2675, train acc: 0.9010, test acc: 0.8834\n",
      "epoch: 30, train loss: 0.2655, train acc: 0.9008, test acc: 0.8859\n",
      "epoch: 31, train loss: 0.2606, train acc: 0.9023, test acc: 0.8864\n",
      "epoch: 32, train loss: 0.2570, train acc: 0.9030, test acc: 0.8831\n",
      "epoch: 33, train loss: 0.2557, train acc: 0.9033, test acc: 0.8834\n",
      "epoch: 34, train loss: 0.2519, train acc: 0.9050, test acc: 0.8862\n",
      "epoch: 35, train loss: 0.2501, train acc: 0.9057, test acc: 0.8879\n",
      "epoch: 36, train loss: 0.2464, train acc: 0.9084, test acc: 0.8901\n",
      "epoch: 37, train loss: 0.2425, train acc: 0.9087, test acc: 0.8961\n",
      "epoch: 38, train loss: 0.2405, train acc: 0.9091, test acc: 0.8884\n",
      "epoch: 39, train loss: 0.2379, train acc: 0.9106, test acc: 0.8815\n",
      "epoch: 40, train loss: 0.2343, train acc: 0.9127, test acc: 0.8915\n",
      "epoch: 41, train loss: 0.2324, train acc: 0.9122, test acc: 0.8891\n",
      "epoch: 42, train loss: 0.2307, train acc: 0.9133, test acc: 0.8951\n",
      "epoch: 43, train loss: 0.2281, train acc: 0.9144, test acc: 0.8960\n",
      "epoch: 44, train loss: 0.2241, train acc: 0.9165, test acc: 0.8884\n",
      "epoch: 45, train loss: 0.2233, train acc: 0.9164, test acc: 0.8910\n",
      "epoch: 46, train loss: 0.2190, train acc: 0.9184, test acc: 0.8904\n",
      "epoch: 47, train loss: 0.2185, train acc: 0.9190, test acc: 0.8746\n",
      "epoch: 48, train loss: 0.2152, train acc: 0.9188, test acc: 0.8972\n",
      "epoch: 49, train loss: 0.2133, train acc: 0.9195, test acc: 0.8943\n",
      "epoch: 50, train loss: 0.2095, train acc: 0.9222, test acc: 0.8918\n",
      "300 300 50\n",
      "write LeNet_epoch_train csv done!\n",
      "write LeNet_train_loss csv done!\n",
      "write LeNet_train_acc csv done!\n",
      "write LeNet_epoch_test csv done!\n",
      "write LeNet_test_acc csv done!\n"
     ]
    }
   ],
   "source": [
    "train_models(LeNet, train_loader, test_loader, epochs=50, lr=0.9, net_type='LeNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, train loss: 1.3502, train acc: 0.4944, test acc: 0.7036\n",
      "epoch: 2, train loss: 0.6580, train acc: 0.7535, test acc: 0.7621\n",
      "epoch: 3, train loss: 0.5401, train acc: 0.7997, test acc: 0.8017\n",
      "epoch: 4, train loss: 0.4760, train acc: 0.8236, test acc: 0.8251\n",
      "epoch: 5, train loss: 0.4291, train acc: 0.8429, test acc: 0.8423\n",
      "epoch: 6, train loss: 0.3985, train acc: 0.8557, test acc: 0.8423\n",
      "epoch: 7, train loss: 0.3767, train acc: 0.8633, test acc: 0.8580\n",
      "epoch: 8, train loss: 0.3581, train acc: 0.8690, test acc: 0.8651\n",
      "epoch: 9, train loss: 0.3423, train acc: 0.8754, test acc: 0.8680\n",
      "epoch: 10, train loss: 0.3295, train acc: 0.8797, test acc: 0.8722\n",
      "epoch: 11, train loss: 0.3177, train acc: 0.8851, test acc: 0.8743\n",
      "epoch: 12, train loss: 0.3066, train acc: 0.8881, test acc: 0.8799\n",
      "epoch: 13, train loss: 0.2971, train acc: 0.8907, test acc: 0.8790\n",
      "epoch: 14, train loss: 0.2900, train acc: 0.8942, test acc: 0.8854\n",
      "epoch: 15, train loss: 0.2808, train acc: 0.8969, test acc: 0.8802\n",
      "epoch: 16, train loss: 0.2723, train acc: 0.9007, test acc: 0.8844\n",
      "epoch: 17, train loss: 0.2672, train acc: 0.9009, test acc: 0.8866\n",
      "epoch: 18, train loss: 0.2605, train acc: 0.9039, test acc: 0.8943\n",
      "epoch: 19, train loss: 0.2549, train acc: 0.9055, test acc: 0.8929\n",
      "epoch: 20, train loss: 0.2478, train acc: 0.9089, test acc: 0.8941\n",
      "epoch: 21, train loss: 0.2436, train acc: 0.9107, test acc: 0.8980\n",
      "epoch: 22, train loss: 0.2371, train acc: 0.9116, test acc: 0.8876\n",
      "epoch: 23, train loss: 0.2315, train acc: 0.9146, test acc: 0.8932\n",
      "epoch: 24, train loss: 0.2269, train acc: 0.9161, test acc: 0.8985\n",
      "epoch: 25, train loss: 0.2189, train acc: 0.9180, test acc: 0.9013\n",
      "epoch: 26, train loss: 0.2168, train acc: 0.9204, test acc: 0.8939\n",
      "epoch: 27, train loss: 0.2109, train acc: 0.9215, test acc: 0.9005\n",
      "epoch: 28, train loss: 0.2045, train acc: 0.9248, test acc: 0.9049\n",
      "epoch: 29, train loss: 0.2023, train acc: 0.9249, test acc: 0.9017\n",
      "epoch: 30, train loss: 0.1966, train acc: 0.9259, test acc: 0.9036\n",
      "epoch: 31, train loss: 0.1919, train acc: 0.9291, test acc: 0.8935\n",
      "epoch: 32, train loss: 0.1868, train acc: 0.9303, test acc: 0.9089\n",
      "epoch: 33, train loss: 0.1826, train acc: 0.9330, test acc: 0.9073\n",
      "epoch: 34, train loss: 0.1785, train acc: 0.9335, test acc: 0.9079\n",
      "epoch: 35, train loss: 0.1763, train acc: 0.9328, test acc: 0.9074\n",
      "epoch: 36, train loss: 0.1705, train acc: 0.9363, test acc: 0.9008\n",
      "epoch: 37, train loss: 0.1667, train acc: 0.9367, test acc: 0.9062\n",
      "epoch: 38, train loss: 0.1636, train acc: 0.9390, test acc: 0.9067\n",
      "epoch: 39, train loss: 0.1580, train acc: 0.9415, test acc: 0.9107\n",
      "epoch: 40, train loss: 0.1547, train acc: 0.9413, test acc: 0.9099\n",
      "epoch: 41, train loss: 0.1516, train acc: 0.9442, test acc: 0.9069\n",
      "epoch: 42, train loss: 0.1463, train acc: 0.9459, test acc: 0.9098\n",
      "epoch: 43, train loss: 0.1415, train acc: 0.9471, test acc: 0.9084\n",
      "epoch: 44, train loss: 0.1391, train acc: 0.9468, test acc: 0.9075\n",
      "epoch: 45, train loss: 0.1345, train acc: 0.9483, test acc: 0.9117\n",
      "epoch: 46, train loss: 0.1305, train acc: 0.9511, test acc: 0.9107\n",
      "epoch: 47, train loss: 0.1309, train acc: 0.9508, test acc: 0.9073\n",
      "epoch: 48, train loss: 0.1224, train acc: 0.9539, test acc: 0.9072\n",
      "epoch: 49, train loss: 0.1189, train acc: 0.9554, test acc: 0.9078\n",
      "epoch: 50, train loss: 0.1161, train acc: 0.9559, test acc: 0.9036\n",
      "300 300 50\n",
      "write AlexNet_epoch_train csv done!\n",
      "write AlexNet_train_loss csv done!\n",
      "write AlexNet_train_acc csv done!\n",
      "write AlexNet_epoch_test csv done!\n",
      "write AlexNet_test_acc csv done!\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = load_data_fashion_mnist(batch_size=128, resize=224)\n",
    "train_models(AlexNet, train_loader, test_loader, epochs=50, lr=0.01, net_type='AlexNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, train loss: 1.7878, train acc: 0.3581, test acc: 0.5738\n",
      "epoch: 2, train loss: 0.9173, train acc: 0.6454, test acc: 0.6816\n",
      "epoch: 3, train loss: 0.7681, train acc: 0.7049, test acc: 0.7151\n",
      "epoch: 4, train loss: 0.6940, train acc: 0.7343, test acc: 0.7283\n",
      "epoch: 5, train loss: 0.6403, train acc: 0.7560, test acc: 0.7579\n",
      "epoch: 6, train loss: 0.6014, train acc: 0.7713, test acc: 0.7685\n",
      "epoch: 7, train loss: 0.5725, train acc: 0.7826, test acc: 0.7765\n",
      "epoch: 8, train loss: 0.5459, train acc: 0.7940, test acc: 0.7886\n",
      "epoch: 9, train loss: 0.5278, train acc: 0.8009, test acc: 0.7987\n",
      "epoch: 10, train loss: 0.5095, train acc: 0.8078, test acc: 0.7907\n",
      "epoch: 11, train loss: 0.4938, train acc: 0.8153, test acc: 0.8056\n",
      "epoch: 12, train loss: 0.4809, train acc: 0.8193, test acc: 0.8110\n",
      "epoch: 13, train loss: 0.4653, train acc: 0.8261, test acc: 0.8170\n",
      "epoch: 14, train loss: 0.4542, train acc: 0.8303, test acc: 0.8246\n",
      "epoch: 15, train loss: 0.4415, train acc: 0.8367, test acc: 0.8246\n",
      "epoch: 16, train loss: 0.4322, train acc: 0.8386, test acc: 0.8285\n",
      "epoch: 17, train loss: 0.4223, train acc: 0.8429, test acc: 0.8322\n",
      "epoch: 18, train loss: 0.4137, train acc: 0.8465, test acc: 0.8393\n",
      "epoch: 19, train loss: 0.4060, train acc: 0.8510, test acc: 0.8427\n",
      "epoch: 20, train loss: 0.3989, train acc: 0.8516, test acc: 0.8469\n",
      "epoch: 21, train loss: 0.3904, train acc: 0.8560, test acc: 0.8474\n",
      "epoch: 22, train loss: 0.3828, train acc: 0.8599, test acc: 0.8506\n",
      "epoch: 23, train loss: 0.3760, train acc: 0.8606, test acc: 0.8513\n",
      "epoch: 24, train loss: 0.3702, train acc: 0.8635, test acc: 0.8552\n",
      "epoch: 25, train loss: 0.3655, train acc: 0.8655, test acc: 0.8616\n",
      "epoch: 26, train loss: 0.3592, train acc: 0.8677, test acc: 0.8588\n",
      "epoch: 27, train loss: 0.3535, train acc: 0.8696, test acc: 0.8599\n",
      "epoch: 28, train loss: 0.3488, train acc: 0.8714, test acc: 0.8601\n",
      "epoch: 29, train loss: 0.3418, train acc: 0.8750, test acc: 0.8617\n",
      "epoch: 30, train loss: 0.3405, train acc: 0.8731, test acc: 0.8603\n",
      "epoch: 31, train loss: 0.3356, train acc: 0.8759, test acc: 0.8674\n",
      "epoch: 32, train loss: 0.3316, train acc: 0.8767, test acc: 0.8672\n",
      "epoch: 33, train loss: 0.3276, train acc: 0.8798, test acc: 0.8707\n",
      "epoch: 34, train loss: 0.3231, train acc: 0.8808, test acc: 0.8698\n",
      "epoch: 35, train loss: 0.3210, train acc: 0.8813, test acc: 0.8690\n",
      "epoch: 36, train loss: 0.3158, train acc: 0.8832, test acc: 0.8743\n",
      "epoch: 37, train loss: 0.3132, train acc: 0.8839, test acc: 0.8729\n",
      "epoch: 38, train loss: 0.3107, train acc: 0.8843, test acc: 0.8750\n",
      "epoch: 39, train loss: 0.3091, train acc: 0.8856, test acc: 0.8743\n",
      "epoch: 40, train loss: 0.3043, train acc: 0.8865, test acc: 0.8720\n",
      "epoch: 41, train loss: 0.2995, train acc: 0.8890, test acc: 0.8744\n",
      "epoch: 42, train loss: 0.2989, train acc: 0.8894, test acc: 0.8762\n",
      "epoch: 43, train loss: 0.2942, train acc: 0.8905, test acc: 0.8789\n",
      "epoch: 44, train loss: 0.2928, train acc: 0.8911, test acc: 0.8737\n",
      "epoch: 45, train loss: 0.2889, train acc: 0.8927, test acc: 0.8834\n",
      "epoch: 46, train loss: 0.2862, train acc: 0.8950, test acc: 0.8793\n",
      "epoch: 47, train loss: 0.2843, train acc: 0.8955, test acc: 0.8829\n",
      "epoch: 48, train loss: 0.2812, train acc: 0.8958, test acc: 0.8837\n",
      "epoch: 49, train loss: 0.2799, train acc: 0.8973, test acc: 0.8843\n",
      "epoch: 50, train loss: 0.2752, train acc: 0.8972, test acc: 0.8809\n",
      "300 300 50\n",
      "write AlexSimple_epoch_train csv done!\n",
      "write AlexSimple_train_loss csv done!\n",
      "write AlexSimple_train_acc csv done!\n",
      "write AlexSimple_epoch_test csv done!\n",
      "write AlexSimple_test_acc csv done!\n"
     ]
    }
   ],
   "source": [
    "# 简化设计\n",
    "train_loader, test_loader = load_data_fashion_mnist(batch_size=128)\n",
    "AlexSimple = models.AlexNetSimple()\n",
    "train_models(AlexSimple, train_loader, test_loader, epochs=50, lr=0.01, net_type='AlexSimple')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, train loss: 0.9032, train acc: 0.6575, test acc: 0.7757\n",
      "epoch: 2, train loss: 0.5027, train acc: 0.8099, test acc: 0.8304\n",
      "epoch: 3, train loss: 0.4153, train acc: 0.8458, test acc: 0.8343\n",
      "epoch: 4, train loss: 0.3654, train acc: 0.8644, test acc: 0.8669\n",
      "epoch: 5, train loss: 0.3381, train acc: 0.8734, test acc: 0.8729\n",
      "epoch: 6, train loss: 0.3147, train acc: 0.8835, test acc: 0.8777\n",
      "epoch: 7, train loss: 0.2944, train acc: 0.8912, test acc: 0.8803\n",
      "epoch: 8, train loss: 0.2849, train acc: 0.8941, test acc: 0.8840\n",
      "epoch: 9, train loss: 0.2699, train acc: 0.8986, test acc: 0.8864\n",
      "epoch: 10, train loss: 0.2590, train acc: 0.9033, test acc: 0.8928\n",
      "epoch: 11, train loss: 0.2468, train acc: 0.9076, test acc: 0.8882\n",
      "epoch: 12, train loss: 0.2379, train acc: 0.9101, test acc: 0.8881\n",
      "epoch: 13, train loss: 0.2279, train acc: 0.9147, test acc: 0.8914\n",
      "epoch: 14, train loss: 0.2186, train acc: 0.9169, test acc: 0.9020\n",
      "epoch: 15, train loss: 0.2078, train acc: 0.9217, test acc: 0.9036\n",
      "epoch: 16, train loss: 0.2009, train acc: 0.9244, test acc: 0.8934\n",
      "epoch: 17, train loss: 0.1903, train acc: 0.9272, test acc: 0.9007\n",
      "epoch: 18, train loss: 0.1822, train acc: 0.9314, test acc: 0.8983\n",
      "epoch: 19, train loss: 0.1729, train acc: 0.9362, test acc: 0.9050\n",
      "epoch: 20, train loss: 0.1633, train acc: 0.9382, test acc: 0.8989\n",
      "epoch: 21, train loss: 0.1535, train acc: 0.9421, test acc: 0.8978\n",
      "epoch: 22, train loss: 0.1468, train acc: 0.9448, test acc: 0.8951\n",
      "epoch: 23, train loss: 0.1380, train acc: 0.9476, test acc: 0.9067\n",
      "epoch: 24, train loss: 0.1283, train acc: 0.9511, test acc: 0.9039\n",
      "epoch: 25, train loss: 0.1225, train acc: 0.9526, test acc: 0.9014\n",
      "epoch: 26, train loss: 0.1119, train acc: 0.9575, test acc: 0.9096\n",
      "epoch: 27, train loss: 0.1052, train acc: 0.9598, test acc: 0.9047\n",
      "epoch: 28, train loss: 0.0977, train acc: 0.9628, test acc: 0.9041\n",
      "epoch: 29, train loss: 0.0902, train acc: 0.9655, test acc: 0.9022\n",
      "epoch: 30, train loss: 0.0839, train acc: 0.9680, test acc: 0.9011\n",
      "epoch: 31, train loss: 0.0772, train acc: 0.9703, test acc: 0.8997\n",
      "epoch: 32, train loss: 0.0720, train acc: 0.9728, test acc: 0.9043\n",
      "epoch: 33, train loss: 0.0665, train acc: 0.9750, test acc: 0.9051\n",
      "epoch: 34, train loss: 0.0606, train acc: 0.9768, test acc: 0.9025\n",
      "epoch: 35, train loss: 0.0565, train acc: 0.9791, test acc: 0.9049\n",
      "epoch: 36, train loss: 0.0502, train acc: 0.9810, test acc: 0.9053\n",
      "epoch: 37, train loss: 0.0487, train acc: 0.9825, test acc: 0.9053\n",
      "epoch: 38, train loss: 0.0457, train acc: 0.9830, test acc: 0.9023\n",
      "epoch: 39, train loss: 0.0410, train acc: 0.9848, test acc: 0.8997\n",
      "epoch: 40, train loss: 0.0381, train acc: 0.9858, test acc: 0.9074\n",
      "epoch: 41, train loss: 0.0364, train acc: 0.9869, test acc: 0.9038\n",
      "epoch: 42, train loss: 0.0327, train acc: 0.9884, test acc: 0.9031\n",
      "epoch: 43, train loss: 0.0313, train acc: 0.9884, test acc: 0.9014\n",
      "epoch: 44, train loss: 0.0291, train acc: 0.9901, test acc: 0.9027\n",
      "epoch: 45, train loss: 0.0235, train acc: 0.9918, test acc: 0.9063\n",
      "epoch: 46, train loss: 0.0253, train acc: 0.9912, test acc: 0.9042\n",
      "epoch: 47, train loss: 0.0210, train acc: 0.9923, test acc: 0.9058\n",
      "epoch: 48, train loss: 0.0204, train acc: 0.9932, test acc: 0.9033\n",
      "epoch: 49, train loss: 0.0173, train acc: 0.9943, test acc: 0.9027\n",
      "epoch: 50, train loss: 0.0191, train acc: 0.9929, test acc: 0.9037\n",
      "300 300 50\n",
      "write AlexSimple_2_epoch_train csv done!\n",
      "write AlexSimple_2_train_loss csv done!\n",
      "write AlexSimple_2_train_acc csv done!\n",
      "write AlexSimple_2_epoch_test csv done!\n",
      "write AlexSimple_2_test_acc csv done!\n"
     ]
    }
   ],
   "source": [
    "train_models(AlexSimple, train_loader, test_loader, epochs=50, lr=0.1, net_type='AlexSimple_2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, train loss: 1.2333, train acc: 0.5367, test acc: 0.6837\n",
      "epoch: 2, train loss: 0.7113, train acc: 0.7345, test acc: 0.7488\n",
      "epoch: 3, train loss: 0.6213, train acc: 0.7701, test acc: 0.7684\n",
      "epoch: 4, train loss: 0.5720, train acc: 0.7910, test acc: 0.7941\n",
      "epoch: 5, train loss: 0.5327, train acc: 0.8081, test acc: 0.7993\n",
      "epoch: 6, train loss: 0.5060, train acc: 0.8209, test acc: 0.8124\n",
      "epoch: 7, train loss: 0.4841, train acc: 0.8292, test acc: 0.8263\n",
      "epoch: 8, train loss: 0.4653, train acc: 0.8371, test acc: 0.8284\n",
      "epoch: 9, train loss: 0.4459, train acc: 0.8429, test acc: 0.8349\n",
      "epoch: 10, train loss: 0.4319, train acc: 0.8477, test acc: 0.8438\n",
      "epoch: 11, train loss: 0.4195, train acc: 0.8529, test acc: 0.8472\n",
      "epoch: 12, train loss: 0.4064, train acc: 0.8561, test acc: 0.8459\n",
      "epoch: 13, train loss: 0.4000, train acc: 0.8606, test acc: 0.8525\n",
      "epoch: 14, train loss: 0.3923, train acc: 0.8637, test acc: 0.8482\n",
      "epoch: 15, train loss: 0.3835, train acc: 0.8652, test acc: 0.8587\n",
      "epoch: 16, train loss: 0.3763, train acc: 0.8676, test acc: 0.8591\n",
      "epoch: 17, train loss: 0.3705, train acc: 0.8711, test acc: 0.8605\n",
      "epoch: 18, train loss: 0.3629, train acc: 0.8721, test acc: 0.8616\n",
      "epoch: 19, train loss: 0.3578, train acc: 0.8738, test acc: 0.8665\n",
      "epoch: 20, train loss: 0.3534, train acc: 0.8759, test acc: 0.8708\n",
      "epoch: 21, train loss: 0.3485, train acc: 0.8771, test acc: 0.8651\n",
      "epoch: 22, train loss: 0.3412, train acc: 0.8808, test acc: 0.8697\n",
      "epoch: 23, train loss: 0.3367, train acc: 0.8807, test acc: 0.8647\n",
      "epoch: 24, train loss: 0.3326, train acc: 0.8830, test acc: 0.8724\n",
      "epoch: 25, train loss: 0.3308, train acc: 0.8831, test acc: 0.8771\n",
      "epoch: 26, train loss: 0.3279, train acc: 0.8860, test acc: 0.8787\n",
      "epoch: 27, train loss: 0.3223, train acc: 0.8878, test acc: 0.8739\n",
      "epoch: 28, train loss: 0.3206, train acc: 0.8869, test acc: 0.8778\n",
      "epoch: 29, train loss: 0.3146, train acc: 0.8896, test acc: 0.8801\n",
      "epoch: 30, train loss: 0.3142, train acc: 0.8889, test acc: 0.8798\n",
      "epoch: 31, train loss: 0.3114, train acc: 0.8895, test acc: 0.8782\n",
      "epoch: 32, train loss: 0.3114, train acc: 0.8903, test acc: 0.8784\n",
      "epoch: 33, train loss: 0.3053, train acc: 0.8933, test acc: 0.8825\n",
      "epoch: 34, train loss: 0.3012, train acc: 0.8932, test acc: 0.8766\n",
      "epoch: 35, train loss: 0.3035, train acc: 0.8921, test acc: 0.8837\n",
      "epoch: 36, train loss: 0.2978, train acc: 0.8944, test acc: 0.8847\n",
      "epoch: 37, train loss: 0.2938, train acc: 0.8959, test acc: 0.8814\n",
      "epoch: 38, train loss: 0.2920, train acc: 0.8958, test acc: 0.8844\n",
      "epoch: 39, train loss: 0.2923, train acc: 0.8967, test acc: 0.8852\n",
      "epoch: 40, train loss: 0.2906, train acc: 0.8979, test acc: 0.8845\n",
      "epoch: 41, train loss: 0.2901, train acc: 0.8978, test acc: 0.8816\n",
      "epoch: 42, train loss: 0.2880, train acc: 0.8989, test acc: 0.8886\n",
      "epoch: 43, train loss: 0.2846, train acc: 0.8998, test acc: 0.8865\n",
      "epoch: 44, train loss: 0.2818, train acc: 0.9007, test acc: 0.8846\n",
      "epoch: 45, train loss: 0.2818, train acc: 0.8998, test acc: 0.8862\n",
      "epoch: 46, train loss: 0.2802, train acc: 0.9013, test acc: 0.8822\n",
      "epoch: 47, train loss: 0.2779, train acc: 0.9019, test acc: 0.8827\n",
      "epoch: 48, train loss: 0.2786, train acc: 0.9020, test acc: 0.8863\n",
      "epoch: 49, train loss: 0.2762, train acc: 0.9027, test acc: 0.8880\n",
      "epoch: 50, train loss: 0.2743, train acc: 0.9025, test acc: 0.8907\n",
      "300 300 50\n",
      "write LeNetPro_epoch_train csv done!\n",
      "write LeNetPro_train_loss csv done!\n",
      "write LeNetPro_train_acc csv done!\n",
      "write LeNetPro_epoch_test csv done!\n",
      "write LeNetPro_test_acc csv done!\n"
     ]
    }
   ],
   "source": [
    "LeNetPro = models.LeNetPro()\n",
    "train_loader, test_loader = load_data_fashion_mnist(batch_size=128)\n",
    "# keep lr = 0.1\n",
    "# use 0.9 被冻结\n",
    "train_models(LeNetPro, train_loader, test_loader, epochs=50, lr=0.1, net_type='LeNetPro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, train loss: 2.3074, train acc: 0.1014, test acc: 0.1000\n",
      "epoch: 2, train loss: 2.2988, train acc: 0.1141, test acc: 0.1470\n",
      "epoch: 3, train loss: 1.9039, train acc: 0.3284, test acc: 0.4779\n",
      "epoch: 4, train loss: 1.1888, train acc: 0.5668, test acc: 0.5971\n",
      "epoch: 5, train loss: 0.9870, train acc: 0.6279, test acc: 0.6315\n",
      "epoch: 6, train loss: 0.8815, train acc: 0.6714, test acc: 0.6727\n",
      "epoch: 7, train loss: 0.8192, train acc: 0.6967, test acc: 0.7047\n",
      "epoch: 8, train loss: 0.7696, train acc: 0.7145, test acc: 0.7163\n",
      "epoch: 9, train loss: 0.7240, train acc: 0.7287, test acc: 0.7116\n",
      "epoch: 10, train loss: 0.6889, train acc: 0.7355, test acc: 0.7132\n",
      "epoch: 11, train loss: 0.6653, train acc: 0.7419, test acc: 0.7401\n",
      "epoch: 12, train loss: 0.6466, train acc: 0.7477, test acc: 0.7421\n",
      "epoch: 13, train loss: 0.6305, train acc: 0.7551, test acc: 0.7578\n",
      "epoch: 14, train loss: 0.6175, train acc: 0.7607, test acc: 0.7600\n",
      "epoch: 15, train loss: 0.6051, train acc: 0.7665, test acc: 0.7670\n",
      "epoch: 16, train loss: 0.5943, train acc: 0.7710, test acc: 0.7726\n",
      "epoch: 17, train loss: 0.5815, train acc: 0.7771, test acc: 0.7771\n",
      "epoch: 18, train loss: 0.5713, train acc: 0.7822, test acc: 0.7762\n",
      "epoch: 19, train loss: 0.5612, train acc: 0.7862, test acc: 0.7859\n",
      "epoch: 20, train loss: 0.5493, train acc: 0.7922, test acc: 0.7874\n",
      "epoch: 21, train loss: 0.5387, train acc: 0.7961, test acc: 0.7932\n",
      "epoch: 22, train loss: 0.5312, train acc: 0.8006, test acc: 0.7792\n",
      "epoch: 23, train loss: 0.5215, train acc: 0.8033, test acc: 0.7999\n",
      "epoch: 24, train loss: 0.5134, train acc: 0.8066, test acc: 0.8036\n",
      "epoch: 25, train loss: 0.5060, train acc: 0.8105, test acc: 0.8024\n",
      "epoch: 26, train loss: 0.4974, train acc: 0.8134, test acc: 0.8041\n",
      "epoch: 27, train loss: 0.4920, train acc: 0.8177, test acc: 0.8149\n",
      "epoch: 28, train loss: 0.4856, train acc: 0.8185, test acc: 0.8079\n",
      "epoch: 29, train loss: 0.4798, train acc: 0.8215, test acc: 0.8189\n",
      "epoch: 30, train loss: 0.4725, train acc: 0.8272, test acc: 0.8167\n",
      "epoch: 31, train loss: 0.4669, train acc: 0.8273, test acc: 0.8238\n",
      "epoch: 32, train loss: 0.4621, train acc: 0.8302, test acc: 0.8245\n",
      "epoch: 33, train loss: 0.4573, train acc: 0.8322, test acc: 0.8247\n",
      "epoch: 34, train loss: 0.4527, train acc: 0.8339, test acc: 0.8269\n",
      "epoch: 35, train loss: 0.4473, train acc: 0.8364, test acc: 0.8073\n",
      "epoch: 36, train loss: 0.4432, train acc: 0.8374, test acc: 0.8143\n",
      "epoch: 37, train loss: 0.4395, train acc: 0.8375, test acc: 0.8180\n",
      "epoch: 38, train loss: 0.4351, train acc: 0.8414, test acc: 0.8245\n",
      "epoch: 39, train loss: 0.4306, train acc: 0.8420, test acc: 0.8338\n",
      "epoch: 40, train loss: 0.4273, train acc: 0.8440, test acc: 0.8349\n",
      "epoch: 41, train loss: 0.4238, train acc: 0.8455, test acc: 0.8341\n",
      "epoch: 42, train loss: 0.4195, train acc: 0.8472, test acc: 0.8276\n",
      "epoch: 43, train loss: 0.4172, train acc: 0.8483, test acc: 0.8343\n",
      "epoch: 44, train loss: 0.4137, train acc: 0.8484, test acc: 0.8336\n",
      "epoch: 45, train loss: 0.4101, train acc: 0.8501, test acc: 0.8365\n",
      "epoch: 46, train loss: 0.4078, train acc: 0.8505, test acc: 0.8378\n",
      "epoch: 47, train loss: 0.4038, train acc: 0.8528, test acc: 0.8429\n",
      "epoch: 48, train loss: 0.4005, train acc: 0.8535, test acc: 0.8472\n",
      "epoch: 49, train loss: 0.3976, train acc: 0.8551, test acc: 0.8484\n",
      "epoch: 50, train loss: 0.3947, train acc: 0.8550, test acc: 0.8474\n",
      "300 300 50\n",
      "write LeNet_0.1__epoch_train csv done!\n",
      "write LeNet_0.1__train_loss csv done!\n",
      "write LeNet_0.1__train_acc csv done!\n",
      "write LeNet_0.1__epoch_test csv done!\n",
      "write LeNet_0.1__test_acc csv done!\n"
     ]
    }
   ],
   "source": [
    "LeNet = models.LeNet()\n",
    "train_loader, test_loader = load_data_fashion_mnist(batch_size=128)\n",
    "# keep lr = 0.1\n",
    "train_models(LeNet, train_loader, test_loader, epochs=50, lr=0.1, net_type='LeNet_0.1_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, train loss: 0.9427, train acc: 0.6481, test acc: 0.7380\n",
      "epoch: 2, train loss: 0.5876, train acc: 0.7853, test acc: 0.7970\n",
      "epoch: 3, train loss: 0.5120, train acc: 0.8167, test acc: 0.8133\n",
      "epoch: 4, train loss: 0.4669, train acc: 0.8337, test acc: 0.8323\n",
      "epoch: 5, train loss: 0.4407, train acc: 0.8429, test acc: 0.8365\n",
      "epoch: 6, train loss: 0.4236, train acc: 0.8514, test acc: 0.8467\n",
      "epoch: 7, train loss: 0.4094, train acc: 0.8556, test acc: 0.8460\n",
      "epoch: 8, train loss: 0.3926, train acc: 0.8595, test acc: 0.8560\n",
      "epoch: 9, train loss: 0.3790, train acc: 0.8657, test acc: 0.8546\n",
      "epoch: 10, train loss: 0.3704, train acc: 0.8683, test acc: 0.8606\n",
      "epoch: 11, train loss: 0.3638, train acc: 0.8708, test acc: 0.8629\n",
      "epoch: 12, train loss: 0.3554, train acc: 0.8739, test acc: 0.8660\n",
      "epoch: 13, train loss: 0.3481, train acc: 0.8755, test acc: 0.8631\n",
      "epoch: 14, train loss: 0.3413, train acc: 0.8768, test acc: 0.8621\n",
      "epoch: 15, train loss: 0.3374, train acc: 0.8795, test acc: 0.8705\n",
      "epoch: 16, train loss: 0.3317, train acc: 0.8810, test acc: 0.8636\n",
      "epoch: 17, train loss: 0.3276, train acc: 0.8832, test acc: 0.8743\n",
      "epoch: 18, train loss: 0.3209, train acc: 0.8849, test acc: 0.8650\n",
      "epoch: 19, train loss: 0.3191, train acc: 0.8862, test acc: 0.8732\n",
      "epoch: 20, train loss: 0.3145, train acc: 0.8868, test acc: 0.8751\n",
      "epoch: 21, train loss: 0.3126, train acc: 0.8898, test acc: 0.8776\n",
      "epoch: 22, train loss: 0.3110, train acc: 0.8894, test acc: 0.8734\n",
      "epoch: 23, train loss: 0.3063, train acc: 0.8909, test acc: 0.8774\n",
      "epoch: 24, train loss: 0.2990, train acc: 0.8937, test acc: 0.8791\n",
      "epoch: 25, train loss: 0.2997, train acc: 0.8937, test acc: 0.8742\n",
      "epoch: 26, train loss: 0.2940, train acc: 0.8955, test acc: 0.8819\n",
      "epoch: 27, train loss: 0.2937, train acc: 0.8952, test acc: 0.8841\n",
      "epoch: 28, train loss: 0.2905, train acc: 0.8972, test acc: 0.8802\n",
      "epoch: 29, train loss: 0.2908, train acc: 0.8974, test acc: 0.8823\n",
      "epoch: 30, train loss: 0.2843, train acc: 0.8989, test acc: 0.8833\n",
      "epoch: 31, train loss: 0.2869, train acc: 0.8985, test acc: 0.8821\n",
      "epoch: 32, train loss: 0.2870, train acc: 0.8971, test acc: 0.8810\n",
      "epoch: 33, train loss: 0.2803, train acc: 0.8999, test acc: 0.8844\n",
      "epoch: 34, train loss: 0.2796, train acc: 0.8990, test acc: 0.8874\n",
      "epoch: 35, train loss: 0.2769, train acc: 0.9018, test acc: 0.8848\n",
      "epoch: 36, train loss: 0.2755, train acc: 0.9003, test acc: 0.8869\n",
      "epoch: 37, train loss: 0.2732, train acc: 0.9032, test acc: 0.8780\n",
      "epoch: 38, train loss: 0.2737, train acc: 0.9030, test acc: 0.8866\n",
      "epoch: 39, train loss: 0.2701, train acc: 0.9043, test acc: 0.8907\n",
      "epoch: 40, train loss: 0.2651, train acc: 0.9058, test acc: 0.8840\n",
      "epoch: 41, train loss: 0.2671, train acc: 0.9054, test acc: 0.8859\n",
      "epoch: 42, train loss: 0.2669, train acc: 0.9058, test acc: 0.8816\n",
      "epoch: 43, train loss: 0.2616, train acc: 0.9065, test acc: 0.8889\n",
      "epoch: 44, train loss: 0.2621, train acc: 0.9060, test acc: 0.8679\n",
      "epoch: 45, train loss: 0.2591, train acc: 0.9078, test acc: 0.8822\n",
      "epoch: 46, train loss: 0.2588, train acc: 0.9076, test acc: 0.8868\n",
      "epoch: 47, train loss: 0.2578, train acc: 0.9071, test acc: 0.8830\n",
      "epoch: 48, train loss: 0.2540, train acc: 0.9093, test acc: 0.8881\n",
      "epoch: 49, train loss: 0.2545, train acc: 0.9100, test acc: 0.8902\n",
      "epoch: 50, train loss: 0.2507, train acc: 0.9111, test acc: 0.8868\n",
      "300 300 50\n",
      "write LeNetPro_0.2_epoch_train csv done!\n",
      "write LeNetPro_0.2_train_loss csv done!\n",
      "write LeNetPro_0.2_train_acc csv done!\n",
      "write LeNetPro_0.2_epoch_test csv done!\n",
      "write LeNetPro_0.2_test_acc csv done!\n"
     ]
    }
   ],
   "source": [
    "LeNetPro = models.LeNetPro()\n",
    "train_loader, test_loader = load_data_fashion_mnist(batch_size=128)\n",
    "# keep lr = 0.1\n",
    "# use 0.9 被冻结\n",
    "train_models(LeNetPro, train_loader, test_loader, epochs=50, lr=0.3, net_type='LeNetPro_0.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, train loss: 2.3041, train acc: 0.1169, test acc: 0.1827\n",
      "epoch: 2, train loss: 1.2234, train acc: 0.5425, test acc: 0.6406\n",
      "epoch: 3, train loss: 0.8257, train acc: 0.6839, test acc: 0.6687\n",
      "epoch: 4, train loss: 0.7074, train acc: 0.7251, test acc: 0.7195\n",
      "epoch: 5, train loss: 0.6507, train acc: 0.7456, test acc: 0.7557\n",
      "epoch: 6, train loss: 0.6152, train acc: 0.7607, test acc: 0.7572\n",
      "epoch: 7, train loss: 0.5799, train acc: 0.7755, test acc: 0.7779\n",
      "epoch: 8, train loss: 0.5560, train acc: 0.7860, test acc: 0.7948\n",
      "epoch: 9, train loss: 0.5270, train acc: 0.7996, test acc: 0.7945\n",
      "epoch: 10, train loss: 0.5069, train acc: 0.8074, test acc: 0.7898\n",
      "epoch: 11, train loss: 0.4881, train acc: 0.8168, test acc: 0.8146\n",
      "epoch: 12, train loss: 0.4700, train acc: 0.8240, test acc: 0.8226\n",
      "epoch: 13, train loss: 0.4575, train acc: 0.8308, test acc: 0.8270\n",
      "epoch: 14, train loss: 0.4436, train acc: 0.8358, test acc: 0.8110\n",
      "epoch: 15, train loss: 0.4322, train acc: 0.8407, test acc: 0.8286\n",
      "epoch: 16, train loss: 0.4192, train acc: 0.8475, test acc: 0.8389\n",
      "epoch: 17, train loss: 0.4121, train acc: 0.8484, test acc: 0.8354\n",
      "epoch: 18, train loss: 0.4027, train acc: 0.8514, test acc: 0.8385\n",
      "epoch: 19, train loss: 0.3954, train acc: 0.8548, test acc: 0.8483\n",
      "epoch: 20, train loss: 0.3901, train acc: 0.8550, test acc: 0.8428\n",
      "epoch: 21, train loss: 0.3807, train acc: 0.8593, test acc: 0.8492\n",
      "epoch: 22, train loss: 0.3755, train acc: 0.8618, test acc: 0.8584\n",
      "epoch: 23, train loss: 0.3701, train acc: 0.8629, test acc: 0.8193\n",
      "epoch: 24, train loss: 0.3638, train acc: 0.8659, test acc: 0.8568\n",
      "epoch: 25, train loss: 0.3582, train acc: 0.8676, test acc: 0.8541\n",
      "epoch: 26, train loss: 0.3515, train acc: 0.8709, test acc: 0.8579\n",
      "epoch: 27, train loss: 0.3496, train acc: 0.8700, test acc: 0.8530\n",
      "epoch: 28, train loss: 0.3433, train acc: 0.8728, test acc: 0.8099\n",
      "epoch: 29, train loss: 0.3404, train acc: 0.8737, test acc: 0.8619\n",
      "epoch: 30, train loss: 0.3378, train acc: 0.8742, test acc: 0.8552\n",
      "epoch: 31, train loss: 0.3322, train acc: 0.8773, test acc: 0.8636\n",
      "epoch: 32, train loss: 0.3284, train acc: 0.8789, test acc: 0.8674\n",
      "epoch: 33, train loss: 0.3255, train acc: 0.8796, test acc: 0.8636\n",
      "epoch: 34, train loss: 0.3233, train acc: 0.8810, test acc: 0.8614\n",
      "epoch: 35, train loss: 0.3184, train acc: 0.8820, test acc: 0.8731\n",
      "epoch: 36, train loss: 0.3166, train acc: 0.8820, test acc: 0.8628\n",
      "epoch: 37, train loss: 0.3125, train acc: 0.8842, test acc: 0.8650\n",
      "epoch: 38, train loss: 0.3101, train acc: 0.8840, test acc: 0.8727\n",
      "epoch: 39, train loss: 0.3078, train acc: 0.8852, test acc: 0.8697\n",
      "epoch: 40, train loss: 0.3046, train acc: 0.8870, test acc: 0.8632\n",
      "epoch: 41, train loss: 0.3021, train acc: 0.8871, test acc: 0.8736\n",
      "epoch: 42, train loss: 0.2990, train acc: 0.8885, test acc: 0.8734\n",
      "epoch: 43, train loss: 0.2978, train acc: 0.8891, test acc: 0.8778\n",
      "epoch: 44, train loss: 0.2957, train acc: 0.8909, test acc: 0.8683\n",
      "epoch: 45, train loss: 0.2936, train acc: 0.8911, test acc: 0.8741\n",
      "epoch: 46, train loss: 0.2914, train acc: 0.8930, test acc: 0.8771\n",
      "epoch: 47, train loss: 0.2892, train acc: 0.8927, test acc: 0.8787\n",
      "epoch: 48, train loss: 0.2863, train acc: 0.8939, test acc: 0.8826\n",
      "epoch: 49, train loss: 0.2840, train acc: 0.8937, test acc: 0.8791\n",
      "epoch: 50, train loss: 0.2840, train acc: 0.8932, test acc: 0.8853\n",
      "300 300 50\n",
      "write LeNet_0.2__epoch_train csv done!\n",
      "write LeNet_0.2__train_loss csv done!\n",
      "write LeNet_0.2__train_acc csv done!\n",
      "write LeNet_0.2__epoch_test csv done!\n",
      "write LeNet_0.2__test_acc csv done!\n"
     ]
    }
   ],
   "source": [
    "LeNet = models.LeNet()\n",
    "train_loader, test_loader = load_data_fashion_mnist(batch_size=128)\n",
    "# keep lr = 0.1\n",
    "train_models(LeNet, train_loader, test_loader, epochs=50, lr=0.3, net_type='LeNet_0.2_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 28, 28])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "x.shape\n",
    "# print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwangxinming\u001b[0m (\u001b[33mgogowang\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "614a8eb298f245d0b7952d08516bbc65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Competition\\deep learning\\DLch7\\01AlexNet\\wandb\\run-20231219_190243-g4p2xafx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gogowang/CH7/runs/g4p2xafx' target=\"_blank\">AlexNet_size_64</a></strong> to <a href='https://wandb.ai/gogowang/CH7' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gogowang/CH7' target=\"_blank\">https://wandb.ai/gogowang/CH7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gogowang/CH7/runs/g4p2xafx' target=\"_blank\">https://wandb.ai/gogowang/CH7/runs/g4p2xafx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, train loss: 1.0748, train acc: 0.6024, test acc: 0.7634\n",
      "epoch: 2, train loss: 0.5389, train acc: 0.8007, test acc: 0.8100\n",
      "epoch: 3, train loss: 0.4422, train acc: 0.8389, test acc: 0.8210\n",
      "epoch: 4, train loss: 0.3879, train acc: 0.8582, test acc: 0.8594\n",
      "epoch: 5, train loss: 0.3571, train acc: 0.8696, test acc: 0.8532\n",
      "epoch: 6, train loss: 0.3337, train acc: 0.8771, test acc: 0.8695\n",
      "epoch: 7, train loss: 0.3136, train acc: 0.8859, test acc: 0.8764\n",
      "epoch: 8, train loss: 0.3006, train acc: 0.8892, test acc: 0.8784\n",
      "epoch: 9, train loss: 0.2858, train acc: 0.8946, test acc: 0.8788\n",
      "epoch: 10, train loss: 0.2740, train acc: 0.8993, test acc: 0.8879\n",
      "epoch: 11, train loss: 0.2637, train acc: 0.9041, test acc: 0.8927\n",
      "epoch: 12, train loss: 0.2528, train acc: 0.9077, test acc: 0.8864\n",
      "epoch: 13, train loss: 0.2432, train acc: 0.9105, test acc: 0.8816\n",
      "epoch: 14, train loss: 0.2358, train acc: 0.9125, test acc: 0.8974\n",
      "epoch: 15, train loss: 0.2275, train acc: 0.9159, test acc: 0.9008\n",
      "epoch: 16, train loss: 0.2181, train acc: 0.9196, test acc: 0.8978\n",
      "epoch: 17, train loss: 0.2128, train acc: 0.9206, test acc: 0.9000\n",
      "epoch: 18, train loss: 0.2041, train acc: 0.9240, test acc: 0.8999\n",
      "epoch: 19, train loss: 0.1974, train acc: 0.9268, test acc: 0.9002\n",
      "epoch: 20, train loss: 0.1907, train acc: 0.9293, test acc: 0.9069\n",
      "epoch: 21, train loss: 0.1857, train acc: 0.9300, test acc: 0.9042\n",
      "epoch: 22, train loss: 0.1777, train acc: 0.9334, test acc: 0.9019\n",
      "epoch: 23, train loss: 0.1710, train acc: 0.9358, test acc: 0.9096\n",
      "epoch: 24, train loss: 0.1649, train acc: 0.9388, test acc: 0.9118\n",
      "epoch: 25, train loss: 0.1587, train acc: 0.9404, test acc: 0.8946\n",
      "epoch: 26, train loss: 0.1537, train acc: 0.9420, test acc: 0.9029\n",
      "epoch: 27, train loss: 0.1471, train acc: 0.9448, test acc: 0.9095\n",
      "epoch: 28, train loss: 0.1432, train acc: 0.9468, test acc: 0.9098\n",
      "epoch: 29, train loss: 0.1373, train acc: 0.9495, test acc: 0.9098\n",
      "epoch: 30, train loss: 0.1300, train acc: 0.9523, test acc: 0.9121\n",
      "epoch: 31, train loss: 0.1239, train acc: 0.9535, test acc: 0.9150\n",
      "epoch: 32, train loss: 0.1194, train acc: 0.9549, test acc: 0.9106\n",
      "epoch: 33, train loss: 0.1158, train acc: 0.9563, test acc: 0.9150\n",
      "epoch: 34, train loss: 0.1082, train acc: 0.9591, test acc: 0.9047\n",
      "epoch: 35, train loss: 0.1018, train acc: 0.9613, test acc: 0.9157\n",
      "epoch: 36, train loss: 0.0998, train acc: 0.9627, test acc: 0.9126\n",
      "epoch: 37, train loss: 0.0953, train acc: 0.9647, test acc: 0.9107\n",
      "epoch: 38, train loss: 0.0905, train acc: 0.9648, test acc: 0.9125\n",
      "epoch: 39, train loss: 0.0843, train acc: 0.9682, test acc: 0.9166\n",
      "epoch: 40, train loss: 0.0814, train acc: 0.9697, test acc: 0.9142\n",
      "epoch: 41, train loss: 0.0799, train acc: 0.9702, test acc: 0.9089\n",
      "epoch: 42, train loss: 0.0740, train acc: 0.9728, test acc: 0.9164\n",
      "epoch: 43, train loss: 0.0707, train acc: 0.9736, test acc: 0.9169\n",
      "epoch: 44, train loss: 0.0636, train acc: 0.9761, test acc: 0.9073\n",
      "epoch: 45, train loss: 0.0644, train acc: 0.9756, test acc: 0.9166\n",
      "epoch: 46, train loss: 0.0594, train acc: 0.9776, test acc: 0.9047\n",
      "epoch: 47, train loss: 0.0581, train acc: 0.9785, test acc: 0.9153\n",
      "epoch: 48, train loss: 0.0531, train acc: 0.9801, test acc: 0.9174\n",
      "epoch: 49, train loss: 0.0518, train acc: 0.9803, test acc: 0.9105\n",
      "epoch: 50, train loss: 0.0506, train acc: 0.9804, test acc: 0.8999\n",
      "300 300 50\n",
      "net_type is None, don't record the data\n"
     ]
    }
   ],
   "source": [
    "# use wandb to monitor the GPU training process\n",
    "import wandb\n",
    "\n",
    "wandb.init(project=\"CH7\",\n",
    "           name=\"AlexNet_size_64\")\n",
    "\n",
    "config = wandb.config  # config的初始化\n",
    "config.batch_size = 64  \n",
    "config.epochs = 50  \n",
    "config.lr = 0.01   \n",
    "config.use_cuda = True\n",
    "\n",
    "train_loader, test_loader = load_data_fashion_mnist(batch_size=config.batch_size, resize=224)\n",
    "AlexNet = models.AlexNet()\n",
    "wandb.watch_called = False \n",
    "\n",
    "train_models(AlexNet, train_loader, test_loader, epochs=config.epochs, lr=config.lr, use_wandb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:g4p2xafx) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>▁▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████████████████</td></tr><tr><td>train_loss</td><td>█▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>0.98038</td></tr><tr><td>train_loss</td><td>0.05063</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">AlexNet_size_64</strong> at: <a href='https://wandb.ai/gogowang/CH7/runs/g4p2xafx' target=\"_blank\">https://wandb.ai/gogowang/CH7/runs/g4p2xafx</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231219_190243-g4p2xafx\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:g4p2xafx). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e17434f950c4464ae83d0357de473b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011288888888884685, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Competition\\deep learning\\DLch7\\01AlexNet\\wandb\\run-20231219_195750-p6wgukw3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gogowang/CH7/runs/p6wgukw3' target=\"_blank\">AlexNet_size_128</a></strong> to <a href='https://wandb.ai/gogowang/CH7' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gogowang/CH7' target=\"_blank\">https://wandb.ai/gogowang/CH7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gogowang/CH7/runs/p6wgukw3' target=\"_blank\">https://wandb.ai/gogowang/CH7/runs/p6wgukw3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, train loss: 1.0093, train acc: 0.6275, test acc: 0.7633\n",
      "epoch: 2, train loss: 0.5321, train acc: 0.8000, test acc: 0.8117\n",
      "epoch: 3, train loss: 0.4397, train acc: 0.8378, test acc: 0.8442\n",
      "epoch: 4, train loss: 0.3876, train acc: 0.8571, test acc: 0.8522\n",
      "epoch: 5, train loss: 0.3549, train acc: 0.8695, test acc: 0.8663\n",
      "epoch: 6, train loss: 0.3316, train acc: 0.8779, test acc: 0.8588\n",
      "epoch: 7, train loss: 0.3144, train acc: 0.8838, test acc: 0.8789\n",
      "epoch: 8, train loss: 0.2988, train acc: 0.8894, test acc: 0.8744\n",
      "epoch: 9, train loss: 0.2859, train acc: 0.8938, test acc: 0.8829\n",
      "epoch: 10, train loss: 0.2758, train acc: 0.8976, test acc: 0.8849\n",
      "epoch: 11, train loss: 0.2638, train acc: 0.9020, test acc: 0.8850\n",
      "epoch: 12, train loss: 0.2548, train acc: 0.9057, test acc: 0.8953\n",
      "epoch: 13, train loss: 0.2450, train acc: 0.9093, test acc: 0.8888\n",
      "epoch: 14, train loss: 0.2383, train acc: 0.9115, test acc: 0.9006\n",
      "epoch: 15, train loss: 0.2294, train acc: 0.9150, test acc: 0.8950\n",
      "epoch: 16, train loss: 0.2235, train acc: 0.9160, test acc: 0.8971\n",
      "epoch: 17, train loss: 0.2152, train acc: 0.9190, test acc: 0.8898\n",
      "epoch: 18, train loss: 0.2078, train acc: 0.9220, test acc: 0.9009\n",
      "epoch: 19, train loss: 0.2028, train acc: 0.9242, test acc: 0.9006\n",
      "epoch: 20, train loss: 0.1960, train acc: 0.9272, test acc: 0.9050\n",
      "epoch: 21, train loss: 0.1907, train acc: 0.9276, test acc: 0.9027\n",
      "epoch: 22, train loss: 0.1833, train acc: 0.9321, test acc: 0.8945\n",
      "epoch: 23, train loss: 0.1761, train acc: 0.9348, test acc: 0.9051\n",
      "epoch: 24, train loss: 0.1712, train acc: 0.9359, test acc: 0.9023\n",
      "epoch: 25, train loss: 0.1665, train acc: 0.9378, test acc: 0.9085\n",
      "epoch: 26, train loss: 0.1599, train acc: 0.9397, test acc: 0.9078\n",
      "epoch: 27, train loss: 0.1545, train acc: 0.9409, test acc: 0.9104\n",
      "epoch: 28, train loss: 0.1476, train acc: 0.9443, test acc: 0.9120\n",
      "epoch: 29, train loss: 0.1424, train acc: 0.9457, test acc: 0.9149\n",
      "epoch: 30, train loss: 0.1383, train acc: 0.9486, test acc: 0.9156\n",
      "epoch: 31, train loss: 0.1327, train acc: 0.9504, test acc: 0.9128\n",
      "epoch: 32, train loss: 0.1272, train acc: 0.9522, test acc: 0.9134\n",
      "epoch: 33, train loss: 0.1216, train acc: 0.9549, test acc: 0.9086\n",
      "epoch: 34, train loss: 0.1176, train acc: 0.9555, test acc: 0.9125\n",
      "epoch: 35, train loss: 0.1120, train acc: 0.9579, test acc: 0.9143\n",
      "epoch: 36, train loss: 0.1053, train acc: 0.9602, test acc: 0.9114\n",
      "epoch: 37, train loss: 0.1019, train acc: 0.9608, test acc: 0.9124\n",
      "epoch: 38, train loss: 0.1007, train acc: 0.9608, test acc: 0.9184\n",
      "epoch: 39, train loss: 0.0920, train acc: 0.9655, test acc: 0.9179\n",
      "epoch: 40, train loss: 0.0890, train acc: 0.9664, test acc: 0.9181\n",
      "epoch: 41, train loss: 0.0845, train acc: 0.9680, test acc: 0.9134\n",
      "epoch: 42, train loss: 0.0796, train acc: 0.9700, test acc: 0.9191\n",
      "epoch: 43, train loss: 0.0766, train acc: 0.9713, test acc: 0.9153\n",
      "epoch: 44, train loss: 0.0729, train acc: 0.9725, test acc: 0.9096\n",
      "epoch: 45, train loss: 0.0726, train acc: 0.9728, test acc: 0.9182\n",
      "epoch: 46, train loss: 0.0655, train acc: 0.9751, test acc: 0.9196\n",
      "epoch: 47, train loss: 0.0632, train acc: 0.9767, test acc: 0.9124\n",
      "epoch: 48, train loss: 0.0595, train acc: 0.9776, test acc: 0.9166\n",
      "epoch: 49, train loss: 0.0552, train acc: 0.9792, test acc: 0.9174\n",
      "epoch: 50, train loss: 0.0545, train acc: 0.9797, test acc: 0.9158\n",
      "300 300 50\n",
      "net_type is None, don't record the data\n"
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"CH7\",\n",
    "           name=\"AlexNet_size_128\")\n",
    "\n",
    "config = wandb.config  # config的初始化\n",
    "config.batch_size = 128  \n",
    "config.epochs = 50  \n",
    "config.lr = 0.01   \n",
    "config.use_cuda = True\n",
    "\n",
    "# train_loader, test_loader = load_data_fashion_mnist(batch_size=config.batch_size, resize=224)\n",
    "# AlexNet = models.AlexNet()\n",
    "wandb.watch_called = False \n",
    "\n",
    "train_models(AlexNet, train_loader, test_loader, epochs=config.epochs, lr=config.lr, use_wandb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:p6wgukw3) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c961b75efd204ccea80b6c0bd184322e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.019 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.057543…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>▁▄▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>train_loss</td><td>█▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>0.9797</td></tr><tr><td>train_loss</td><td>0.05455</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">AlexNet_size_128</strong> at: <a href='https://wandb.ai/gogowang/CH7/runs/p6wgukw3' target=\"_blank\">https://wandb.ai/gogowang/CH7/runs/p6wgukw3</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231219_195750-p6wgukw3\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:p6wgukw3). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d56eaf8b5a4b70b72fa3a4c5fba532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Competition\\deep learning\\DLch7\\01AlexNet\\wandb\\run-20231219_205415-96kiqjse</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gogowang/CH7/runs/96kiqjse' target=\"_blank\">AlexNet_size_256</a></strong> to <a href='https://wandb.ai/gogowang/CH7' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gogowang/CH7' target=\"_blank\">https://wandb.ai/gogowang/CH7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gogowang/CH7/runs/96kiqjse' target=\"_blank\">https://wandb.ai/gogowang/CH7/runs/96kiqjse</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, train loss: 0.9569, train acc: 0.6430, test acc: 0.7544\n",
      "epoch: 2, train loss: 0.5255, train acc: 0.8019, test acc: 0.8102\n",
      "epoch: 3, train loss: 0.4324, train acc: 0.8411, test acc: 0.8345\n",
      "epoch: 4, train loss: 0.3814, train acc: 0.8596, test acc: 0.8591\n",
      "epoch: 5, train loss: 0.3502, train acc: 0.8716, test acc: 0.8612\n",
      "epoch: 6, train loss: 0.3287, train acc: 0.8789, test acc: 0.8658\n",
      "epoch: 7, train loss: 0.3111, train acc: 0.8851, test acc: 0.8735\n",
      "epoch: 8, train loss: 0.2968, train acc: 0.8901, test acc: 0.8802\n",
      "epoch: 9, train loss: 0.2845, train acc: 0.8953, test acc: 0.8759\n",
      "epoch: 10, train loss: 0.2730, train acc: 0.8980, test acc: 0.8790\n",
      "epoch: 11, train loss: 0.2625, train acc: 0.9018, test acc: 0.8896\n",
      "epoch: 12, train loss: 0.2537, train acc: 0.9051, test acc: 0.8933\n",
      "epoch: 13, train loss: 0.2441, train acc: 0.9095, test acc: 0.8943\n",
      "epoch: 14, train loss: 0.2371, train acc: 0.9129, test acc: 0.8916\n",
      "epoch: 15, train loss: 0.2293, train acc: 0.9141, test acc: 0.8993\n",
      "epoch: 16, train loss: 0.2210, train acc: 0.9175, test acc: 0.8996\n",
      "epoch: 17, train loss: 0.2140, train acc: 0.9196, test acc: 0.8922\n",
      "epoch: 18, train loss: 0.2062, train acc: 0.9226, test acc: 0.9040\n",
      "epoch: 19, train loss: 0.2007, train acc: 0.9247, test acc: 0.9041\n",
      "epoch: 20, train loss: 0.1936, train acc: 0.9270, test acc: 0.8953\n",
      "epoch: 21, train loss: 0.1892, train acc: 0.9293, test acc: 0.9014\n",
      "epoch: 22, train loss: 0.1836, train acc: 0.9312, test acc: 0.9044\n",
      "epoch: 23, train loss: 0.1764, train acc: 0.9339, test acc: 0.9038\n",
      "epoch: 24, train loss: 0.1718, train acc: 0.9349, test acc: 0.9093\n",
      "epoch: 25, train loss: 0.1630, train acc: 0.9376, test acc: 0.9081\n",
      "epoch: 26, train loss: 0.1588, train acc: 0.9400, test acc: 0.9043\n",
      "epoch: 27, train loss: 0.1524, train acc: 0.9436, test acc: 0.9071\n",
      "epoch: 28, train loss: 0.1470, train acc: 0.9442, test acc: 0.8983\n",
      "epoch: 29, train loss: 0.1419, train acc: 0.9453, test acc: 0.9034\n",
      "epoch: 30, train loss: 0.1353, train acc: 0.9492, test acc: 0.9095\n",
      "epoch: 31, train loss: 0.1314, train acc: 0.9499, test acc: 0.9094\n",
      "epoch: 32, train loss: 0.1254, train acc: 0.9524, test acc: 0.9124\n",
      "epoch: 33, train loss: 0.1219, train acc: 0.9536, test acc: 0.9118\n",
      "epoch: 34, train loss: 0.1163, train acc: 0.9561, test acc: 0.9066\n",
      "epoch: 35, train loss: 0.1100, train acc: 0.9584, test acc: 0.9153\n",
      "epoch: 36, train loss: 0.1062, train acc: 0.9606, test acc: 0.9117\n",
      "epoch: 37, train loss: 0.1019, train acc: 0.9613, test acc: 0.9109\n",
      "epoch: 38, train loss: 0.0973, train acc: 0.9634, test acc: 0.9087\n",
      "epoch: 39, train loss: 0.0942, train acc: 0.9643, test acc: 0.9119\n",
      "epoch: 40, train loss: 0.0875, train acc: 0.9671, test acc: 0.9101\n",
      "epoch: 41, train loss: 0.0847, train acc: 0.9676, test acc: 0.9119\n",
      "epoch: 42, train loss: 0.0812, train acc: 0.9688, test acc: 0.9093\n",
      "epoch: 43, train loss: 0.0752, train acc: 0.9717, test acc: 0.9108\n",
      "epoch: 44, train loss: 0.0728, train acc: 0.9729, test acc: 0.9090\n",
      "epoch: 45, train loss: 0.0691, train acc: 0.9739, test acc: 0.9134\n",
      "epoch: 46, train loss: 0.0656, train acc: 0.9752, test acc: 0.9097\n",
      "epoch: 47, train loss: 0.0628, train acc: 0.9762, test acc: 0.9160\n",
      "epoch: 48, train loss: 0.0580, train acc: 0.9782, test acc: 0.9165\n",
      "epoch: 49, train loss: 0.0582, train acc: 0.9777, test acc: 0.9123\n",
      "epoch: 50, train loss: 0.0537, train acc: 0.9798, test acc: 0.9128\n",
      "300 300 50\n",
      "net_type is None, don't record the data\n"
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"CH7\",\n",
    "           name=\"AlexNet_size_256\")\n",
    "\n",
    "config = wandb.config  # config的初始化\n",
    "config.batch_size = 256  \n",
    "config.epochs = 50  \n",
    "config.lr = 0.01   \n",
    "config.use_cuda = True\n",
    "\n",
    "# train_loader, test_loader = load_data_fashion_mnist(batch_size=config.batch_size, resize=224)\n",
    "# AlexNet = models.AlexNet()\n",
    "wandb.watch_called = False \n",
    "\n",
    "train_models(AlexNet, train_loader, test_loader, epochs=config.epochs, lr=config.lr, use_wandb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 使用块的网络（VGG）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from utlis import *\n",
    "import models\n",
    "import wandb\n",
    "\n",
    "def train_models(net, \n",
    "                 train_loader, \n",
    "                 test_loader, \n",
    "                 epochs, \n",
    "                 lr, \n",
    "                 net_type : str =  None,\n",
    "                 use_wandb : bool = False,\n",
    "                 device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')):\n",
    "    # 设置优化器\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "    # 设置损失函数\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "    net.apply(init_weights)\n",
    "    net = net.to(device)\n",
    "    \n",
    "    epoch_list = []\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    \n",
    "    if use_wandb:\n",
    "        wandb.watch(net, log=\"all\")\n",
    "\n",
    "    print('training on', device)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "    # 训练\n",
    "        loss_sum = 0\n",
    "        acc = 0\n",
    "        net.train()\n",
    "        for idx, (x, y) in enumerate(train_loader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predict = net(x)\n",
    "            l = loss(predict, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_sum += l.item() * x.size(0)\n",
    "            acc += predict.max(dim=1)[1].eq(y).sum().item()\n",
    "            if (idx + 1) % (len(train_loader) // 5) == 0 or idx == len(train_loader) - 1:\n",
    "                if (idx + 1) != len(train_loader):\n",
    "                    loss_now = loss_sum / (idx + 1) / x.size(0)\n",
    "                    acc_now = acc / (idx + 1) / x.size(0)\n",
    "                else:\n",
    "                    loss_now = loss_sum / len(train_loader.dataset)\n",
    "                    acc_now = acc / len(train_loader.dataset)\n",
    "                \n",
    "                if use_wandb:\n",
    "                    wandb.log({\"train_loss\": loss_now, \"train_acc\": acc_now})\n",
    "                train_acc.append(acc_now)\n",
    "                train_loss.append(loss_now)\n",
    "                epoch_list.append(epoch + (idx + 1) / len(train_loader))         \n",
    "        # loss_sum /= len(train_loader.dataset)\n",
    "        # acc /= len(train_loader.dataset)\n",
    "        # train_loss.append(loss_sum)\n",
    "        # train_acc.append(acc)\n",
    "            \n",
    "        # 测试\n",
    "        loss_sum = 0\n",
    "        acc = 0\n",
    "        for idx, (x, y) in enumerate(test_loader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            predict = net(x)\n",
    "            l = loss(predict, y)\n",
    "            \n",
    "            acc += predict.max(dim=1)[1].eq(y).sum().item()\n",
    "            \n",
    "        acc /= len(test_loader.dataset)\n",
    "        test_acc.append(acc)\n",
    "        # print(epoch_list)\n",
    "        \n",
    "        print('epoch: {}, train loss: {:.4f}, train acc: {:.4f}, test acc: {:.4f}'.format(epoch + 1, train_loss[-1], train_acc[-1], test_acc[-1]))\n",
    "    # print(len(train_loss), len(train_acc), len(test_acc))\n",
    "    \n",
    "    epoch_test = list(range(1, int(epoch_list[-1]) + 1, 1))\n",
    "    \n",
    "    if net_type is not None: \n",
    "    \n",
    "        write2csv('ch7_02.csv', epoch_list, net_type + '_epoch_train')\n",
    "        write2csv('ch7_02.csv', train_loss, net_type + '_train_loss')\n",
    "        write2csv('ch7_02.csv', train_acc, net_type + '_train_acc')\n",
    "        write2csv('ch7_02_eval.csv', epoch_test, net_type + '_epoch_test')\n",
    "        write2csv('ch7_02_eval.csv', test_acc, net_type + '_test_acc')\n",
    "        print('record the data')\n",
    "    else:\n",
    "        print('net_type is None, don\\'t record the data')\n",
    "    \n",
    "    # draw_loss_acc(train_loss, train_acc, test_acc, epoch_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda:0\n",
      "epoch: 1, train loss: 0.7362, train acc: 0.7266, test acc: 0.8459\n",
      "epoch: 2, train loss: 0.3385, train acc: 0.8759, test acc: 0.8769\n",
      "epoch: 3, train loss: 0.2792, train acc: 0.8973, test acc: 0.8855\n",
      "epoch: 4, train loss: 0.2437, train acc: 0.9105, test acc: 0.9026\n",
      "epoch: 5, train loss: 0.2172, train acc: 0.9193, test acc: 0.9095\n",
      "epoch: 6, train loss: 0.1975, train acc: 0.9273, test acc: 0.9114\n",
      "epoch: 7, train loss: 0.1796, train acc: 0.9334, test acc: 0.9087\n",
      "epoch: 8, train loss: 0.1611, train acc: 0.9413, test acc: 0.9153\n",
      "epoch: 9, train loss: 0.1444, train acc: 0.9463, test acc: 0.9156\n",
      "epoch: 10, train loss: 0.1291, train acc: 0.9519, test acc: 0.9095\n",
      "epoch: 11, train loss: 0.1171, train acc: 0.9565, test acc: 0.9177\n",
      "epoch: 12, train loss: 0.1025, train acc: 0.9625, test acc: 0.9191\n",
      "epoch: 13, train loss: 0.0873, train acc: 0.9679, test acc: 0.9195\n",
      "epoch: 14, train loss: 0.0766, train acc: 0.9723, test acc: 0.9223\n",
      "epoch: 15, train loss: 0.0671, train acc: 0.9756, test acc: 0.9149\n",
      "epoch: 16, train loss: 0.0593, train acc: 0.9778, test acc: 0.9165\n",
      "epoch: 17, train loss: 0.0542, train acc: 0.9796, test acc: 0.9141\n",
      "epoch: 18, train loss: 0.0445, train acc: 0.9839, test acc: 0.9240\n",
      "epoch: 19, train loss: 0.0421, train acc: 0.9849, test acc: 0.9239\n",
      "epoch: 20, train loss: 0.0348, train acc: 0.9871, test acc: 0.9180\n",
      "record the data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Vgg11 = models.vgg11(coef=4)\n",
    "train_loader, test_loader = load_data_fashion_mnist(batch_size=64, resize=224)\n",
    "\n",
    "train_models(Vgg11, train_loader, test_loader, epochs=20, lr=0.05, net_type='Vgg11_224')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda:0\n",
      "epoch: 1, train loss: 1.1540, train acc: 0.5661, test acc: 0.8133\n",
      "epoch: 2, train loss: 0.4084, train acc: 0.8477, test acc: 0.8560\n",
      "epoch: 3, train loss: 0.3296, train acc: 0.8778, test acc: 0.8737\n",
      "epoch: 4, train loss: 0.2906, train acc: 0.8930, test acc: 0.8790\n",
      "epoch: 5, train loss: 0.2630, train acc: 0.9028, test acc: 0.8943\n",
      "epoch: 6, train loss: 0.2408, train acc: 0.9103, test acc: 0.8961\n",
      "epoch: 7, train loss: 0.2202, train acc: 0.9195, test acc: 0.8925\n",
      "epoch: 8, train loss: 0.2042, train acc: 0.9244, test acc: 0.9055\n",
      "epoch: 9, train loss: 0.1890, train acc: 0.9304, test acc: 0.8958\n",
      "epoch: 10, train loss: 0.1747, train acc: 0.9344, test acc: 0.9142\n",
      "epoch: 11, train loss: 0.1606, train acc: 0.9409, test acc: 0.9138\n",
      "epoch: 12, train loss: 0.1492, train acc: 0.9443, test acc: 0.9045\n",
      "epoch: 13, train loss: 0.1353, train acc: 0.9495, test acc: 0.9146\n",
      "epoch: 14, train loss: 0.1253, train acc: 0.9527, test acc: 0.8762\n",
      "epoch: 15, train loss: 0.1140, train acc: 0.9579, test acc: 0.9107\n",
      "epoch: 16, train loss: 0.1027, train acc: 0.9618, test acc: 0.9119\n",
      "epoch: 17, train loss: 0.0958, train acc: 0.9641, test acc: 0.9096\n",
      "epoch: 18, train loss: 0.0875, train acc: 0.9668, test acc: 0.9124\n",
      "epoch: 19, train loss: 0.0786, train acc: 0.9698, test acc: 0.9129\n",
      "epoch: 20, train loss: 0.0729, train acc: 0.9726, test acc: 0.9187\n",
      "record the data\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = load_data_fashion_mnist(batch_size=64, resize=96)\n",
    "Vgg11 = models.vgg11(coef=4, input_size=96)\n",
    "train_models(Vgg11, train_loader, test_loader, epochs=20, lr=0.05, net_type='Vgg11_96')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 网络中的网络(NiN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from utlis import *\n",
    "import models\n",
    "import wandb\n",
    "\n",
    "def train_models(net, \n",
    "                 train_loader, \n",
    "                 test_loader, \n",
    "                 epochs, \n",
    "                 lr, \n",
    "                 net_type : str =  None,\n",
    "                 use_wandb : bool = False,\n",
    "                 device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')):\n",
    "    # 设置优化器\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "    # 设置损失函数\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "    net.apply(init_weights)\n",
    "    net = net.to(device)\n",
    "    \n",
    "    epoch_list = []\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    \n",
    "    if use_wandb:\n",
    "        wandb.watch(net, log=\"all\")\n",
    "\n",
    "    print('training on', device)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "    # 训练\n",
    "        loss_sum = 0\n",
    "        acc = 0\n",
    "        net.train()\n",
    "        for idx, (x, y) in enumerate(train_loader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predict = net(x)\n",
    "            l = loss(predict, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_sum += l.item() * x.size(0)\n",
    "            acc += predict.max(dim=1)[1].eq(y).sum().item()\n",
    "            if (idx + 1) % (len(train_loader) // 5) == 0 or idx == len(train_loader) - 1:\n",
    "                if (idx + 1) != len(train_loader):\n",
    "                    loss_now = loss_sum / (idx + 1) / x.size(0)\n",
    "                    acc_now = acc / (idx + 1) / x.size(0)\n",
    "                else:\n",
    "                    loss_now = loss_sum / len(train_loader.dataset)\n",
    "                    acc_now = acc / len(train_loader.dataset)\n",
    "                \n",
    "                if use_wandb:\n",
    "                    wandb.log({\"train_loss\": loss_now, \"train_acc\": acc_now})\n",
    "                train_acc.append(acc_now)\n",
    "                train_loss.append(loss_now)\n",
    "                epoch_list.append(epoch + (idx + 1) / len(train_loader))         \n",
    "        # loss_sum /= len(train_loader.dataset)\n",
    "        # acc /= len(train_loader.dataset)\n",
    "        # train_loss.append(loss_sum)\n",
    "        # train_acc.append(acc)\n",
    "            \n",
    "        # 测试\n",
    "        loss_sum = 0\n",
    "        acc = 0\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            for idx, (x, y) in enumerate(test_loader):\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                predict = net(x)\n",
    "                l = loss(predict, y)\n",
    "                \n",
    "                acc += predict.max(dim=1)[1].eq(y).sum().item()\n",
    "                \n",
    "            acc /= len(test_loader.dataset)\n",
    "            test_acc.append(acc)\n",
    "        if use_wandb:\n",
    "            wandb.log({\"test_loss\": acc})\n",
    "        # print(epoch_list)\n",
    "        if epoch / epochs > 0.5:\n",
    "            print('epoch: {}, train loss: {:.4f}, train acc: {:.4f}, test acc: {:.4f}'.format(epoch + 1, train_loss[-1], train_acc[-1], test_acc[-1]))\n",
    "    # print(len(train_loss), len(train_acc), len(test_acc))\n",
    "    \n",
    "    epoch_test = list(range(1, int(epoch_list[-1]) + 1, 1))\n",
    "    \n",
    "    if net_type is not None: \n",
    "    \n",
    "        write2csv('ch7_03.csv', epoch_list, net_type + '_epoch_train')\n",
    "        write2csv('ch7_03.csv', train_loss, net_type + '_train_loss')\n",
    "        write2csv('ch7_03.csv', train_acc, net_type + '_train_acc')\n",
    "        write2csv('ch7_03_eval.csv', epoch_test, net_type + '_epoch_test')\n",
    "        write2csv('ch7_03_eval.csv', test_acc, net_type + '_test_acc')\n",
    "        print('record the data')\n",
    "    else:\n",
    "        print('net_type is None, don\\'t record the data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_0 = [0.1, 30, 128]\n",
    "hyper_1 = [0.08, 30, 128]\n",
    "hyper_2 = [0.2, 30, 128]\n",
    "hyper_3 = [0.1, 30, 64]\n",
    "hyper_4 = [0.2, 30, 64]\n",
    "hyper_5 = [0.1, 30, 512]\n",
    "hyper_params = [hyper_0, hyper_1, hyper_2, hyper_3, hyper_4, hyper_5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.1, epochs: 30, batch_size: 128\n",
      "training on cuda:0\n",
      "epoch: 17, train loss: 0.4615, train acc: 0.8193, test acc: 0.8107\n",
      "epoch: 18, train loss: 0.4544, train acc: 0.8215, test acc: 0.8074\n",
      "epoch: 19, train loss: 0.4432, train acc: 0.8248, test acc: 0.8057\n",
      "epoch: 20, train loss: 0.4375, train acc: 0.8282, test acc: 0.8115\n",
      "epoch: 21, train loss: 0.4266, train acc: 0.8319, test acc: 0.8153\n",
      "epoch: 22, train loss: 0.4248, train acc: 0.8322, test acc: 0.8100\n",
      "epoch: 23, train loss: 0.4140, train acc: 0.8357, test acc: 0.8145\n",
      "epoch: 24, train loss: 0.4096, train acc: 0.8365, test acc: 0.8180\n",
      "epoch: 25, train loss: 0.4022, train acc: 0.8395, test acc: 0.7872\n",
      "epoch: 26, train loss: 0.3946, train acc: 0.8420, test acc: 0.8091\n",
      "epoch: 27, train loss: 0.3896, train acc: 0.8446, test acc: 0.8167\n",
      "epoch: 28, train loss: 0.3857, train acc: 0.8444, test acc: 0.8190\n",
      "epoch: 29, train loss: 0.3771, train acc: 0.8492, test acc: 0.8208\n",
      "epoch: 30, train loss: 0.3744, train acc: 0.8487, test acc: 0.7982\n",
      "record the data\n",
      "lr: 0.08, epochs: 30, batch_size: 128\n",
      "training on cuda:0\n",
      "epoch: 17, train loss: 0.4657, train acc: 0.8157, test acc: 0.7976\n",
      "epoch: 18, train loss: 0.4594, train acc: 0.8179, test acc: 0.7982\n",
      "epoch: 19, train loss: 0.4509, train acc: 0.8209, test acc: 0.8079\n",
      "epoch: 20, train loss: 0.4434, train acc: 0.8241, test acc: 0.8088\n",
      "epoch: 21, train loss: 0.4362, train acc: 0.8249, test acc: 0.8071\n",
      "epoch: 22, train loss: 0.4317, train acc: 0.8282, test acc: 0.8137\n",
      "epoch: 23, train loss: 0.4236, train acc: 0.8297, test acc: 0.8132\n",
      "epoch: 24, train loss: 0.4178, train acc: 0.8319, test acc: 0.7983\n",
      "epoch: 25, train loss: 0.4107, train acc: 0.8348, test acc: 0.8075\n",
      "epoch: 26, train loss: 0.4089, train acc: 0.8352, test acc: 0.8172\n",
      "epoch: 27, train loss: 0.4005, train acc: 0.8385, test acc: 0.7645\n",
      "epoch: 28, train loss: 0.3937, train acc: 0.8406, test acc: 0.8086\n",
      "epoch: 29, train loss: 0.3878, train acc: 0.8425, test acc: 0.8157\n",
      "epoch: 30, train loss: 0.3832, train acc: 0.8447, test acc: 0.7904\n",
      "record the data\n",
      "lr: 0.2, epochs: 30, batch_size: 128\n",
      "training on cuda:0\n",
      "epoch: 17, train loss: 0.2237, train acc: 0.9188, test acc: 0.9098\n",
      "epoch: 18, train loss: 0.2103, train acc: 0.9249, test acc: 0.8949\n",
      "epoch: 19, train loss: 0.2064, train acc: 0.9251, test acc: 0.9133\n",
      "epoch: 20, train loss: 0.1924, train acc: 0.9297, test acc: 0.9088\n",
      "epoch: 21, train loss: 0.1864, train acc: 0.9321, test acc: 0.9129\n",
      "epoch: 22, train loss: 0.1765, train acc: 0.9350, test acc: 0.9150\n",
      "epoch: 23, train loss: 0.1673, train acc: 0.9393, test acc: 0.9149\n",
      "epoch: 24, train loss: 0.1651, train acc: 0.9397, test acc: 0.9168\n",
      "epoch: 25, train loss: 0.1577, train acc: 0.9421, test acc: 0.9106\n",
      "epoch: 26, train loss: 0.1488, train acc: 0.9455, test acc: 0.9099\n",
      "epoch: 27, train loss: 0.1482, train acc: 0.9451, test acc: 0.8842\n",
      "epoch: 28, train loss: 0.1410, train acc: 0.9477, test acc: 0.9192\n",
      "epoch: 29, train loss: nan, train acc: 0.6158, test acc: 0.1000\n",
      "epoch: 30, train loss: nan, train acc: 0.1000, test acc: 0.1000\n",
      "record the data\n",
      "lr: 0.1, epochs: 30, batch_size: 64\n",
      "training on cuda:0\n",
      "epoch: 17, train loss: 1.6303, train acc: 0.3971, test acc: 0.3930\n",
      "epoch: 18, train loss: 1.6304, train acc: 0.3973, test acc: 0.3953\n",
      "epoch: 19, train loss: 1.6282, train acc: 0.3977, test acc: 0.3950\n",
      "epoch: 20, train loss: 1.6277, train acc: 0.3974, test acc: 0.3928\n",
      "epoch: 21, train loss: 1.6276, train acc: 0.3972, test acc: 0.3947\n",
      "epoch: 22, train loss: 1.6267, train acc: 0.3976, test acc: 0.3925\n",
      "epoch: 23, train loss: 1.6268, train acc: 0.3976, test acc: 0.3943\n",
      "epoch: 24, train loss: 1.6267, train acc: 0.3975, test acc: 0.3958\n",
      "epoch: 25, train loss: 1.6260, train acc: 0.3978, test acc: 0.3952\n",
      "epoch: 26, train loss: 1.6255, train acc: 0.3976, test acc: 0.3934\n",
      "epoch: 27, train loss: 1.6241, train acc: 0.3979, test acc: 0.3950\n",
      "epoch: 28, train loss: 1.6267, train acc: 0.3973, test acc: 0.3948\n",
      "epoch: 29, train loss: 1.6242, train acc: 0.3982, test acc: 0.3939\n",
      "epoch: 30, train loss: 1.6236, train acc: 0.3983, test acc: 0.3961\n",
      "record the data\n",
      "lr: 0.2, epochs: 30, batch_size: 64\n",
      "training on cuda:0\n",
      "epoch: 17, train loss: 0.4410, train acc: 0.8259, test acc: 0.8097\n",
      "epoch: 18, train loss: 0.4302, train acc: 0.8294, test acc: 0.7720\n",
      "epoch: 19, train loss: 0.4255, train acc: 0.8309, test acc: 0.8049\n",
      "epoch: 20, train loss: 0.4184, train acc: 0.8333, test acc: 0.8149\n",
      "epoch: 21, train loss: 0.4102, train acc: 0.8367, test acc: 0.8124\n",
      "epoch: 22, train loss: 0.4057, train acc: 0.8385, test acc: 0.8114\n",
      "epoch: 23, train loss: 0.3984, train acc: 0.8396, test acc: 0.8171\n",
      "epoch: 24, train loss: 0.3967, train acc: 0.8405, test acc: 0.8062\n",
      "epoch: 25, train loss: 0.3903, train acc: 0.8428, test acc: 0.8069\n",
      "epoch: 26, train loss: 0.3827, train acc: 0.8454, test acc: 0.8172\n",
      "epoch: 27, train loss: 0.3875, train acc: 0.8452, test acc: 0.7934\n",
      "epoch: 28, train loss: 0.3911, train acc: 0.8436, test acc: 0.8157\n",
      "epoch: 29, train loss: 0.3800, train acc: 0.8475, test acc: 0.8129\n",
      "epoch: 30, train loss: 0.3676, train acc: 0.8512, test acc: 0.8117\n",
      "record the data\n",
      "lr: 0.1, epochs: 30, batch_size: 512\n",
      "training on cuda:0\n",
      "epoch: 17, train loss: 0.8620, train acc: 0.6508, test acc: 0.6347\n",
      "epoch: 18, train loss: 0.8375, train acc: 0.6613, test acc: 0.6347\n",
      "epoch: 19, train loss: 0.8250, train acc: 0.6658, test acc: 0.6628\n",
      "epoch: 20, train loss: 0.8104, train acc: 0.6709, test acc: 0.6516\n",
      "epoch: 21, train loss: 0.7979, train acc: 0.6767, test acc: 0.6669\n",
      "epoch: 22, train loss: 0.7852, train acc: 0.6806, test acc: 0.6516\n",
      "epoch: 23, train loss: 0.7782, train acc: 0.6851, test acc: 0.6545\n",
      "epoch: 24, train loss: 0.7722, train acc: 0.6856, test acc: 0.6690\n",
      "epoch: 25, train loss: 0.7615, train acc: 0.6890, test acc: 0.6809\n",
      "epoch: 26, train loss: 0.7549, train acc: 0.6918, test acc: 0.6700\n",
      "epoch: 27, train loss: 0.7515, train acc: 0.6933, test acc: 0.6693\n",
      "epoch: 28, train loss: 0.7444, train acc: 0.6963, test acc: 0.6916\n",
      "epoch: 29, train loss: 0.7361, train acc: 0.6984, test acc: 0.6755\n",
      "epoch: 30, train loss: 0.7254, train acc: 0.7025, test acc: 0.6465\n",
      "record the data\n"
     ]
    }
   ],
   "source": [
    "for idx, hyper in enumerate(hyper_params):\n",
    "    Nin_net = models.NiN()\n",
    "    lr, epochs, batch_size = hyper\n",
    "    train_loader, test_loader = load_data_fashion_mnist(batch_size=batch_size, resize=96)\n",
    "    print('lr: {}, epochs: {}, batch_size: {}'.format(lr, epochs, batch_size))\n",
    "    train_models(Nin_net, train_loader, test_loader, epochs=epochs, lr=lr, net_type='Nin_hp' + str(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.1, epochs: 30, batch_size: 64\n",
      "training on cuda:0\n",
      "epoch: 17, train loss: 0.3958, train acc: 0.8399, test acc: 0.8220\n",
      "epoch: 18, train loss: 0.3895, train acc: 0.8420, test acc: 0.8024\n",
      "epoch: 19, train loss: 0.3808, train acc: 0.8452, test acc: 0.8140\n",
      "epoch: 20, train loss: 0.3781, train acc: 0.8472, test acc: 0.8186\n",
      "epoch: 21, train loss: 0.3704, train acc: 0.8489, test acc: 0.7731\n",
      "epoch: 22, train loss: 0.3593, train acc: 0.8533, test acc: 0.8185\n",
      "epoch: 23, train loss: 0.3602, train acc: 0.8523, test acc: 0.8081\n",
      "epoch: 24, train loss: 0.3460, train acc: 0.8577, test acc: 0.8131\n",
      "epoch: 25, train loss: 0.3453, train acc: 0.8589, test acc: 0.8056\n",
      "epoch: 26, train loss: 0.3424, train acc: 0.8587, test acc: 0.8220\n",
      "epoch: 27, train loss: 0.3396, train acc: 0.8609, test acc: 0.8158\n",
      "epoch: 28, train loss: 0.3317, train acc: 0.8635, test acc: 0.8222\n",
      "epoch: 29, train loss: 0.3310, train acc: 0.8638, test acc: 0.8188\n",
      "epoch: 30, train loss: 0.3258, train acc: 0.8650, test acc: 0.7884\n",
      "record the data\n"
     ]
    }
   ],
   "source": [
    "Nin_net = models.NiN()\n",
    "train_loader, test_loader = load_data_fashion_mnist(batch_size=64, resize=96)\n",
    "print('lr: {}, epochs: {}, batch_size: {}'.format(0.1, 30, 64))\n",
    "train_models(Nin_net, train_loader, test_loader, epochs=30, lr=0.1, net_type='Nin_hp' + str(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training NiN net\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwangxinming\u001b[0m (\u001b[33mgogowang\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Competition\\deep learning\\DLch7\\01AlexNet\\wandb\\run-20231222_210144-ndmgypj1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gogowang/CH7/runs/ndmgypj1' target=\"_blank\">NiN</a></strong> to <a href='https://wandb.ai/gogowang/CH7' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gogowang/CH7' target=\"_blank\">https://wandb.ai/gogowang/CH7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gogowang/CH7/runs/ndmgypj1' target=\"_blank\">https://wandb.ai/gogowang/CH7/runs/ndmgypj1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda:0\n",
      "epoch: 17, train loss: 0.6305, train acc: 0.7530, test acc: 0.7368\n",
      "epoch: 18, train loss: 0.6209, train acc: 0.7552, test acc: 0.7392\n",
      "epoch: 19, train loss: 0.6162, train acc: 0.7567, test acc: 0.7394\n",
      "epoch: 20, train loss: 0.6084, train acc: 0.7582, test acc: 0.7205\n",
      "epoch: 21, train loss: 0.6013, train acc: 0.7609, test acc: 0.7337\n",
      "epoch: 22, train loss: 0.5966, train acc: 0.7611, test acc: 0.7331\n",
      "epoch: 23, train loss: 0.5949, train acc: 0.7622, test acc: 0.7364\n",
      "epoch: 24, train loss: 0.5882, train acc: 0.7647, test acc: 0.7416\n",
      "epoch: 25, train loss: 0.5801, train acc: 0.7664, test acc: 0.7408\n",
      "epoch: 26, train loss: 0.5759, train acc: 0.7667, test acc: 0.7231\n",
      "epoch: 27, train loss: 0.5719, train acc: 0.7683, test acc: 0.7381\n",
      "epoch: 28, train loss: 0.5695, train acc: 0.7692, test acc: 0.7244\n",
      "epoch: 29, train loss: 0.5671, train acc: 0.7688, test acc: 0.7425\n",
      "epoch: 30, train loss: 0.5577, train acc: 0.7729, test acc: 0.7398\n",
      "record the data\n"
     ]
    }
   ],
   "source": [
    "print('training NiN net')\n",
    "wandb.init(project=\"CH7\",\n",
    "           name=\"NiN\")\n",
    "config = wandb.config  # config的初始化\n",
    "\n",
    "config.batch_size = 64  \n",
    "config.epochs = 30 \n",
    "config.lr = 0.1   \n",
    "config.use_cuda = True\n",
    "wandb.watch_called = False \n",
    "train_loader, test_loader = load_data_fashion_mnist(batch_size=config.batch_size, resize=96)\n",
    "Nin_net = models.NiN()\n",
    "\n",
    "train_models(Nin_net, train_loader, test_loader, epochs=config.epochs, lr=config.lr, net_type='NiN', use_wandb=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training NiNSimple net\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ndmgypj1) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e5c80fd9f24f4c8c4988e778ed500e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.102 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.010771…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_loss</td><td>▁▃▃▄▄▇▇▇▇▇██▇██████▇█████▇█▇██</td></tr><tr><td>train_acc</td><td>▁▂▅▆▆▆▆▆▇▇▇█████████████████████████████</td></tr><tr><td>train_loss</td><td>█▇▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_loss</td><td>0.7398</td></tr><tr><td>train_acc</td><td>0.77285</td></tr><tr><td>train_loss</td><td>0.5577</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">NiN</strong> at: <a href='https://wandb.ai/gogowang/CH7/runs/ndmgypj1' target=\"_blank\">https://wandb.ai/gogowang/CH7/runs/ndmgypj1</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231222_210144-ndmgypj1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ndmgypj1). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Competition\\deep learning\\DLch7\\01AlexNet\\wandb\\run-20231222_210940-s7xra9yb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gogowang/CH7/runs/s7xra9yb' target=\"_blank\">NiN_simple</a></strong> to <a href='https://wandb.ai/gogowang/CH7' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gogowang/CH7' target=\"_blank\">https://wandb.ai/gogowang/CH7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gogowang/CH7/runs/s7xra9yb' target=\"_blank\">https://wandb.ai/gogowang/CH7/runs/s7xra9yb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda:0\n",
      "epoch: 17, train loss: 0.1570, train acc: 0.9412, test acc: 0.9187\n",
      "epoch: 18, train loss: 0.1544, train acc: 0.9418, test acc: 0.9111\n",
      "epoch: 19, train loss: 0.1435, train acc: 0.9468, test acc: 0.9166\n",
      "epoch: 20, train loss: 0.1362, train acc: 0.9485, test acc: 0.9161\n",
      "epoch: 21, train loss: 0.1349, train acc: 0.9500, test acc: 0.9153\n",
      "epoch: 22, train loss: 0.1256, train acc: 0.9528, test acc: 0.9170\n",
      "epoch: 23, train loss: 0.1210, train acc: 0.9544, test acc: 0.9131\n",
      "epoch: 24, train loss: 0.1157, train acc: 0.9562, test acc: 0.9196\n",
      "epoch: 25, train loss: 0.1072, train acc: 0.9601, test acc: 0.9183\n",
      "epoch: 26, train loss: 0.1091, train acc: 0.9591, test acc: 0.9141\n",
      "epoch: 27, train loss: 0.1015, train acc: 0.9613, test acc: 0.9120\n",
      "epoch: 28, train loss: 0.0969, train acc: 0.9635, test acc: 0.9094\n",
      "epoch: 29, train loss: 0.0923, train acc: 0.9646, test acc: 0.9139\n",
      "epoch: 30, train loss: 0.0884, train acc: 0.9673, test acc: 0.9037\n",
      "record the data\n"
     ]
    }
   ],
   "source": [
    "print('training NiNSimple net')\n",
    "\n",
    "wandb.init(project=\"CH7\",\n",
    "           name=\"NiN_simple\")\n",
    "config = wandb.config  # config的初始化\n",
    "config.batch_size = 64  \n",
    "config.epochs = 30 \n",
    "config.lr = 0.1   \n",
    "config.use_cuda = True\n",
    "wandb.watch_called = False\n",
    "\n",
    "train_loader, test_loader = load_data_fashion_mnist(batch_size=config.batch_size, resize=96)\n",
    "Nin_net = models.NiN_Simple()\n",
    "train_models(Nin_net, train_loader, test_loader, epochs=config.epochs, lr=config.lr, net_type='NiN_Simple', use_wandb=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
