{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "huFZurQ5H6Ge"
      },
      "source": [
        "# 第6章 卷积神经网络"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdtYxLrmH6Gf"
      },
      "source": [
        "## 6.1 从全连接层到卷积"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHXEMzORH6Gg"
      },
      "source": [
        "### 练习 6.1.1\n",
        "\n",
        "假设卷积层(6.3)覆盖的局部区域$\\Delta = 0$。在这种情况下，证明卷积内核为每组通道独立地实现一个全连接层。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJmj0EV6H6Gg"
      },
      "source": [
        "**解答：** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPSRZGQ_H6Gg"
      },
      "source": [
        "&emsp;&emsp;局部区域$\\Delta=0$ 表示卷积核的大小等于输入的大小。实际就是问，1×1的卷积核是否等价于全连接（参见本书7.3节：NiN网络结构）。因此，每个卷积核只能覆盖一个像素点，在这种情况下，卷积层的计算方式与全连接层非常相似。因为每个卷积核只能看到一个通道的信息，相当于每个卷积核只是一个全连接层的权重矩阵。 所以，卷积内核可以看作是每组通道独立地实现一个全连接层。每个卷积核都有自己的权重，每个输入通道都被独立处理，输出通道是各个输入通道的加权和。这种独立处理的方式有效地减少了权重的数量，从而降低了计算成本，并且能够提取出输入数据中的空间特征。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDK67aUiH6Gg",
        "outputId": "7a6b52e0-493d-421f-ad12-6258356a3399"
      },
      "outputs": [],
      "source": [
        "# 代码验证\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# 定义一个神经网络类MyNet1，继承自nn.Module\n",
        "class MyNet1(nn.Module):\n",
        "    def __init__(self, linear1, linear2):\n",
        "        super(MyNet1, self).__init__()\n",
        "        # 初始化两个线性层\n",
        "        self.linear1 = linear1\n",
        "        self.linear2 = linear2\n",
        "\n",
        "    def forward(self, X):\n",
        "        # 定义前向传播过程：先展平输入X，然后通过两个线性层\n",
        "        return self.linear2(self.linear1(nn.Flatten()(X)))\n",
        "\n",
        "\n",
        "# 定义另一个神经网络类MyNet2，也继承自nn.Module\n",
        "class MyNet2(nn.Module):\n",
        "    def __init__(self, linear, conv2d):\n",
        "        super(MyNet2, self).__init__()\n",
        "        # 初始化一个线性层和一个卷积层\n",
        "        self.linear = linear\n",
        "        self.conv2d = conv2d\n",
        "\n",
        "    def forward(self, X):\n",
        "        # 定义前向传播过程\n",
        "        X = self.linear(nn.Flatten()(X))  # 先展平输入X，然后通过线性层\n",
        "        X = X.reshape(X.shape[0], -1, 1, 1)  # 调整X的形状以适应卷积层的输入\n",
        "        X = nn.Flatten()(self.conv2d(X))  # 通过卷积层，然后再次展平\n",
        "        return X\n",
        "\n",
        "\n",
        "# 初始化线性层和卷积层\n",
        "linear1 = nn.Linear(15, 10)\n",
        "linear2 = nn.Linear(10, 5)\n",
        "conv2d = nn.Conv2d(10, 5, 1)\n",
        "\n",
        "# 将卷积层的权重和偏置参数重塑并分别赋值给linear2的权重和偏置\n",
        "linear2.weight = nn.Parameter(conv2d.weight.reshape(linear2.weight.shape))\n",
        "linear2.bias = nn.Parameter(conv2d.bias)\n",
        "\n",
        "# 实例化两个网络\n",
        "net1 = MyNet1(linear1, linear2)\n",
        "net2 = MyNet2(linear1, conv2d)\n",
        "\n",
        "# 创建一个随机输入张量X\n",
        "X = torch.randn(2, 3, 5)\n",
        "# 打印两个网络对同一输入X的输出结果\n",
        "# 两个结果实际存在一定的误差，直接print(net1(X) == net2(X))得到的结果不全是True\n",
        "print(net1(X))\n",
        "print(net2(X))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BX-MU4UWH6Gi"
      },
      "source": [
        "### 练习 6.1.2\n",
        "\n",
        "为什么平移不变性可能也不是好主意呢？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjZs5ROiH6Gi"
      },
      "source": [
        "**解答：** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVHNkPmfH6Gi"
      },
      "source": [
        "&emsp;&emsp;平移不变性是一种在信号处理和图像分析中常见的特性，尤其是在卷积神经网络（`CNNs`）中。它意味着一个系统或函数对输入数据的平移是不变的，即如果输入数据发生平移，输出不会改变。这对于很多应用来说是有益的，比如在图像识别中，无论物体在图像中的位置如何，模型都能识别它。\\\n",
        "&emsp;&emsp;平移不变性可能也存在一些局限性或不足之处：\\\n",
        "&emsp;&emsp;1.丢失空间信息：当模型对位置不敏感时，它可能无法识别对象的确切位置或对象之间的空间关系。在某些任务中，如场景理解或对象定位，这种空间信息非常重要。\\\n",
        "&emsp;&emsp;2.不适用于所有任务：对于一些特定的任务，如图像中文本的识别或布局分析，平移不变性可能不是一个理想的特性，因为这些任务需要对位置和排列非常敏感。\\\n",
        "&emsp;&emsp;3.过度泛化：平移不变性可能导致模型过度泛化，无法识别某些应该被视为不同的模式或对象。例如，在医学成像分析中，肿瘤的确切位置对于诊断至关重要。\\\n",
        "&emsp;&emsp;4.缩放和旋转问题：虽然平移不变性处理位置的变化，但它不处理缩放或旋转，这可能是图像识别中的关键因素。\\\n",
        "&emsp;&emsp;5.计算效率：为了实现平移不变性，卷积网络通常需要更多的参数和计算资源，这可能导致效率低下，尤其是在资源受限的环境中。\\\n",
        "&emsp;&emsp;6.局部性限制：平移不变性通常通过局部感受野实现，这可能限制模型捕捉长距离依赖或大尺度结构的能力。\n",
        "\n",
        "&emsp;&emsp;参考：[https://arxiv.org/pdf/1805.12177.pdf](https://arxiv.org/pdf/1805.12177.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwOkiEoTH6Gj"
      },
      "source": [
        "### 练习 6.1.3\n",
        "\n",
        "当从图像边界像素获取隐藏表示时，我们需要思考哪些问题？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-VNPxDKH6Gj"
      },
      "source": [
        "**解答：** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5z7Gh2xH6Gj"
      },
      "source": [
        "&emsp;&emsp;考虑是否填充`padding`，以及填充多大的`padding`的问题。可以使用`torch.nn`模块中的`functional.pad`函数对图像进行填充操作，以保证边界像素的信息完整。填充后还需要进行额外的处理，例如使用图像复制、填充、平滑等方法来获取隐藏表示。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmJosLlWH6Gj"
      },
      "source": [
        "### 练习 6.1.4\n",
        "\n",
        "描述一个类似的音频卷积层的架构。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCd3SLSwH6Gj"
      },
      "source": [
        "**解答：** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zktnQ2nWH6Gj"
      },
      "source": [
        "&emsp;&emsp;一种基于卷积神经网络的音频特征生成方法，首先对声音信号进行预处理和离散傅里叶变换计算声音信号的幅度谱，形成二维谱图信号；然后搭建以上述二维谱图信号为输入的一维卷积神经网络并进行模型训练，得到特征生成器模型；最后对待测声音进行预处理和离散傅里叶变换得到二维谱图信号，并将其送入训练好的一维卷积神经网络，通过卷积网络计算，得到输出即为所要生成的音频特征，实现声音信号的音频特征生成。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGBTjTXVH6Gj"
      },
      "source": [
        "### 练习 6.1.5\n",
        "\n",
        "卷积层也适合于文本数据吗？为什么？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGi_RvNkH6Gk"
      },
      "source": [
        "**解答：** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "il8LGBpTH6Gk"
      },
      "source": [
        "&emsp;&emsp;卷积层也适合于文本数据。 在自然语言处理中，文本数据通常表示为词向量矩阵，其中每行代表一个词的向量表示。卷积层可以在这个矩阵上进行卷积操作，类似于图像卷积层中对图像进行卷积操作。 在卷积层中，卷积核会在输入矩阵上进行滑动窗口计算，输出一个新的特征矩阵。在文本数据中，这个特征矩阵可以看作是对输入文本的不同`n-gram`特征的提取。例如，一个大小为3的卷积核可以提取出输入文本中每个长度为3的n-gram特征。这些特征可以用于后续的分类或者回归任务。 此外，卷积层还可以与循环神经网络（`RNN`）结合使用，形成卷积神经网络（`CNN`）和循环神经网络（`RNN`）的混合模型。这种模型可以同时捕捉文本中的局部特征和全局特征，提高模型的性能。 因此，卷积层适用于文本数据，可以对文本数据进行卷积操作，提取出不同`n-gram`特征，并且可以与`RNN`结合使用，提高模型的性能。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8asaRppH6Gk"
      },
      "source": [
        "### 练习 6.1.6\n",
        "\n",
        "证明在式(6.6)中，$f * g = g * f$。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeBwPGyRH6Gk"
      },
      "source": [
        "**解答：** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZOvmmOkH6Gk"
      },
      "source": [
        "&emsp;&emsp;通过式(6.6)的定义，我们可以得到：\n",
        "\n",
        "$$(f * g)(x) = \\int_{-\\infty}^{\\infty}f(y)g(x-y)dy$$\n",
        "\n",
        "$$(g * f)(x) = \\int_{-\\infty}^{\\infty}g(y)f(x-y)dy$$\n",
        "\n",
        "&emsp;&emsp;要证明$f * g = g * f$，即证明：\n",
        "\n",
        "$$\\int_{-\\infty}^{\\infty}f(y)g(x-y)dy = \\int_{-\\infty}^{\\infty}g(y)f(x-y)dy$$\n",
        "\n",
        "&emsp;&emsp;为了证明上式成立，我们将其中一个积分的变量名改为$t=x-y$，则有：\n",
        "\n",
        "$$\\int_{-\\infty}^{\\infty}f(y)g(x-y)dy = \\int_{-\\infty}^{\\infty}f(x-t)g(t)dt$$\n",
        "\n",
        "&emsp;&emsp;再将这个式子代回式(6.6)中：\n",
        "\n",
        "$$(f * g)(x) = \\int_{-\\infty}^{\\infty}f(x-t)g(t)dt$$\n",
        "\n",
        "&emsp;&emsp;对比式(6.6)和上面的式子，可以发现它们的形式是完全一样的，只是积分变量名不同而已。因此，我们可以得到：\n",
        "\n",
        "$$(f * g)(x) = \\int_{-\\infty}^{\\infty}f(y)g(x-y)dy = \\int_{-\\infty}^{\\infty}g(y)f(x-y)dy = (g * f)(x)$$\n",
        "\n",
        "&emsp;&emsp;因此，$f * g = g * f$，证毕。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnoQzdf5H6Gk"
      },
      "source": [
        "## 6.2 图像卷积"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_ATWo-vH6Gk"
      },
      "source": [
        "### 练习 6.2.1  \n",
        "\n",
        "构建一个具有对角线边缘的图像`X`。\n",
        "1. 如果将本节中举例的卷积核`K`应用于`X`，会发生什么情况？\n",
        "2. 如果转置`X`会发生什么？\n",
        "3. 如果转置`K`会发生什么？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBApQWHKH6Gk"
      },
      "source": [
        "**解答：** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkEMrGV4SCW5"
      },
      "source": [
        "**第1问：**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kq6MSSR8H6Gk"
      },
      "source": [
        "&emsp;&emsp;在对角线处有分别为1和-1的数据，其他区域都为0。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiEfX0AoH6Gl",
        "outputId": "37bc785e-2ef0-44d2-d047-0c732d1cb81e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l\n",
        "\n",
        "def corr2d(X, K):  #@save\n",
        "    \"\"\"计算二维互相关运算\"\"\"\n",
        "    h, w = K.shape  # 获取卷积核的高度和宽度\n",
        "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))  # 初始化输出矩阵 Y，大小为 (X行数 - K行数 + 1) x (X列数 - K列数 + 1)\n",
        "    for i in range(Y.shape[0]):  # 遍历 Y 的行数\n",
        "        for j in range(Y.shape[1]):  # 遍历 Y 的列数\n",
        "            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()  # 在 Y 的每个位置，计算 X 与 K 的互相关运算结果\n",
        "    return Y\n",
        "\n",
        "# 创建一个8x8的单位矩阵 X\n",
        "X = torch.eye(8)\n",
        "\n",
        "# 创建一个1x2的卷积核 K，该卷积核在X上进行互相关运算\n",
        "K = torch.tensor([[1.0, -1.0]])\n",
        "\n",
        "# 调用corr2d函数，计算互相关运算结果Y\n",
        "Y = corr2d(X, K)\n",
        "\n",
        "# 打印互相关运算结果Y\n",
        "print(Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3Oz9yYDSdI3"
      },
      "source": [
        "**第2问：**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUbUM7rZH6Gl"
      },
      "source": [
        "&emsp;&emsp;转置后结果不变。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02INn5CxH6Gl",
        "outputId": "8841140a-0258-40f5-a0be-1f9306ef7b60"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l\n",
        "\n",
        "def corr2d(X, K):  #@save\n",
        "    \"\"\"计算二维互相关运算\"\"\"\n",
        "    h, w = K.shape  # 获取卷积核的高度和宽度\n",
        "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))  # 初始化输出矩阵 Y，大小为 (X行数 - K行数 + 1) x (X列数 - K列数 + 1)\n",
        "    for i in range(Y.shape[0]):  # 遍历 Y 的行数\n",
        "        for j in range(Y.shape[1]):  # 遍历 Y 的列数\n",
        "            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()  # 在 Y 的每个位置，计算 X 与 K 的互相关运算结果\n",
        "    return Y\n",
        "\n",
        "# 创建一个8x8的单位矩阵 X\n",
        "X = torch.eye(8)\n",
        "\n",
        "# 创建一个1x2的卷积核 K，该卷积核在X的转置上进行互相关运算\n",
        "K = torch.tensor([[1.0, -1.0]])\n",
        "\n",
        "# 调用corr2d函数，计算互相关运算结果Y\n",
        "Y = corr2d(X.T, K)\n",
        "\n",
        "# 打印输入矩阵 X 和互相关运算结果 Y\n",
        "X, Y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWwEZaw0StRK"
      },
      "source": [
        "**第3问：**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTOiA5ipH6Gl"
      },
      "source": [
        "&emsp;&emsp;K转置后，结果也转置了 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yD4k1hrmH6Gl",
        "outputId": "6502c42f-12b4-4260-fc74-f1987a55f870"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l\n",
        "\n",
        "def corr2d(X, K):  #@save\n",
        "    \"\"\"计算二维互相关运算\"\"\"\n",
        "    h, w = K.shape  # 获取卷积核的高度和宽度\n",
        "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))  # 初始化输出矩阵 Y，大小为 (X行数 - K行数 + 1) x (X列数 - K列数 + 1)\n",
        "    for i in range(Y.shape[0]):  # 遍历 Y 的行数\n",
        "        for j in range(Y.shape[1]):  # 遍历 Y 的列数\n",
        "            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()  # 在 Y 的每个位置，计算 X 与 K 的互相关运算结果\n",
        "    return Y\n",
        "\n",
        "# 创建一个8x8的单位矩阵 X\n",
        "X = torch.eye(8)\n",
        "\n",
        "# 创建一个1x2的卷积核 K，将其转置后与 X 进行互相关运算\n",
        "K = torch.tensor([[1.0, -1.0]])\n",
        "\n",
        "# 调用 corr2d 函数，计算互相关运算结果 Y\n",
        "Y = corr2d(X, K.T)\n",
        "\n",
        "# 打印输入矩阵 X 和互相关运算结果 Y\n",
        "X, Y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jxhbn1TzH6Gl"
      },
      "source": [
        "### 练习 6.2.2 \n",
        "\n",
        "在我们创建的`Conv2D`自动求导时，有什么错误消息？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZh5dWTzH6Gl"
      },
      "source": [
        "**解答：** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsZJjjLoH6Gl"
      },
      "source": [
        "&emsp;&emsp;会提示维度不对称的错误信息，因为torch提供的二维卷积层是nn.Conv2d() 采用的是四维输入和输出格式（批量大小、通道、高度、宽度）,而我们自定义的仅仅是二维的。\n",
        "\n",
        "&emsp;&emsp;代码验证如下"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjVH6TpjAKGX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l\n",
        "\n",
        "def corr2d(X, K):  #@save\n",
        "    \"\"\"计算二维互相关运算\"\"\"\n",
        "    h, w = K.shape  # 获取卷积核的高度和宽度\n",
        "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))  # 初始化输出矩阵 Y，大小为 (X行数 - K行数 + 1) x (X列数 - K列数 + 1)\n",
        "    for i in range(Y.shape[0]):  # 遍历 Y 的行数\n",
        "        for j in range(Y.shape[1]):  # 遍历 Y 的列数\n",
        "            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()  # 在 Y 的每个位置，计算 X 与 K 的互相关运算结果\n",
        "    return Y\n",
        "\n",
        "# 定义一个简单的二维卷积层\n",
        "class Conv2D(nn.Module):\n",
        "    def __init__(self, kernel_size):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.rand(kernel_size))\n",
        "        self.bias = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return corr2d(x, self.weight) + self.bias\n",
        "\n",
        "# 创建一个6x8的矩阵 X，其中一部分值为0\n",
        "X = torch.ones((6, 8))\n",
        "X[:, 2:6] = 0\n",
        "X\n",
        "\n",
        "# 创建一个1x2的卷积核 K，该卷积核用于检测输入 X 中的边缘特征\n",
        "K = torch.tensor([[1.0, -1.0]])\n",
        "\n",
        "# 使用卷积核 K 对输入 X 进行互相关运算，将边缘特征检测的结果存储在 Y 中\n",
        "Y = corr2d(X, K)\n",
        "Y\n",
        "\n",
        "# 对输入 X 进行转置，然后再次使用卷积核 K 进行互相关运算\n",
        "corr2d(X.t(), K)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6bL6eI5xFLN"
      },
      "source": [
        "&emsp;&emsp;使用`nn.Conv2d`时可以正常运行"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37pG9gVxBv-x",
        "outputId": "1c256b5d-79c0-4da3-b8c8-c5f480aeb04d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l\n",
        "\n",
        "def corr2d(X, K):  #@save\n",
        "    \"\"\"计算二维互相关运算\"\"\"\n",
        "    h, w = K.shape  # 获取卷积核的高度和宽度\n",
        "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))  # 初始化输出矩阵 Y，大小为 (X行数 - K行数 + 1) x (X列数 - K列数 + 1)\n",
        "    for i in range(Y.shape[0]):  # 遍历 Y 的行数\n",
        "        for j in range(Y.shape[1]):  # 遍历 Y 的列数\n",
        "            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()  # 在 Y 的每个位置，计算 X 与 K 的互相关运算结果\n",
        "    return Y\n",
        "\n",
        "# 定义一个简单的二维卷积层\n",
        "class Conv2D(nn.Module):\n",
        "    def __init__(self, kernel_size):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.rand(kernel_size))\n",
        "        self.bias = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return corr2d(x, self.weight) + self.bias\n",
        "\n",
        "# 创建一个6x8的矩阵 X，其中一部分值为0\n",
        "X = torch.ones((6, 8))\n",
        "X[:, 2:6] = 0\n",
        "X\n",
        "\n",
        "# 创建一个1x2的卷积核 K，该卷积核用于检测输入 X 中的边缘特征\n",
        "K = torch.tensor([[1.0, -1.0]])\n",
        "\n",
        "# 使用卷积核 K 对输入 X 进行互相关运算，将边缘特征检测的结果存储在 Y 中\n",
        "Y = corr2d(X, K)\n",
        "Y\n",
        "\n",
        "# 对输入 X 进行转置，然后再次使用卷积核 K 进行互相关运算\n",
        "corr2d(X.t(), K)\n",
        "# 构造一个二维卷积层，它具有1个输出通道和形状为（1，2）的卷积核\n",
        "conv2d = nn.Conv2d(1,1, kernel_size=(1, 2), bias=False)\n",
        "\n",
        "# 这个二维卷积层使用四维输入和输出格式（批量大小、通道、高度、宽度），\n",
        "# 其中批量大小和通道数都为1\n",
        "# 将输入 X 和输出 Y 调整为四维张量的形状\n",
        "X = X.reshape((1, 1, 6, 8))\n",
        "Y = Y.reshape((1, 1, 6, 7))\n",
        "lr = 3e-2  # 学习率\n",
        "\n",
        "# 训练卷积层的权重\n",
        "for i in range(10):\n",
        "    Y_hat = conv2d(X)\n",
        "    l = (Y_hat - Y) ** 2\n",
        "    conv2d.zero_grad()\n",
        "    l.sum().backward()\n",
        "    # 迭代卷积核权重\n",
        "    conv2d.weight.data[:] -= lr * conv2d.weight.grad\n",
        "    if (i + 1) % 2 == 0:\n",
        "        print(f'epoch {i+1}, loss {l.sum():.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPIxue-DxOx-"
      },
      "source": [
        "&emsp;&emsp;使用创建的`Conv2D`时可以会报如下错误"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msTn2YsGDq87",
        "outputId": "83bafb8a-8ffe-42fb-ccb5-775efe42b095"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The size of tensor a (0) must match the size of tensor b (7) at non-singleton dimension 3\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l\n",
        "\n",
        "def corr2d(X, K):  #@save\n",
        "    \"\"\"计算二维互相关运算\"\"\"\n",
        "    h, w = K.shape  # 获取卷积核的高度和宽度\n",
        "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))  # 初始化输出矩阵 Y，大小为 (X行数 - K行数 + 1) x (X列数 - K列数 + 1)\n",
        "    for i in range(Y.shape[0]):  # 遍历 Y 的行数\n",
        "        for j in range(Y.shape[1]):  # 遍历 Y 的列数\n",
        "            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()  # 在 Y 的每个位置，计算 X 与 K 的互相关运算结果\n",
        "    return Y\n",
        "conv2d = Conv2D(kernel_size=(1, 2)) # 使用正确的 kernel_size 参数\n",
        "\n",
        "try:\n",
        "  for i in range(10):\n",
        "    Y_hat = conv2d(X)\n",
        "    l = (Y_hat - Y) ** 2\n",
        "    conv2d.zero_grad()\n",
        "    l.sum().backward()\n",
        "    # 迭代卷积核\n",
        "    conv2d.weight.data[:] -= lr * conv2d.weight.grad\n",
        "    if (i + 1) % 2 == 0:\n",
        "        print(f'epoch {i+1}, loss {l.sum():.3f}')\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gax6VocXxTnp"
      },
      "source": [
        "&emsp;&emsp;可以通过下述方式调整"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMFz77vLEcrd",
        "outputId": "650e4729-b223-4fb7-9bd9-429b4dcb77eb"
      },
      "outputs": [],
      "source": [
        "\n",
        "X = X.reshape((6, 8))\n",
        "Y = Y.reshape((6, 7))\n",
        "lr = 3e-2  # 学习率\n",
        "\n",
        "for i in range(10):\n",
        "    Y_hat = conv2d(X)\n",
        "    l = (Y_hat - Y) ** 2\n",
        "    conv2d.zero_grad()\n",
        "    l.sum().backward()\n",
        "    # 迭代卷积核\n",
        "    conv2d.weight.data[:] -= lr * conv2d.weight.grad\n",
        "    if (i + 1) % 2 == 0:\n",
        "        print(f'epoch {i+1}, loss {l.sum():.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rl_95PQH6Gm"
      },
      "source": [
        "### 练习 6.2.3\n",
        "\n",
        "如何通过改变输入张量和卷积核张量，将互相关运算表示为矩阵乘法？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqzn_dbiH6Gm"
      },
      "source": [
        "**解答：** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzbnEauZH6Gm"
      },
      "source": [
        "&emsp;&emsp;题目的意思应该是如何通过矩阵乘法得到互相关（卷积）运算"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIojUxrQH6Gm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l\n",
        "\n",
        "def conv2d_by_mul(X, K):\n",
        "    # 获取卷积核大小\n",
        "    h, w = K.shape\n",
        "    # 计算输出图像大小\n",
        "    outh = X.shape[0] - h + 1\n",
        "    outw = X.shape[1] - w + 1\n",
        "    # 调整卷积核形状以便做乘法\n",
        "    K = K.reshape(-1, 1)\n",
        "    # 将输入图像切成卷积核大小的块，打平成一维，存放在列表 Y 中\n",
        "    Y = []\n",
        "    for i in range(outh):\n",
        "        for j in range(outw):\n",
        "            Y.append(X[i:i + h, j:j + w].reshape(-1))\n",
        "    # 将列表 Y 转为张量，每行代表一块的打平结果\n",
        "    Y = torch.stack(Y, 0)\n",
        "    # 用矩阵乘法表示互相关运算\n",
        "    res = (torch.matmul(Y, K)).reshape(outh, outw)\n",
        "    # 返回输出结果\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ta-sCxFH6Gm"
      },
      "source": [
        "### 练习 6.2.4\n",
        "\n",
        "手工设计一些卷积核。\n",
        "1. 二阶导数的核的形式是什么？\n",
        "1. 积分的核的形式是什么？\n",
        "1. 得到$d$次导数的最小核的大小是多少？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TpbNlmoH6Gm"
      },
      "source": [
        "**解答：** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sj92on4zTnmx"
      },
      "source": [
        "**第1问：**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFTY9ib1Tr6j"
      },
      "source": [
        "&emsp;&emsp;一维的二阶导数的核的形式是：\n",
        "\n",
        "$$\\begin{bmatrix}-1 & 2 & -1\\end{bmatrix}$$\n",
        "\n",
        "&emsp;&emsp;这个一维卷积核可以用于计算信号或函数在每个点的二阶导数。它通过对信号进行两次微分来强调信号的曲率和变化率，通常用于边缘检测和特征检测等任务。\n",
        "\n",
        "&emsp;&emsp;在二维情况下，可以使用以下卷积核来计算图像的二阶导数：\n",
        "\n",
        "\n",
        "\\begin{bmatrix}\n",
        "    1 & 0 & -1 \\\\\n",
        "    2 & 0 & -2 \\\\\n",
        "    1 & 0 & -1 \\\\\n",
        "\\end{bmatrix}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjHoDCPDTwf8"
      },
      "source": [
        "**第2问：**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-kf1IxsH6Gm"
      },
      "source": [
        "&emsp;&emsp;积分的核的形式是：\n",
        "\n",
        "$$\\begin{bmatrix}1 & 1 & 1 & \\cdots & 1\\end{bmatrix}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20GNRTaGT3vl"
      },
      "source": [
        "**第3问：**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLPsGuj_T560"
      },
      "source": [
        "&emsp;&emsp;得到 𝑑 次导数的最小核的大小是 $d+1$。例如，一阶导数的最小核大小为 $2$，二阶导数的最小核大小为 $3$，三阶导数的最小核大小为 $4$，以此类推。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xxl6bZHSH6Gm"
      },
      "source": [
        "## 6.3 填充和步幅 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4I1FfTtH6Gm"
      },
      "source": [
        "### 练习 6.3.1  \n",
        "\n",
        "对于本节中的最后一个示例，计算其输出形状，以查看它是否与实验结果一致。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db2l2ZVaH6Gm"
      },
      "source": [
        "**解答：** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUp1JW-cH6Gm"
      },
      "source": [
        "$$out_{shape}=\\lfloor(n_h-k_h+p_h*2+s_h)/s_h\\rfloor \\times \\lfloor(n_w-k_w+p_w*2+s_w)/s_w\\rfloor.$$\n",
        "\n",
        "&emsp;&emsp;示例中$X.shape = [8, 8]$，计算得出$out_shape = [(8-3+0+3)/3, (8-5+2+4)/4] = [2.67, 2.25]$,向下取整，所以为$[2, 2]$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTN4MS6Vzv-Y"
      },
      "source": [
        "&emsp;&emsp; 代码验证如下"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Un2WNAxXzVvO",
        "outputId": "caf4dd00-192e-4cbb-def6-8dc86a3a632c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "# 为了方便起见，我们定义了一个计算卷积层的函数。\n",
        "# 此函数初始化卷积层权重，并对输入和输出提高和缩减相应的维数\n",
        "def comp_conv2d(conv2d, X):\n",
        "    # 这里的（1，1）表示批量大小和通道数都是1\n",
        "    X = X.reshape((1, 1) + X.shape)\n",
        "    Y = conv2d(X)\n",
        "    # 省略前两个维度：批量大小和通道\n",
        "    return Y.reshape(Y.shape[2:])\n",
        "\n",
        "# 请注意，这里每边都填充了1行或1列，因此总共添加了2行或2列\n",
        "X = torch.rand(size=(8, 8))\n",
        "conv2d = nn.Conv2d(1, 1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))\n",
        "comp_conv2d(conv2d, X).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brG_eGKcH6Gn"
      },
      "source": [
        "### 练习 6.3.2\n",
        "\n",
        "在本节中的实验中，试一试其他填充和步幅组合。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFoUTJ82H6Gn"
      },
      "source": [
        "**解答：** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8bXitvc1p0Q"
      },
      "source": [
        "&emsp;&emsp; 下面将举两个其他的填充和步幅组合"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RMAerR41NZk",
        "outputId": "e84b746f-5981-4e7d-a571-2cfb8dd7beda"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "def comp_conv2d(conv2d, X):\n",
        "    # 这里的（1，1）表示批量大小和通道数都是1\n",
        "    X = X.reshape((1, 1) + X.shape)\n",
        "    Y = conv2d(X)\n",
        "    # 省略前两个维度：批量大小和通道\n",
        "    return Y.reshape(Y.shape[2:])\n",
        "\n",
        "# 请注意，这里每边都填充了1行或1列，因此总共添加了2行或2列\n",
        "X = torch.rand(size=(8, 8))\n",
        "conv2d = nn.Conv2d(1, 1, kernel_size=(3, 5), padding=(1, 2), stride=(2, 3))\n",
        "comp_conv2d(conv2d, X).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJ3E2kSW1xuG"
      },
      "source": [
        "&emsp;&emsp;在`padding`大小为$[1,2]$，`stride`大小为$[2,3]$时，输出形状为$[4,3]$\n",
        "\n",
        "&emsp;&emsp;示例中$X.shape = [8, 8]$，计算得出$out_{shape} = [(8-3+2+2)/2, (8-5+4+3)/3] = [4.5, 3,33]$,向下取整，所以为$[4, 3]$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbrqAE0G1bok",
        "outputId": "217140fc-29d4-45a5-eb18-c69ead1c1ef6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "def comp_conv2d(conv2d, X):\n",
        "    # 这里的（1，1）表示批量大小和通道数都是1\n",
        "    X = X.reshape((1, 1) + X.shape)\n",
        "    Y = conv2d(X)\n",
        "    # 省略前两个维度：批量大小和通道\n",
        "    return Y.reshape(Y.shape[2:])\n",
        "\n",
        "# 请注意，这里每边都填充了1行或1列，因此总共添加了2行或2列\n",
        "X = torch.rand(size=(8, 8))\n",
        "conv2d = nn.Conv2d(1, 1, kernel_size=(3, 5), padding=(2, 3), stride=(1, 2))\n",
        "comp_conv2d(conv2d, X).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KVUq8Zu4Okx"
      },
      "source": [
        "&emsp;&emsp;在`padding`大小为$[2,3]$，`stride`大小为$[1,2]$时，输出形状为$[10,5]$\n",
        "\n",
        "&emsp;&emsp;示例中$X.shape = [8, 8]$，计算得出$out_{shape} = [(8-3+4+1)/1, (8-5+6+2)/2] = [10, 5.5]$,向下取整，所以为$[10, 5]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTl-5K1SH6Gn"
      },
      "source": [
        "### 练习 6.3.3\n",
        "\n",
        "对于音频信号，步幅$2$说明什么？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNXFYzlhH6Gn"
      },
      "source": [
        "**解答：** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuueFr5_H6Gn"
      },
      "source": [
        "&emsp;&emsp;对于音频信号而言，步幅为2就是以2为周期对信号进行采样计算。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3anPQxXH6Gn"
      },
      "source": [
        "### 练习 6.3.4\n",
        "\n",
        "步幅大于$1$的计算优势是什么？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q02rwtZnH6Gn"
      },
      "source": [
        "**解答：** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_po2mkiH6Gn"
      },
      "source": [
        "&emsp;&emsp;减小计算量， 减小内存占用， 提高模型的泛化能力。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLzkTd9KH6Gn"
      },
      "source": [
        "## 6.4 多输入多输出通道 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0Acj8q_H6Gn"
      },
      "source": [
        "### 练习 6.4.1\n",
        "\n",
        "假设我们有两个卷积核，大小分别为$k_1$和$k_2$（中间没有非线性激活函数）。\n",
        "1. 证明运算可以用单次卷积来表示。\n",
        "1. 这个等效的单个卷积核的维数是多少呢？\n",
        "1. 反之亦然吗？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WvYPWNzH6Go"
      },
      "source": [
        "**解答：** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCHo1nmiW269"
      },
      "source": [
        "**第1问：**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0WKW_2rH6Go"
      },
      "source": [
        "&emsp;&emsp;假设输入的图像大小为$W×H$，设卷积核1的大小为$k_1$，卷积核2的大小为$k2$，它们分别作用于输入矩阵$x$，得到的输出矩阵分别为$y_1$和$y_2$。则可以将$y1$与$y2$的每一个元素相加，得到最终输出矩阵$y$。\n",
        "\n",
        "&emsp;&emsp;即：$$y[i][j] = y_1[i][j] + y_2[i][j]$$\n",
        "\n",
        "&emsp;&emsp;可以将两个卷积核的大小相加，得到一个新的卷积核大小为$(k_1+k_2-1)×(k_1+k_2-1)$。然后可以将这个新的卷积核应用于输入图像，得到一个输出图像。这个输出图像的大小为$(W-k_1-k_2+2)×(H-k_1-k_2+2)$。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_oG7ejjZIts"
      },
      "source": [
        "**第2问：**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu5hDtACZI_Q"
      },
      "source": [
        "&emsp;&emsp;可以使用一个大小为$(k_1+k_2-1)×(k_1+k_2-1)$的卷积核来表示这两个卷积核的运算。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucb3Vc2pZJUL"
      },
      "source": [
        "**第3问：**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKoiH9o-ZJbF"
      },
      "source": [
        "&emsp;&emsp;反之亦然。如果有一个大小为$k_1+k_2-1$的卷积核，可以将其分解为两个大小分别为$k1$和$k2$的卷积核。这两个卷积核之间没有非线性激活函数，所以它们的运算可以被视为一个单独的卷积核。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5H7cCLpH6Go"
      },
      "source": [
        "### 练习 6.4.2\n",
        "\n",
        "假设输入为$c_i\\times h\\times w$，卷积核大小为$c_o\\times c_i\\times k_h\\times k_w$，填充为$(p_h, p_w)$，步幅为$(s_h, s_w)$。\n",
        "1. 前向传播的计算成本（乘法和加法）是多少？\n",
        "1. 内存占用是多少？\n",
        "1. 反向传播的内存占用是多少？\n",
        "1. 反向传播的计算成本是多少？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a-nXFFtH6Go"
      },
      "source": [
        "**解答：** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPC8XK92bNMD"
      },
      "source": [
        "**第1问：**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlNurvDQbNQ0"
      },
      "source": [
        "&emsp;&emsp;前向计算成本为\n",
        "\n",
        "$$flops_{forward} = c_i \\times c_o \\times k_h \\times k_w \\times m_h \\times m_w$$\n",
        "\n",
        "&emsp;&emsp;其中$m_h=\\lfloor \\frac{h+2p_h-k_h}{s_h}+1 \\rfloor$, $m_w=\\lfloor \\frac{w+2p_w-k_w}{s_w}+1 \\rfloor$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_oix-mgbNZ3"
      },
      "source": [
        "**第2问：**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh1SQkZbbNhz"
      },
      "source": [
        "&emsp;&emsp;输入需要$c_i*h*w$个浮点数，卷积核需要$c_o*c_i*k_h*k_w$个浮点数，输出需要$c_o*m_h*m_w$个浮点数。此外，还需要存储中间结果，即填充后的输入和反向传播时的梯度信息。因此，总内存占用为\n",
        "$$memory_{forward}=(c_i+k_h-1)*(h+k_w-1)*c_0+2*c_i*h*w$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMGABMnGbNrB"
      },
      "source": [
        "**第3问：**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ef0fAhubNy-"
      },
      "source": [
        "&emsp;&emsp;反向传播的内存作用与前向传播相同，总内存占用为\n",
        "$$memory_{backward}=(c_i+k_h-1)*(h+k_w-1)*c_0+2*c_i*h*w$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqGjYQ5LbN7L"
      },
      "source": [
        "**第4问：**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JrDsP6pbOCT"
      },
      "source": [
        "&emsp;&emsp;反向计算成本为\n",
        "\n",
        "$$flops_{backward} = c_i \\times c_o \\times k_h \\times k_w \\times m_h \\times m_w$$\n",
        "\n",
        "&emsp;&emsp;其中$m_h$和$m_w$的定义同上"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKKQr0MPH6Go"
      },
      "source": [
        "### 练习 6.4.3\n",
        "\n",
        "如果我们将输入通道$c_i$和输出通道$c_o$的数量加倍，计算数量会增加多少？如果我们把填充数量翻一番会怎么样？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N825fzRxH6Go"
      },
      "source": [
        "**解答：** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CifxEEGjH6Go"
      },
      "source": [
        "&emsp;&emsp;如果我们将输入通道$c_i$和输出通道$c_o$的数量加倍，计算数量会增加$4$倍。如果我们把填充数量翻一番，计算数量会增加$2$倍。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLWGRhpQH6Go"
      },
      "source": [
        "### 练习 6.4.4\n",
        "\n",
        "如果卷积核的高度和宽度是$k_h=k_w=1$，前向传播的计算复杂度是多少？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MBYJCXuH6Go"
      },
      "source": [
        "**解答：**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4Wcrl3XH6Gp"
      },
      "source": [
        "$$flops = c_i \\times c_o \\times \\frac{h-p_h}{s_h} \\times \\frac{w-p_w}{s_w}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khZEzrrsH6Gp"
      },
      "source": [
        "### 练习 6.4.5\n",
        "\n",
        "本节最后一个示例中的变量`Y1`和`Y2`是否完全相同？为什么？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4amqFuFH6Gp"
      },
      "source": [
        "**解答：**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9YjhzTfH6Gp"
      },
      "source": [
        "&emsp;&emsp;浮点数计算有误差，因而两者不完全相同。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGIsrbxRH6Gp"
      },
      "source": [
        "### 练习 6.4.6\n",
        "\n",
        "当卷积窗口不是$1\\times 1$时，如何使用矩阵乘法实现卷积？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHRBB75GH6Gp"
      },
      "source": [
        "**解答：** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JowXZuVH6Gp"
      },
      "source": [
        "&emsp;&emsp;可以将输入张量和卷积核张量分别展开为二维矩阵，然后对这两个矩阵进行乘法运算，得到的结果再变换为输出张量。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQluJheeH6Gp"
      },
      "source": [
        "## 6.5 汇聚层 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXmo99OwH6Gp"
      },
      "source": [
        "### 练习 6.5.1\n",
        "\n",
        "尝试将平均汇聚层作为卷积层的特殊情况实现。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQDuZsKZH6Gq"
      },
      "source": [
        "**解答：** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpS7o9ZjH6Gq"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn \n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module): \n",
        "    def init(self):\n",
        "        super(Net, self).init()\n",
        "        # 定义卷积层1，输入通道数为1，输出通道数为6，卷积核大小为5x5\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        # 定义池化层，将6个通道的特征图进行平均池化\n",
        "        self.pool = nn.Conv2d(6, 6, 5)\n",
        "        # 定义卷积层2，输入通道数为6，输出通道数为16，卷积核大小为5x5\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        # 定义全连接层1，输入大小为16x5x5，输出大小为120\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        # 定义全连接层2，输入大小为120，输出大小为84\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        # 定义全连接层3，输入大小为84，输出大小为10（对应10个类别的分类）\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 使用ReLU激活函数进行卷积层1的前向传播\n",
        "        x = F.relu(self.conv1(x))\n",
        "        # 使用平均池化进行池化层的前向传播\n",
        "        x = F.avg_pool2d(x, (2, 2))\n",
        "        # 使用ReLU激活函数进行卷积层2的前向传播\n",
        "        x = F.relu(self.conv2(x))\n",
        "        # 使用平均池化进行池化层的前向传播\n",
        "        x = F.avg_pool2d(x, (2, 2))\n",
        "        # 将特征图展平为一维向量\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        # 使用ReLU激活函数进行全连接层1的前向传播\n",
        "        x = F.relu(self.fc1(x))\n",
        "        # 使用ReLU激活函数进行全连接层2的前向传播\n",
        "        x = F.relu(self.fc2(x))\n",
        "        # 全连接层3的前向传播，没有使用激活函数\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:] # 获取除了批量维度之外的其他维度的大小\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryQl9D8qH6Gr"
      },
      "source": [
        "### 练习 6.5.2\n",
        "\n",
        "尝试将最大汇聚层作为卷积层的特殊情况实现。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-09jj2UrH6Gr"
      },
      "source": [
        "**解答：** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQd7cXluH6Gr"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn \n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module): \n",
        "    def init(self):\n",
        "        super(Net, self).init()\n",
        "        # 定义卷积层1，输入通道数为1，输出通道数为6，卷积核大小为5x5\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        # 定义池化层，将6个通道的特征图进行平均池化\n",
        "        self.pool = nn.Conv2d(6, 6, 5)\n",
        "        # 定义卷积层2，输入通道数为6，输出通道数为16，卷积核大小为5x5\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        # 定义全连接层1，输入大小为16x5x5，输出大小为120\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        # 定义全连接层2，输入大小为120，输出大小为84\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        # 定义全连接层3，输入大小为84，输出大小为10（对应10个类别的分类）\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 使用ReLU激活函数进行卷积层1的前向传播\n",
        "        x = F.relu(self.conv1(x))\n",
        "        # 使用平均池化进行池化层的前向传播\n",
        "        x = F.avg_pool2d(x, (2, 2))\n",
        "        # 使用ReLU激活函数进行卷积层2的前向传播\n",
        "        x = F.relu(self.conv2(x))\n",
        "        # 使用平均池化进行池化层的前向传播\n",
        "        x = F.avg_pool2d(x, (2, 2))\n",
        "        # 将特征图展平为一维向量\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        # 使用ReLU激活函数进行全连接层1的前向传播\n",
        "        x = F.relu(self.fc1(x))\n",
        "        # 使用ReLU激活函数进行全连接层2的前向传播\n",
        "        x = F.relu(self.fc2(x))\n",
        "        # 全连接层3的前向传播，没有使用激活函数\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:] # 获取除了批量维度之外的其他维度的大小\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5YqgAbbH6Gr"
      },
      "source": [
        "### 练习 6.5.3\n",
        "\n",
        "假设汇聚层的输入大小为$c\\times h\\times w$，则汇聚窗口的形状为$p_h\\times p_w$，填充为$(p_h, p_w)$，步幅为$(s_h, s_w)$。这个汇聚层的计算成本是多少？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMjxs6WbH6Gr"
      },
      "source": [
        "**解答：** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykP2lf_CH6Gr"
      },
      "source": [
        "$$flops=\\frac{c \\times h \\times w \\times p_h \\times p_w}{s_h \\times s_w}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yiOraa6H6Gs"
      },
      "source": [
        "### 练习 6.5.4\n",
        "\n",
        "为什么最大汇聚层和平均汇聚层的工作方式不同？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6onxEYAyH6Gs"
      },
      "source": [
        "**解答：** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkRiZ35eH6Gs"
      },
      "source": [
        "&emsp;&emsp;最大池化层和平均池化层的工作方式不同，因为它们使用不同的池化方法。最大池化层将输入张量分成不重叠的区域，并在每个区域中选择最大值。平均池化层将输入张量分成不重叠的区域，并计算每个区域的平均值。这些方法的主要区别在于它们如何处理输入张量中的信息。最大池化层通常用于提取输入张量中的显著特征，而平均池化层通常用于减少输入张量的大小并提高模型的计算效率。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dE1ihMORH6Gs"
      },
      "source": [
        "### 练习 6.5.5\n",
        "\n",
        "我们是否需要最小汇聚层？可以用已知函数替换它吗？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80MiSdFlH6Gs"
      },
      "source": [
        "**解答：** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nbP1cklH6Gs"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def min_pool2d(x, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False):\n",
        "    # 将输入张量 x 取负数\n",
        "    neg_x = -x\n",
        "    # 使用 F.max_pool2d 对负数张量进行最大池化操作\n",
        "    neg_min_pool = F.max_pool2d(neg_x, kernel_size, stride=stride, padding=padding, dilation=dilation, ceil_mode=ceil_mode)\n",
        "    # 对最大池化结果再取负数，得到最小池化的结果\n",
        "    min_pool = -neg_min_pool\n",
        "    return min_pool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PNLI8uzH6Gt"
      },
      "source": [
        "### 练习 6.5.6\n",
        "\n",
        "除了平均汇聚层和最大汇聚层，是否有其它函数可以考虑（提示：回想一下`softmax`）？为什么它不流行？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCnILmizH6Gt"
      },
      "source": [
        "**解答：** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5eyNAZyxOWX"
      },
      "source": [
        "&emsp;&emsp;除了平均汇聚层和最大汇聚层，还有一些其他的池化函数，例如Lp池化和随机池化。Softmax函数通常用于多分类问题，它将每个输出分类的结果赋予一个概率值，表示属于每个类别的可能性。但是，Softmax函数不适用于池化层，因为它会将所有输入数据转换为概率分布，这会导致信息丢失。因此，Softmax函数不流行用于池化层。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1AdlfjBH6Gt"
      },
      "source": [
        "## 6.6 卷积神经网络（LeNet） "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmcAHKVpH6Gt"
      },
      "source": [
        "### 练习 6.6.1\n",
        "\n",
        "将平均汇聚层替换为最大汇聚层，会发生什么？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3D3TAOpH6Gt"
      },
      "source": [
        "**解答：** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c84yG4GSH6Gt"
      },
      "source": [
        "&emsp;&emsp;输出更大，梯度更大，训练更容易（AlexNet改进的方式之一）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVg6Ri3Mxxv2"
      },
      "source": [
        "&emsp;&emsp; LeNet原始代码如下："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98dd44b7",
        "origin_pos": 2,
        "pycharm": {
          "name": "#%%\n"
        },
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l\n",
        "\n",
        "# 创建一个LeNet模型，包括卷积层、池化层和全连接层\n",
        "net = nn.Sequential(\n",
        "    nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(),\n",
        "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "    nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(),\n",
        "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(),\n",
        "    nn.Linear(120, 84), nn.Sigmoid(),\n",
        "    nn.Linear(84, 10))\n",
        "# 创建一个随机输入张量\n",
        "X = torch.rand(size=(1, 1, 28, 28), dtype=torch.float32)\n",
        "# 遍历LeNet模型的各层并输出各层的输出形状\n",
        "for layer in net:\n",
        "    X = layer(X)\n",
        "    print(layer.__class__.__name__,'output shape: \\t',X.shape)\n",
        "# 加载Fashion MNIST数据集\n",
        "batch_size = 256\n",
        "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)\n",
        "# 定义用于在GPU上评估模型精度的函数\n",
        "def evaluate_accuracy_gpu(net, data_iter, device=None): #@save\n",
        "    \"\"\"使用GPU计算模型在数据集上的精度\"\"\"\n",
        "    if isinstance(net, nn.Module):\n",
        "        net.eval()  # 设置为评估模式\n",
        "        if not device:\n",
        "            device = next(iter(net.parameters())).device\n",
        "    # 正确预测的数量，总预测的数量\n",
        "    metric = d2l.Accumulator(2)\n",
        "    with torch.no_grad():\n",
        "        for X, y in data_iter:\n",
        "            if isinstance(X, list):\n",
        "                # BERT微调所需的（之后将介绍）\n",
        "                X = [x.to(device) for x in X]\n",
        "            else:\n",
        "                X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            metric.add(d2l.accuracy(net(X), y), y.numel())\n",
        "    return metric[0] / metric[1]\n",
        "# 定义用于训练模型的函数@save\n",
        "def train_ch6(net, train_iter, test_iter, num_epochs, lr, device):\n",
        "\n",
        "    \"\"\"用GPU训练模型(在第六章定义)\"\"\"\n",
        "    # 定义初始化权重的函数，使用Xavier初始化\n",
        "    def init_weights(m):\n",
        "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "    \n",
        "    # 对模型的所有层应用初始化权重的函数\n",
        "    net.apply(init_weights)\n",
        "    \n",
        "    # 打印在哪个设备上进行训练（CPU或GPU）\n",
        "    print('training on', device)\n",
        "    \n",
        "    # 将模型移动到指定的设备（CPU或GPU）\n",
        "    net.to(device)\n",
        "    \n",
        "    # 定义优化器，使用随机梯度下降（SGD）优化器\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
        "    \n",
        "    # 定义损失函数，使用交叉熵损失\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # 创建用于绘制训练过程的动画\n",
        "    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
        "                            legend=['train loss', 'train acc', 'test acc'])\n",
        "    \n",
        "    # 创建计时器和获取每个epoch的批量数\n",
        "    timer, num_batches = d2l.Timer(), len(train_iter)\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        # 训练损失之和，训练准确率之和，样本数\n",
        "        metric = d2l.Accumulator(3)\n",
        "        \n",
        "        # 设置模型为训练模式\n",
        "        net.train()\n",
        "        \n",
        "        for i, (X, y) in enumerate(train_iter):\n",
        "            timer.start()\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # 将输入数据和标签移动到指定的设备上\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            \n",
        "            # 前向传播\n",
        "            y_hat = net(X)\n",
        "            \n",
        "            # 计算损失\n",
        "            l = loss(y_hat, y)\n",
        "            \n",
        "            # 反向传播\n",
        "            l.backward()\n",
        "            \n",
        "            # 更新模型参数\n",
        "            optimizer.step()\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                # 记录训练损失、训练准确率和样本数\n",
        "                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
        "            timer.stop()\n",
        "            train_l = metric[0] / metric[2]\n",
        "            train_acc = metric[1] / metric[2]\n",
        "            \n",
        "            # 每完成一定批量数的训练，更新动画\n",
        "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
        "                animator.add(epoch + (i + 1) / num_batches,\n",
        "                            (train_l, train_acc, None))\n",
        "        \n",
        "        # 计算并记录测试集上的准确率\n",
        "        test_acc = evaluate_accuracy_gpu(net, test_iter)\n",
        "        animator.add(epoch + 1, (None, None, test_acc))\n",
        "    \n",
        "    # 打印最终的训练损失、训练准确率和测试准确率\n",
        "    print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '\n",
        "        f'test acc {test_acc:.3f}')\n",
        "    \n",
        "    # 打印训练速度（样本/秒）\n",
        "    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '\n",
        "        f'on {str(device)}')\n",
        "# 设置学习率和训练周期，并开始训练LeNet模型\n",
        "lr, num_epochs = 0.9, 10\n",
        "train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNRiATeIuQMX"
      },
      "source": [
        "&emsp;&emsp; 代码验证如下："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4384f90",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l\n",
        "\n",
        "# 定义一个包含最大池化层的LeNet模型\n",
        "net_maxpool = nn.Sequential(\n",
        "    nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(),  # 第一个卷积层\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2),  # 第一个最大池化层\n",
        "    nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(),  # 第二个卷积层\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2),  # 第二个最大池化层\n",
        "    nn.Flatten(),  # 展平操作，将多维张量展平为一维\n",
        "    nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(),  # 第一个全连接层\n",
        "    nn.Linear(120, 84), nn.Sigmoid(),  # 第二个全连接层\n",
        "    nn.Linear(84, 10))  # 输出层，共10个类别\n",
        "\n",
        "# 创建一个随机输入张量\n",
        "X = torch.rand(size=(1, 1, 28, 28), dtype=torch.float32)\n",
        "\n",
        "# 遍历LeNet模型的各层并输出各层的输出形状\n",
        "for layer in net_maxpool:\n",
        "    X = layer(X)\n",
        "    print(layer.__class__.__name__,'output shape: \\t',X.shape)\n",
        "\n",
        "# 设置学习率和训练周期\n",
        "lr, num_epochs = 0.9, 10\n",
        "\n",
        "# 调用train_ch6函数来在GPU上训练模型，使用你之前定义的函数\n",
        "train_ch6(net_maxpool, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-RxShsf46LS"
      },
      "source": [
        "&emsp;&emsp; 可见用最大汇聚层替换平均汇聚层后，在训练集和测试集上均得到了更好的结果"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etARpxbiH6Gt"
      },
      "source": [
        "### 练习 6.6.2\n",
        "\n",
        "尝试构建一个基于LeNet的更复杂的网络，以提高其准确性。\n",
        "1. 调整卷积窗口大小。\n",
        "1. 调整输出通道的数量。\n",
        "1. 调整激活函数（如ReLU）。\n",
        "1. 调整卷积层的数量。\n",
        "1. 调整全连接层的数量。\n",
        "1. 调整学习率和其他训练细节（例如，初始化和轮数）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmc7aQICH6Gt"
      },
      "source": [
        "**解答：** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjvzTbAXulfz"
      },
      "source": [
        "**第1问：**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-E6t0GsulmH"
      },
      "source": [
        "&emsp;&emsp; `nn.Conv2d(1, 6, kernel_size = 7)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnqV2DSSulx5"
      },
      "source": [
        "**第2问：**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bh0S_tDful5D"
      },
      "source": [
        "&emsp;&emsp; `nn.Conv2d(1, 10, kernel_size = 5)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPIOeTz_umAj"
      },
      "source": [
        "**第3问：**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7wIit-_umHa"
      },
      "source": [
        "&emsp;&emsp; `nn.Conv2d(1, 6, kernel_size = 5).ReLU()`或者直接将`nn.Sigmoid()`改为`nn.ReLU()`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ev80H6ydumOY"
      },
      "source": [
        "**第4问：**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YahX5BwumU5"
      },
      "source": [
        "&emsp;&emsp; 添加`conv3`为`nn.Cov2d(16, 120, kernel_size = 5). ReLU()`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKPp_2wBumbv"
      },
      "source": [
        "**第5问：**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDpiFmlBumh2"
      },
      "source": [
        "&emsp;&emsp; 添加`nn.Linear(84, 20)`并将`nn.Linear(84, 10)`替换为`nn.Linear(20, 10)`，添加`nn.Sigmoid()`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuBEDHOfumpI"
      },
      "source": [
        "**第6问：**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Va4i3jXKumvh"
      },
      "source": [
        "&emsp;&emsp; `lr, num_epochs = 0.1, 50`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv0dRt72H6Gu"
      },
      "source": [
        "### 练习 6.6.3\n",
        "\n",
        "在MNIST数据集上尝试以上改进的网络。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rbz9BNXqH6Gu"
      },
      "source": [
        "**解答：** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "082f2fe4",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l\n",
        "\n",
        "# 定义一个包含改进的LeNet模型\n",
        "net_improve = nn.Sequential(\n",
        "    nn.Conv2d(1, 6, kernel_size=3, padding=2), nn.ReLU(),  # 第一个卷积层\n",
        "    nn.AvgPool2d(kernel_size=2, stride=2),  # 第一个平均池化层\n",
        "    nn.Conv2d(6, 10, kernel_size=3), nn.ReLU(),  # 第二个卷积层\n",
        "    nn.AvgPool2d(kernel_size=2, stride=2),  # 第二个平均池化层\n",
        "    nn.Conv2d(10, 16, kernel_size=3), nn.ReLU(),  # 第三个卷积层\n",
        "    nn.AvgPool2d(kernel_size=2, stride=2),  # 第三个平均池化层\n",
        "    nn.Flatten(),  # 展平操作，将多维张量展平为一维\n",
        "    nn.Linear(16 * 2 * 2, 120), nn.ReLU(),  # 第一个全连接层\n",
        "    nn.Linear(120, 84), nn.ReLU(),  # 第二个全连接层\n",
        "    nn.Linear(84, 20), nn.ReLU(),  # 第三个全连接层\n",
        "    nn.Linear(20, 10))  # 输出层，共10个类别\n",
        "\n",
        "# 创建一个随机输入张量\n",
        "X = torch.rand(size=(1, 1, 28, 28), dtype=torch.float32)\n",
        "\n",
        "# 遍历改进的LeNet模型的各层并输出各层的输出形状\n",
        "for layer in net_improve:\n",
        "    X = layer(X)\n",
        "    print(layer.__class__.__name__,'output shape: \\t',X.shape)\n",
        "\n",
        "# 设置学习率和训练周期\n",
        "lr, num_epochs = 0.1, 50\n",
        "\n",
        "# 调用train_ch6函数来在GPU上训练模型，使用你之前定义的函数\n",
        "train_ch6(net_improve, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cek7LSVZ5PSb"
      },
      "source": [
        "&emsp;&emsp; 改进之后的网络在训练集和测试集上均表现出了更好的效果"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDIBoQa0H6Gu"
      },
      "source": [
        "### 练习 6.6.4\n",
        "\n",
        "显示不同输入（例如毛衣和外套）时，LeNet第一层和第二层的激活值。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mq9U1KWRH6Gu"
      },
      "source": [
        "**解答：** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TK6125gm5eK_"
      },
      "source": [
        "&emsp;&emsp; 通过在所有epoch结束后添加`d2l.show_images`展示了激活值"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "346c706c",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l\n",
        "\n",
        "# 定义一个包含多个卷积层和全连接层的LeNet模型\n",
        "net = nn.Sequential(\n",
        "    nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(),  # 第一个卷积层\n",
        "    nn.AvgPool2d(kernel_size=2, stride=2),  # 第一个平均池化层\n",
        "    nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(),  # 第二个卷积层\n",
        "    nn.AvgPool2d(kernel_size=2, stride=2),  # 第二个平均池化层\n",
        "    nn.Flatten(),  # 展平操作，将多维张量展平为一维\n",
        "    nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(),  # 第一个全连接层\n",
        "    nn.Linear(120, 84), nn.Sigmoid(),  # 第二个全连接层\n",
        "    nn.Linear(84, 10))  # 输出层，共10个类别\n",
        "\n",
        "# 创建一个随机输入张量\n",
        "X = torch.rand(size=(1, 1, 28, 28), dtype=torch.float32)\n",
        "\n",
        "# 遍历LeNet模型的各层并输出各层的输出形状\n",
        "for layer in net:\n",
        "    X = layer(X)\n",
        "    print(layer.__class__.__name__,'output shape: \\t',X.shape)\n",
        "\n",
        "# 设置训练批量大小\n",
        "batch_size = 256\n",
        "\n",
        "# 加载Fashion MNIST数据集并创建训练和测试迭代器\n",
        "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)\n",
        "\n",
        "# 定义一个用于在GPU上评估模型精度的函数\n",
        "def evaluate_accuracy_gpu(net, data_iter, device=None):\n",
        "    \"\"\"使用GPU计算模型在数据集上的精度\"\"\"\n",
        "    if isinstance(net, nn.Module):\n",
        "        net.eval()  # 设置为评估模式\n",
        "        if not device:\n",
        "            device = next(iter(net.parameters())).device\n",
        "    # 正确预测的数量，总预测的数量\n",
        "    metric = d2l.Accumulator(2)\n",
        "    with torch.no_grad():\n",
        "        for X, y in data_iter:\n",
        "            if isinstance(X, list):\n",
        "                # BERT微调所需的（之后将介绍）\n",
        "                X = [x.to(device) for x in X]\n",
        "            else:\n",
        "                X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            metric.add(d2l.accuracy(net(X), y), y.numel())\n",
        "    return metric[0] / metric[1]\n",
        "\n",
        "# 定义一个函数来在GPU上训练模型\n",
        "def train_ch6(net, train_iter, test_iter, num_epochs, lr, device):\n",
        "    \"\"\"用GPU训练模型(在第六章定义)\"\"\"\n",
        "    def init_weights(m):\n",
        "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "    net.apply(init_weights)\n",
        "    print('training on', device)\n",
        "    net.to(device)\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
        "                            legend=['train loss', 'train acc', 'test acc'])\n",
        "    timer, num_batches = d2l.Timer(), len(train_iter)\n",
        "    for epoch in range(num_epochs):\n",
        "        # 训练损失之和，训练准确率之和，样本数\n",
        "        metric = d2l.Accumulator(3)\n",
        "        net.train()\n",
        "        for i, (X, y) in enumerate(train_iter):\n",
        "            timer.start()\n",
        "            optimizer.zero_grad()\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            y_hat = net(X)\n",
        "            l = loss(y_hat, y)\n",
        "            l.backward()\n",
        "            optimizer.step()\n",
        "            with torch.no_grad():\n",
        "                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
        "            timer.stop()\n",
        "            train_l = metric[0] / metric[2]\n",
        "            train_acc = metric[1] / metric[2]\n",
        "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
        "                animator.add(epoch + (i + 1) / num_batches,\n",
        "                            (train_l, train_acc, None))\n",
        "        test_acc = evaluate_accuracy_gpu(net, test_iter)\n",
        "        animator.add(epoch + 1, (None, None, test_acc))\n",
        "\n",
        "    # 以下代码用于可视化模型中的某些层的输出\n",
        "    x_first_Sigmoid_layer = net[0:2](X)[0:9, 1, :, :]\n",
        "    d2l.show_images(x_first_Sigmoid_layer.reshape(9, 28, 28).cpu().detach(), 1, 9)\n",
        "    x_second_Sigmoid_layer = net[0:5](X)[0:9, 1, :, :]\n",
        "    d2l.show_images(x_second_Sigmoid_layer.reshape(9, 10, 10).cpu().detach(), 1, 9)\n",
        "\n",
        "    print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '\n",
        "        f'test acc {test_acc:.3f}')\n",
        "    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '\n",
        "        f'on {str(device)}')\n",
        "\n",
        "# 设置学习率和训练周期\n",
        "lr, num_epochs = 0.9, 10\n",
        "\n",
        "# 调用train_ch6函数来在GPU上训练模型\n",
        "train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
